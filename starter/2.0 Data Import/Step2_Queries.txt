Q1 - https://udacitycognitive.search.windows.net/indexes/azureblob-index/docs?api-version=2021-04-30-Preview&search=*
Output -

{ "@odata.context": "https://udacitycognitive.search.windows.net/indexes('azureblob-index')/$metadata#docs(*)", "value": [ { "@search.score": 1, "content": "\nContext‑aware rule learning \nfrom smartphone data: survey, challenges \nand future directions\nIqbal H. Sarker1,2*\n\nIntroduction\nIn recent days, smartphones have become an essential part of our daily life and con-\nsidered as highly personal devices of individuals. These devices are also known as one \nof the most important IoT (Internet of Things) devices, because of their capabilities \nto interconnect their users with the Internet, and corresponding data processing [1]. \nSmartphones are also considered as “next generation, multifunctional cell phones that \nfacilitates data processing as well as enhanced wireless connectivity” [2]. The cellular net-\nwork coverage has reached 96.8% of the world population, and this number even reaches \n100% of the population in the developed countries [3]. In recent statistics, according to \nGoogle Trends [4] we have shown in Fig.  1, that users’ interest on “Mobile Phones” is \nmore and more than other platforms like “Desktop Computer”, “Laptop Computer” or \n\nAbstract \n\nSmartphones are considered as one of the most essential and highly personal devices \nof individuals in our current world. Due to the popularity of context-aware technol-\nogy and recent developments in smartphones, these devices can collect and process \nraw contextual data about users’ surrounding environment and their corresponding \nbehavioral activities with their phones. Thus, smartphone data analytics and building \ndata-driven context-aware systems have gained wide attention from both academia \nand industry in recent days. In order to build intelligent context-aware applications on \nsmartphones, effectively learning a set of context-aware rules from smartphone data \nis the key. This requires advanced data analytical techniques with high precision and \nintelligent decision making strategies based on contexts. In comparison to traditional \napproaches, machine learning based techniques provide more effective and efficient \nresults for smartphone data analytics and corresponding context-aware rule learning. \nThus, this article first makes a survey on previous work in the area of contextual smart-\nphone data analytics and then presents a discussion of challenges and future directions \nfor effectively learning context-aware rules from smartphone data, in order to build \nrule-based automated and intelligent systems.\n\nKeywords: Smartphone data, Machine learning, Data science, Clustering, \nClassification, Association, Rule learning, Personalization, Time-series, User behavior \nmodeling, Predictive analytics, Context-aware computing, Mobile and IoT services, \nIntelligent systems\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nSarker J Big Data (2019) 6:95 \nhttps://doi.org/10.1186/s40537‑019‑0258‑4\n\n*Correspondence: \nmsarker@swin.edu.au \n1 Swinburne University \nof Technology, \nMelbourne VIC-3122, \nAustralia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0258-4&domain=pdf\n\n\nPage 2 of 25Sarker J Big Data (2019) 6:95 \n\n“Tablet Computer” for the last 5 years from 2014 to 2019. Figure 1 represents timestamp \ninformation in terms of particular date in x-axis and corresponding search interests in \nthe range of 0 to 100 in terms of popularity relative to the highest point on the chart in \ny-axis. For instance, a value of 100 (maximum) in y-axis represents the peak popularity \nfor a particular term, while 0 (minimum) means the term was lowest in terms of popu-\nlarity [4].\n\nDue to the advanced features and recent developments in smartphones, these devices \ncan collect raw contextual data about users’ surrounding environment and their corre-\nsponding behavioral activities with their phones in a daily basis [5]. As a result, smart-\nphone data becomes a great source to understand users’ behavioral activity patterns in \ndifferent contexts, and to derive useful information, i.e., context-aware rules, for the pur-\npose of building rule-based intelligent context-aware systems. A context-aware rule has \ntwo parts, which follows “IF-THEN” logical structure to formulate [6]. The antecedent \npart represents users’ surrounding contextual information, e.g., temporal context, spa-\ntial context, social contexts, or others relevant contextual information and the conse-\nquent part represents their corresponding behavioral activities or usage. Let’s consider \nan example of a context-aware mobile notification management system for a smart-\nphone user Alice. A context-aware rule for such system could be “The user typically \ndismisses mobile notifications while at work; however, accepts the notifications in the \nevening from her family members, even though she is in work”. A set of such context-\naware behavioral rules including general and specific exceptions, may vary from user-to-\nuser according to their preferences. In addition to the personalized services mentioned \nabove, the relevant context-aware rules in different surrounding contexts could be appli-\ncable to other broad application areas, like context-aware  software and IoT services, \nintelligent eHealth services, and context-aware smart city services, intelligent cybersecu-\nrity services etc. utilizing the relevant contextual data of that particular domain. Overall, \nthis study is typically for those data science and machine learning researchers, and prac-\ntitioners who particularly want to work on data-driven intelligent context-aware systems \nand services based on machine learning rules.\n\nEffectively learning context-aware rules from smartphone data is challenging because \nof many reasons, ranging from understanding raw data to applications. A number of \nresearch [7–9] has been done on mining context-aware rules from smartphone data for \nvarious purposes. However, to effectively learn such rules for the purpose of building \n\nFig. 1 Users’ interest trends over time, where x-axis and y-axis represent a particular timestamp and \ncorresponding search interests in numeric values in terms of world-wide popularity respectively\n\n\n\nPage 3 of 25Sarker J Big Data (2019) 6:95 \n\nintelligent context-aware systems, a deeper analysis in contextual data patterns and \nlearning according to individuals’ usage is needed. Thus, advanced data analysis based \non machine learning techniques, can be used to make effective and efficient decision-\nmaking capabilities in different context-aware test cases for smartphones. Several \nmachine learning and data mining techniques, such as contextual data clustering, fea-\nture optimization and selection, rule-based classification and association analysis, incre-\nmental learning for dynamic updating and management, and corresponding rule-based \nprediction model can be designed to provide smartphone data analytic solutions. The \nreason is that such machine learning techniques can be more accurate, and more precise \nfor analyzing huge amount of contextual data. The aim of these advanced analytic tech-\nniques is to discover information, hidden patterns, and unknown correlations among the \ncontexts and eventually generate context-aware rules. For instance, a detailed analysis \nof time-series data and corresponding data clustering based on similar behavioral pat-\nterns, could lead to capture the diverse behaviors of an individual’s activities, thereby \nenabling more optimal time-based context-aware rules than the traditional approaches \n[10]. Thus, intelligent data-driven decisions using machine learning techniques can \nprofit better decision making capability over the traditional approaches while consider-\ning the multi-dimensional contexts.\n\nBased on our survey and analysis on existing research, little work has been done in \nterms of how machine learning techniques significantly impact on contextual smart-\nphone data and to learn corresponding context-aware rules. To address this short-\ncoming, this article first makes a survey on previous work in the area of contextual \nsmartphone data analytics in several perspectives involved in context-aware rules, such \nas time-series modeling that is also known as a discretization of temporal context, rule \ndiscovery techniques, and incremental learning and rule updation techniques, which has \nbeen highlighted in our earlier work [6]. After that this article presents a brief discussion \non challenges and future directions to overcome these issues. Based on our discussion, \nfinally we suggest a machine learning based context-aware rule learning framework for \nthe purpose of effectively learning context-aware rules from smartphone data, in order \nto build rule-based automated and intelligent systems.\n\nThe contributions of this paper are summarized as follows.\n\n• We first make a brief survey on previous work in the area of smartphone data analyt-\nics in several perspectives related to context-aware rule learning and summarize the \nshortcomings of these research.\n\n• We then present a brief discussion on the challenges and future directions to over-\ncome the issues to learn context-aware rules from smartphone data.\n\n• Finally, we suggest a machine learning based context-aware rule learning framework \nand briefly discuss the role of various layers associated with the framework, for the \npurpose of building rule-based intelligent context-aware systems.\n\nTo the best of our knowledge, this is the first article surveying context-aware rule learn-\ning strategies from smrtphone data. The remainder of the paper is organized as follows. \n“Background: contexts and smartphone data” section presents background information \non contexts and contextual smartphone data. “Context-aware rule learning strategies” \n\n\n\nPage 4 of 25Sarker J Big Data (2019) 6:95 \n\nsection  surveys previous work in various perspectives related to context-aware rule \nlearning. “Challenges and future directions” section briefly discusses the challenges and \nfuture directions of research regarding context-aware rule learning from smartphone \ndata. In “Suggested machine learning based framework” section we suggest a machine \nlearning based context-aware rule learning framework and discuss various layers with \ntheir roles while learning rules. Context-aware rule based applications section summa-\nrizes a number of real world applications based on context-aware rules. Finally, “Conclu-\nsion” section concludes this paper.\n\nBackground: contexts and smartphone data\nThis section reviews background information on the main characteristics of contexts \nand contextual smartphone data that address learning context-aware rules for the pur-\npose of building rule-based intelligent systems.\n\nCharacteristics of contexts\n\nThe term context can be used with a variety of different meanings in different purposes. \nThe notion of context has been used in numerous areas, including Pervasive and Ubiq-\nuitous Computing, Human Computer Interaction, Computer-Supported Collaborative \nWork, and Ambient Intelligence [11]. In this section, first we briefly review what is con-\ntext in the area of mobile and context-aware computing. In Ubiquitous and Pervasive \nComputing area, early works on context-awareness referred to context as primarily \nthe location of people and objects [12]. In recent works, context has been extended to \ninclude a broader collection of factors, such as physical and social aspects of an entity, \nas well as the activities of users [11]. Having examined the definitions and categories of \ncontext given by the pervasive and ubiquitous computing community, this section seeks \nto define our view of context within the scope of smartphone data analytics. As the defi-\nnitions of context to pervasive and ubiquitous computing area are also broad, this dis-\ncussion is intended to be illustrative rather than exhaustive.\n\nSeveral studies have attempted to define and represent the context from different \nperspectives. For instance, the user’s location information, the surrounding people and \nobjects around the user, and the changes to those objects are considered as contexts by \nSchilit et al. [12]. Brown et al. [13] also define contexts as user’s locational information, \ntemporal information, the surrounding people around the user, temperature, etc. Simi-\nlarly, the user’s locational information, environmental information, temporal informa-\ntion, user’s identity, are also taken into account as contexts by Ryan et  al. [14]. Other \ndefinitions of context have simply provided synonyms for context such as context as the \nenvironment or social situation. A number of researchers are taken into account the \ncontext as the environmental information of the user. For instance, in [15], the environ-\nmental information that the user’s computer knows about are taken into account as con-\ntext by Brown et al., whereas the social situation of the user is considered as a context \nin Franklin et al. [16]. On the other hand, a number of other researchers consider it to \nbe the environment related to the applications. For instance, Ward et al. [17] consider \nthe state of the surrounding information of the applications as contexts. Hull et al. [18] \ndefine context as the aspects of the current situation of the user and include the entire \n\n\n\nPage 5 of 25Sarker J Big Data (2019) 6:95 \n\nenvironment. The settings of applications are also treated as context in Rodden et  al. \n[19].\n\nAccording to Schilit et  al. [20] the important aspects of context are: (i) where you \nare, (ii) whom you are with, and (iii) what resources are nearby. The information of the \nchanging environment is taken into account as context in their definition. In addition to \nthe user environment (e.g., user location, nearby people around the user, and the cur-\nrent social situation of the user), they also include the computing environment and the \nphysical environment. For instance, connectivity, available processors, user input and \ndisplay, network capacity, and costs of computing can be the examples of the computing \nenvironment, while the noise level, temperature, the lighting level, can be the examples \nof the physical environment. Dey et al. [21] present a survey of alternative view of con-\ntext, which are largely imprecise and indirect, typically defining context by synonym or \nexample. Finally, they offer the following definition of context, which is perhaps now the \nmost widely accepted. According to Dey et al. [21] “Context is any information that can \nbe used to characterize the situation of an entity. An entity is person, place or object \nthat is considered relevant to the interaction between a user and an application, includ-\ning the user and the application themselves”. Thus, based on the definition of Dey et al. \n[21], we can define context in the scope of this work as “Context is any information that \ncan be used to characterize users’ day-to-day situations that have an influence on their \nsmartphone usage”. An example of relevant contexts could be temporal context, spatial \ncontext, or social context etc. that might have an influence to make individuals’ diverse \ndecisions on smartphone usage in their daily life activities.\n\nContextual smartphone data\n\nWe live in the age of data [22], where everything that surrounds us is linked to a data \nsource and everything in our lives is captured digitally. Mobile or cellular phones have \nbecome increasingly ubiquitous and powerful to log user diverse activities for under-\nstanding their preferences and phone usage behavior. For instance, smart mobile phones \nhave the ability to log various types of context data related to a user’s phone call activities \nabout when the user makes outgoing calls, or accepts, rejects, and misses the incoming \ncalls [23–26]. In addition to such call related meta data, other dimensions of contex-\ntual information such as user location [27], user’s day-to-day situation [28], the social \nrelationship between the caller an callee identified by the individual’s unique phone \ncontact number [29] are also recorded by the smart mobile phones. Thus, call log data \ncollected by the smart mobile phone can be used as a context source to modeling indi-\nvidual mobile phone user behavior in smart context-aware mobile communication sys-\ntems [30]. In addition to voice communication, short message service (SMS) is known \nas text communication service allows the exchange of short text messages of individual \nmobile phone users, using standardized communications rules or protocols. According \nto the International Telecommunication Union [31], short messages have become a mas-\nsive commercial industry, worth over 81 billion dollars globally. The numerous growth \nin the number of mobile phone users in the world has lead to a dramatic increasing of \nspam messages [32]. The SMS log contains all the message including the spam and non-\nspam text messages [32, 33], which can be used in the task of automatic spam filtering \n[25, 32], or predicting good time or bad time to deliver such messages [33].\n\n\n\nPage 6 of 25Sarker J Big Data (2019) 6:95 \n\nWith the rapid development of smartphones, people use these devices for using vari-\nous categories of apps such as Multimedia, Facebook, Gmail, Youtube, Skype, Game [9, \n34]. Thus, smartphone apps log contains these usage with relevant contextual informa-\ntion [8, 9, 35–37]. Such logs can be used for mining the contextual behavioral patterns of \nindividual mobile phone users that is, which app is preferred by a particular user under \na certain context to provide personalized context-aware recommendation. In the real \nworld, a variety of smart mobile applications use notifications in order to inform the \nusers about various kinds of events, news or just to send them reminders or alerts. For \ninstance, the notifications of inviting games on social networks, social or promotional \nemails, or a number of predictive suggestions by various smart phone applications, \ne.g., Twitter, Facebook, LinkedIN, WhatsApp, Viver, Skype, Youtube [7]. The extracted \ncontextual patterns from smartphone notification logs can be used to build intelligent \nmobile notification management systems according to their preferences.\n\nUser navigation in the web in another major activities of individual users. Thus, web \nlog contains the information about user mobile web navigation, web searching, e-mail, \nentertainment, chat, misc, news, TV, netting, travel, sport, banking, and related contex-\ntual information [38–40]. Mining contextual usage patterns from such log data, can be \nused to make accurate context-aware predictions about user navigation and to adapt the \nportal structure according to the needs of users. Similarly, game log contains the infor-\nmation about playing various types such games such as action, adventure, casual, puzzle, \nRPG, strategy, sports etc. of individual mobile phone users, and related contextual infor-\nmation [41]. The extracted contextual patterns from such logs data, can be used to build \npersonalized mobile game recommendation system for individual mobile phone users \naccording to their own preferences.\n\nThe ubiquity of smart mobile phones and their computing capabilities for various real \nlife purposes provide an opportunity of using these devices as a life-logging device, i.e., \npersonal e-memories [42]. In a more technical sense, life-logs sense and store individ-\nual’s contextual information from their surrounding environment through a variety of \nsensors available in their smart mobile phones, which are the core components of life-\nlogs such as user phone calls, SMS headers (no content), App use (e.g., Skype, What-\nsapp, Youtube etc.), physical activities form Google play API, and related contextual \ninformation such as WiFi and Bluetooth devices in user’s proximity, geographical loca-\ntion, temporal information [42]. The extracted contextual patterns or behavioral rules of \nindividual mobile phone users utilizing such life log data, can be used to improve user \nexperience in their daily life. In addition to these personalized log data, smartphones are \nalso capable for collecting and processing IoT data [1]. Based on such smartphone data \nhaving contextual information, in this paper, we briefly review the existing rule learn-\ning strategies and discuss the open challenges and opportunities by highlighting future \ndirections for context-aware rule learning.\n\nContext‑aware rule learning strategies\nIn this section, we review existing strategies related to learning rules based on contex-\ntual information in various perspectives. This includes time-series modeling that cre-\nates behavioral data clusters for generating temporal context based rules, contextual rule \n\n\n\nPage 7 of 25Sarker J Big Data (2019) 6:95 \n\ndiscovery by taking into account multi-dimensional contexts, such as temporal, spatial \nor social contexts, and incremental learning to dynamic updating of rules.\n\nModeling time‑series smartphone data\n\nTime is the most important context that impacts on mobile user behavior for making \ndecisions [38]. Individual’s behaviors vary over time in the real world and the mobile \nphones record the exact time of all diverse activities of the users with their mobile \nphones. A time series is a sequence of data points ordered in time [43]. However, to use \nsuch time-series data into behavioral rules, an effective modeling of temporal context \nis needed. Thus, time-series segmentation becomes one of the research focuses in this \nstudy as exact time in mobile phone data is not very informative to mine behavioral rules \nof individual mobile phone users. According to [44], time-based behavior modeling is an \nopen problem. Hence, we summarize the existing time-series segmentation approaches \n\nTable 1 Various types of static time segments used in different applications\n\nTime interval type Number \nof segments\n\nUsed time interval and segment details References\n\nEqual 3 Morning [7:00–12:00], afternoon [13:00–18:00] and \nevening [19:00–24:00]\n\nSong et al. [46]\n\nEqual 3 [0:00–7:59], [8:00–15:59] and [16:00–23:59] Rawassizadeh et al. [47]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nMukherji et al. [48]\n\nEqual 4 Morning [6:00–12:00], afternoon [12:00–18:00], \nevening [18:00–24:00] and night [0:00–6:00]\n\nBayir et al. [49]\n\nEqual 4 Morning, afternoon, evening and night Paireekreng et al. [41]\n\nEqual 4 Morning [6:00–11:59], day [12:00–17:59], evening \n[18:00–23:59], overnight [0:00–5:59]\n\nJayarajah et al. [50]\n\nEqual 4 Night [0:00–6:00 a.m.], morning [6:00 a.m.–12:00 \np.m.], afternoon [12:00–6:00 p.m.], and evening \n[6:00 p.m.–0:00 a.m.]\n\nDo et al. [51]\n\nUnequal 3 Morning (beginning at 6:00 a.m. and ending at \nnoon), afternoon (ending at 6:00 p.m.), night (all \nremaining hours)\n\nXu et al. [52]\n\nUnequal 4 Morning [6:00–12:00], afternoon [12:00–16:00], \nevening [16:00–20:00] and night [20:00–24:00 \nand 0:00–6:00]\n\nMehrotra et al. [7]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00] and so on\n\nZhu et al. [9]\n\nUnequal 5 Morning, forenoon, afternoon, evening, and night Oulasvirta et al. [53]\n\nUnequal 5 Morning [7:00–11:00], noon [11:00–14:00], after-\nnoon [14:00–18:00], evening [18:00–21:00], and \nnight [21:00–Next day 7:00]\n\nYu et al. [54]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nNaboulsi et al. [55]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nDashdorj et al. [56]\n\nUnequal > 5 Early morning, morning, late morning, midnight \nand so on\n\nShin et al. [57]\n\nUnequal 8 S1[0:00–7:00 a.m.], S2[7:00–9:00 a.m.], S3[9:00–\n11:00 a.m.], S4[11:00 a.m.–2:00 p.m.], S5[2:00–\n5:00 p.m.], S6[5:00–7:00 p.m.], S7[7:00–9:00 p.m.] \nand S8[9:00 p.m.–12:00 a.m.]\n\nFarrahi et al. [58]\n\n\n\nPage 8 of 25Sarker J Big Data (2019) 6:95 \n\ninto two broad categories; (i) static segmentation, and (ii) dynamic segmentation, that \nare used in various mobile applications.\n\nStatic segmentation\n\nA static segmentation is easy to understand and can be useful to analyze population \nbehavior comparing across the mobile phone users. In order to generate segments, \nrecently, most of the researchers (shown in Table 1) take into account only the temporal \ncoverage (24-h-a-day) and statically segment time into arbitrary categories (e.g., morn-\ning) or periods (e.g., 1 h). Such static segmentation of time mainly focuses on time inter-\nvals. According to [45], there are mainly two types of time intervals: one is equal and \nanother one is unequal. For instance, four different time segments, i.e., morning [6:00–\n12:00], afternoon [12:00–18:00], evening [18:00–24:00] and night [0:00–6:00] can be an \nexample of equal interval based segmentation because of their same interval length. On \nthe other hand, another four time slots such as morning [6:00–12:00], afternoon [12:00–\n16:00], evening [16:00–20:00] and night [20:00–24:00 and 0:00–6:00] can be an example \nof unequal interval based segmentation. For this example, different lengths of time inter-\nval are used to do the segmentation. In Table 1, we have summarized a number of works \nthat use static segmentation considering either equal or unequal time interval in various \npurposes.\n\nAlthough, various time intervals and corresponding segmentation summarized in \nTable 1 are used in different purposes, these approaches take into account a fixed num-\nber of segments for all users. However, while performing such segmentation users’ behav-\nioral evidence that differs from user-to-user over time in the real world, is not taken into \naccount. Thus, these static generation of segments may not suitable for producing high \nconfidence temporal rules for individual smartphone users. For instance, N1 number \nof segments might give meaningful results for one case, while N2 number of segments \ncould give better results for another case, where N1 = N2 . Therefore, a dynamic segmen-\ntation of time rather than statically generation could be able to reflect individuals’ behav-\nioral evidence over time and can play a role to produce high confidence rules according \nto their usage records.\n\nDynamic segmentation\n\nAs discussed above, a segmentation technique that generates variable number of seg-\nments would be more meaningful to model users’ behavior. Thus, dynamic segmenta-\ntion technique rather than static segmentation can be used in order to achieve the goal. \nIn a dynamic segmentation, the number of segments are not fixed and predefined; may \nchange depending on their behavioral characteristics, patterns or preferences. Several \ndynamic segmentation techniques in terms of generating variable number of segments \nexist for modeling users’ behavioral activities in temporal contexts. A number of authors \nsimply take into account a single parameter, e.g., interval length or base period, to gener-\nate the segments. The number of time segments varies according to this period. If Tmax \nrepresents the whole time period of 24-h-a-day and BP is a base period, then the num-\nber of segments will be Tmax/BP [10]. If the base period increases, the number of time \nsegments decreases and vice-versa. For instance, if the base period is 5 min, then the \nnumber of segments will be the division result of 24-h-a-day and 5. In this example, a \n\n\n\nPage 9 of 25Sarker J Big Data (2019) 6:95 \n\nbase period, e.g., 5 min, is assumed as the finest granularity to distinguish day-to-day \nactivities of an individual. If the base period incremented to 15 min, then the number \nof segments decreases, where 15 min can be assumed as the finest granularity. Thus the \nnumber of segments varies based on the base time period. Similarly, individuals’ calen-\ndar schedules and corresponding time boundaries can also be used to determine var-\niable length of time segments, in order to model users’ behavior in temporal context, \nwhich may vary according to users’ preferences [59]. For instance, one user may have a \nparticular event between 1 and 2 p.m., while another may have in another time bound-\nary between 1:30 and 2:30 p.m.. Thus, the time segmentation varies according to their \ndaily life activities scheduled in their personal calendars. Similarly, multiple thresholds, \nsliding window, data shape based approaches are used in several applications, shown \nin Table 2. In addition to these approaches, a number of authors use machine learning \ntechniques such as clustering, genetic algorithm etc. In Table  2, we have summarized \na number of works that use such type of dynamic segmentation techniques in various \npurposes.\n\nClustering highlighted in Table  2 is one of the important machine learning tech-\nniques in forming large time segments where certain user behavior patterns are taken \ninto account. Usually, clustering algorithms are designed with certain assumptions and \nfavor certain type of problems. In this sense, it is not accurate to say ‘best’ in the con-\ntext of clustering algorithms; it depends on specific application [75]. Among the cluster-\ning algorithms the K-means algorithm is the best-known squared error-based clustering \nalgorithm [76]. However, this algorithm needs to specify the initial partitions and fixed \nnumber of clusters K. The convergence centroids also vary with different initial points. \nSometimes this algorithm is influenced by outliers because of mean value calculation. \n\nTable 2 Various types of dynamic time segments used in different applications\n\nBase technique Description References\n\nSingle parameter A predefined value of time interval, e.g., 15 min \nis used to generate segments\n\nOzer et al. [60]\n\nA different value of time interval, e.g., 30 min is \nused for segmentation\n\nDo et al. [61], Farrahi et al. [62]\n\nA relatively large value of the parameter, e.g., \n2-h is used to generate time segments\n\nKaratzoglou et al. [63]\n\nAnother large value of time interval, e.g., 3-h is \nused for segmentation to make the number \nof segments small\n\nPhithakkitnukoon et al. [64]\n\nCalendar Various calendar schedules and corresponding \ntime boundaries are used to model users’ \nbehavior in temporal context\n\nKhail et al. [65], Dekel et al. [66], Zulkernain \net al. [67], Seo et al. [68], Sarker et al. [28, \n59]\n\nMulti-thresholds To identify the lower and upper boundary \nof a particular segment for the purpose of \nsegmenting time-series log data\n\nHalvey et al. [38]\n\nData shape A data shape based time-series data analysis Zhang et al. [45], Shokoohi et al. [69]\n\nSliding window A sliding window is used to analyze time-series \ndata\n\nHartono et al. [70], Keogh et al. [71]\n\nClustering A predefined number of clusters is used to \ndiscover rules from time-series data\n\nDas et al. [72]\n\nGenetic algorithm A genetic algorithm is used to analyze time-\nseries data\n\nLu et al. [73], Kandasamy et al. [74]\n\n\n\nPage 10 of 25Sarker J Big Data (2019) 6:95 \n\nMore importantly, the characteristic of this algorithm might not be directly applicable \nfor the purpose of learning  context-aware rules. The reason is that users’ behave dif-\nferently in different contexts, which also may vary from user-to-user in the real world. \nThus, it’s difficult to assume a number of clusters K to capture their diverse behaviors \neffectively. Another similar K-medoids method [77] is more robust than K-means algo-\nrithm in the presence of outliers because a medoid is less influenced by outliers than a \nmean. Though it minimizes the outlier problem but the other characteristic mismatches \nexist between K-means and the problem of time-series modeling.\n\nAs the size and number of time segments depend on the user’s behavior and it differs \nfrom user-to-user, a bottom-up hierarchical data processing can help to make behavioral \nclusters. Existing hierarchical algorithms are mainly classified as agglomerative methods \nand device methods. However, the device clustering method is not commonly used in \npractice [75]. The simplest and most popular agglomerative clustering is single linkage \n[78] and complete linkage [79]. Another method, nearest neighbor [75], is also similar to \nthe single linkage agglomerative clustering algorithm. All these hierarchical algorithms \nuse a proximity matrix which is generated by computing the distance between a new \ncluster and other clusters. Then according to the matrix value these algorithms succes-\nsively merge the clusters until the desired cluster structure is obtained. However, it is \nnot possible to predict the level at which the merging is best according to a proximity \nmatrix because of the variations in users’ behavior. Thus applying such clustering tech-\nniques could generate the segments according to users’ behavioral patterns available in \ntime-series. Similarly, genetic algorithm based approaches shown in Table  2 also pro-\nduce dynamic segments.\n\nIn a summary, we can conclude that time-series modeling in terms of both static seg-\nmentation and dynamic segmentation approaches discussed above, are able to generate \nvarious time segments that can be used in different purposes. However, the above time-\nseries modeling approaches do not necessarily map to the patterns of individuals’ usage \naccording to their preferences, which is based on users’ diverse behaviors over time-of-\nthe-week and may vary from user to user. A machine learning based behavior-oriented \ndynamic time-series modeling technique by taking into account such patterns, could be \nsignificant in order to effectively use temporal context as the basis for discovering rules \ncapturing smartphone usage behavior.\n\nRule discovery\n\nAnother major issue focus in this study is discovering useful behavioral rules of individ-\nual mobile phone users based on multi-dimensional contexts, such as temporal, spatial, \nor social contexts, utilizing their smartphone data. In the area of machine learning both \nassociation rule learning [80] and classification rule learning [81] are the most common \ntechniques to discover such type of rules of individual mobile phone users. In the follow-\ning, we give an brief overview of both association and classification techniques for the \npurpose of discovering rules based on multi-dimensional contexts.\n\n\n\nPage 11 of 25Sarker J Big Data (2019) 6:95 \n\nAssociation rules\n\nAssociation rule learning algorithm discovers association rules that satisfy the prede-\nfined minimum support and confidence constraints from a given dataset [80]. Many \nassociation rule learning algorithms have been proposed in the data mining literature, \nsuch as logic based [82], frequent pattern based [80, 83, 84], tree-based [85] etc. Asso-\nciation rule learning technique is well defined in terms of rule’s performance, e.g., accu-\nracy, and flexibility as it has the own parameter support and confidence [86]. A number \nof researchers [7–9] have used association rule learning technique (e.g., Apriori) [80] \nto mine rules capturing mobile phone users’ behavior. However, the existing  associa-\ntion rule learning techniques might not be suitable for discovering users’ behavioral \nrules because of several reasons. In the following, we summarize the drawbacks of asso-\nciation rules for discovering the behavioral rules of individual mobile phone users by \ntaking into account multi-dimensional contexts.\n\nLacking in understanding the impact of contexts Different contexts in mobile phone \ndata, such as temporal, spatial or social context, may have different impact or influence \non the behavioral rules of individual mobile phone users. For instance, incoming phone \ncalls from a significant person, e.g., mother, is always answered by an individual, even \nthough she is in a meeting because of her family priority. In this case, the importance \nof social relationship between individuals ( social relationship → mother ) in making \nbehavioral decision, is higher than other relevant contexts such as time period, weekday \nor holiday, location, accompany with etc. However, the typical association rule learn-\ning technique implicitly assumes all the contexts in the datasets have the similar nature, \nand/or impact while discovering rules based on multi-dimensional contexts.\n\nRedundancy Association rule learning technique, e.g., Apriori, discovers all the con-\ntextual associations in a given dataset, if it satisfies the user preference, specified as \nminimum support value and minimum confidence value. As a result, association rule \nlearning technique produces a huge number of redundant rules as it does not take into \naccount the usefulness of a particular context or corresponding patterns while produc-\ning the associations. For instance, it produces up to 83% redundant rules for a given \ndataset that makes the rule-set unnecessarily large [87]. Therefore, it is very difficult to \ndetermine the most interesting ones among the huge amount of rules generated. As a \nresult, it makes the rule-based decision making process ineffective and more complex, \nwhich is not effective to build a context-aware intelligent system [88].\n\nComputational complexity and high training time In order to produce rules, associ-\nation rule learning technique takes huge amount of training time. For instance, in an \nexperimental study in mobile phone domain, the authors observe a high running time \nspanning several hours when the association rule learning algorithm is used to discover \nuser behavioral rules [8]. The main reason for taking high training time is that typical \nassociation techniques compute all the possible associations among contexts and are \nunable to filter the interesting rules that can be used to make effective decisions. As a \nresult the unnecessary generation of patterns increases the computational complexity \nand training time.\n\nIn a summary, by taking into account the impact of contexts, redundancy problem \nwhile generating rules, and computational complexity, typical association rule learning \n\n\n\nPage 12 of 25Sarker J Big Data (2019) 6:95 \n\ntechniques may not suitable to produce users’ behavioral rules in multi-dimensional \ncontexts, for the purpose of building intelligent context-aware systems.\n\nClassification rules\n\nClassification is another technique to discover user behavioral rules from the datasets. \nSeveral classification algorithms exist with the ability of rule generation like ZeroR [89], \nOneR [90], RIDOR [89], RIPPER [91], PART [92], DTNB [93], Decision Trees [81, 94] \netc. Among these techniques, decision tree is one of the most popular rule-based clas-\nsification algorithms as it has several advantages, such as easier to interpret; the ability \nto handle high dimensional data; simplicity and speed; good accuracy; and the ability to \ngenerate human understandable classification rules [95, 96]. In particular, a number of \nauthors [67, 97–100] have used decision tree classification technique to discover rules \ncapturing mobile phone users’ behavior for various purposes. However, the exisitng rule-\nbased classification techniques might not be suitable to model users’ behavior because of \nseveral reasons. In the following, we summarize the drawbacks of rule-based classifica-\ntion techniques for discovering the behavioral rules of individual mobile phone users in \nmulti-dimensional contexts.\n\nLow reliability In general, reliability refers the quality of being trustworthy or of per-\nforming consistently well. A pattern or rule is called reliable, if the relationship described \nby the pattern occurs in a high percentage of applicable cases. According to Geng et al. \n[101], a classification rule will be reliable, if it gives high prediction accuracy, and an \nassociation rule will be reliable, if it has high confidence that is associated to the accu-\nracy. However, the classification rules discovered by the typical rule-based classifica-\ntion techniques, e.g., decision trees, mostly have low reliability in many cases [7, 102]. \nAccording to Freitas et al. [86], a classification rule may not ensure a high accuracy in \npredictions. The reason is that it may has over-fitting problem and inductive bias, which \ndecrease the prediction accuracy of a machine learning based model.\n\nLacking in flexibility Traditional rule-based classification techniques, e.g., decision \ntrees, have no flexibility to set users’ preferences and consequently it makes rigid decision \nfor a particular test case [81]. However, rigid decision in modeling user behavior might \nnot be meaningful by considering real-world cases. The reason is that individuals’ pref-\nerences are not static in the real word; may vary from user-to-user [103]. For instance, \none individual may want the phone call agent to decline the incoming calls where she \ndid not answer the calls more than, say, 80% of the time in the past. For another person, \nthis preference could be 95% of the time according to her preference. Thus considering \nflexibility in users’ preferences while modeling their behavior could be another issue for \nmaking meaningful decisions in various context-aware test cases. \n\nLacking in generalization Typically, generality measures the comprehensiveness of a \npattern or rule, that is, the fraction of all the relevant records in the dataset that matches \nthe pattern. According to Geng et  al. [101], if a pattern characterizes more informa-\ntion in the relevant dataset, it tends to be more useful and interesting. Traditional clas-\nsification techniques consider data-driven generalization while producing classification \nrules. Besides this, users’ behavior-oriented generalization might be interested for learn-\ning context-aware rules. For instance, users’ behavior might be similar for a collection \nof contexts and have exceptions only in few cases [6]. Thus, users’ behavior oriented \n\n\n\nPage 13 of 25Sarker J Big Data (2019) 6:95 \n\ngeneralization could give more precise results for modeling their usage behavior. The \ngeneralization not only simplifies the resultant machine learning based model but also \nminimizes the over-fitting problem and improves the prediction accuracy.\n\nIn a summary, by taking into account the reliability, flexibility, and generalization dis-\ncussed above, typical classification rule learning techniques may not be suitable to pro-\nduce users’ behavioral rules in multi-dimensional contexts, for the purpose of building \nintelligent context-aware systems.\n\nIncremental learning and updating\n\nIn the area of data mining, a number of updating techniques, known as incremental rule \nmining, have been proposed for discovering rules in a dynamic database. These tech-\nniques use existing discovered rules and the incremental part of the dataset to get a com-\nplete updated set of rules. For instance, FUP algorithm proposed by Cheung et al. [104] \nis the first incremental updating technique for maintaining association rules when new \ndata are inserted to database. The FUP algorithm is based on Apriori [80] algorithm and \nis used to discover the new frequent itemsets in a dynamic database. In [105], Cheung \net al. propose a new algorithm FUP2 which is an extension of FUP algorithm. Another \nincremental association rule mining algorithm is proposed by Xu et al. [106]. They pro-\npose an IFP-tree technique which is an extension of FP-tree [85]. Thomas et  al. [107] \npropose an algorithm based on the concept of negative border which maintains both \nfrequent itemsets and border itemsets.\n\nA few number of algorithms [108, 109] are proposed based on three-way decision that \nis an extension of the commonly used binary-decision model with an added third option. \nA theory of three-way decision is constructed based on the notions of acceptance, rejec-\ntion and no commitment proposed by Yao et al. [110]. In [111], Amornchewin et al. pro-\npose a probability-based incremental association rule discovery algorithm. Thusaranon \net  al. [112] propose another probability-based incremental association rule discovery \nalgorithm that is an extension work of the algorithm introduced by Amornchewin and \nKreesuradej [111].\n\nThe above incremental mining techniques mainly take into account the faster process-\ning, e.g., efficiency, of overall mining process. While processing, these techniques reduce \nthe scanning on the given datasets by mining the incremental part separately, instead of \nprocessing the merged dataset that includes the initial dataset and the incremental part. \nThus, the overall mining process of such traditional updating techniques reflects on the \nprocessing time to discover a complete set of updated rules. However, to model users’ \nbehavior the freshness of rules, e.g., rules based on recent patterns are significant, which \nhas not been taken into account in these techniques. The reason is that users’ behav-\nior are not static in the real world; may change over time. Thus, the updation in terms \nof freshness in users’ behavior while producing rules are needed to effectively modeling \nsmartphone users’ behavior in relevant multi-dimensional contexts.\n\nIn order to produce rules according to the current behavior of an individual, a number \nof researchers use the behavioral patterns of recent mobile phone log data to predict \nthe future behavior than the patterns derived from the entire historical logs. However, \nthey use a static period of recent historical data that might not meaningful for discover-\ning users’ recent behavioral rules. For instance, Lee et al. [113] have studied the mobile \n\n\n\nPage 14 of 25Sarker J Big Data (2019) 6:95 \n\nphone users’ calling patterns and design a call recommendation algorithm for an adap-\ntive speed-call list using a recent call list data. In their approach, they extract call logs \nfor previous three months to achieve their goal. In order to predict the outgoing calls, \nBarzaiq et al. [114] propose an approach that analyzes mobile phone historical data from \na period of two years and observe relatively additional computational load which seems \nto be unnecessary. Phithakkitnukoon et  al. [115], conduct their study on reality min-\ning datasets that were collected over the period of nine months and observe that only \na recent portion of communication history is more significant. In another work, Phith-\nakkitnukoon et al. [116] present a model for predicting phone calls for the next twenty \nfour hours based on the users’ past communication history. In their approach, they \nhave shown that the recent trend of the user’s calling pattern is more significant than \nthe order one and has higher correlation to the future pattern than the pattern derived \nfrom the entire historical data. As such, the latest sixty days call records in the call logs \nare assumed to be the future observed call activities in order to get better prediction \naccuracy [116]. However, such static period of time consideration may not be suitable to \nreflect one’ current behavior, as users’ behaviors are not consistent in the real world; may \nvary from user-to-user over time.\n\nBesides these approaches, a number of authors [117, 118] deal with the problem of \nmanaging personal information, such as individual’s contact lists in their mobile phone, \nmore specifically, the task of searching the desirable contact number when making an \noutgoing call. According to Bergman et al. [117], a number of contacts in mobile phones \nare never actually used albeit the contact lists become increasingly bigger. Their experi-\nmental results show that 47% of the contacts of the users had not been used for over \nsix months or had never been used at all. To predict future behavior, Stefanis et al. [118], \nhave used window based model for managing and searching of personal information \non mobile phones. In their experiment, they have shown that the training window for \npredicting individual’s mobile phone usages behavior should be long enough to provide \nsufficient data. However, at the same time, a training window of more than two weeks \nwould likely fail to capture the dynamic changes in the behavioral patterns for making \nphone calls. In Addition, a training window of less than seven days would fail to capture \nthe behavioral changes for all the days-of-the-week including a change of social circum-\nstances in the weekends.\n\nIn a summary, by taking into account the freshness in rules reflecting users’ current \nbehavior and their dynamic updation, typical updating techniques discussed above may \nnot be suitable to produce a complete set of users’ behavioral rules in multi-dimensional \ncontexts, for the purpose of building intelligent context-aware systems, in order to pro-\nvide relevant services to the end smartphone users.\n\nChallenges and future directions\nWith the rapid development of smartphones, IoT, data science and machine learning, \nand context-aware computing, the most fundamental challenge is to explore contex-\ntual data collected from relevant sources and to extract context-aware rules for future \nactions. We highlight and analyze the main challenges in extracting rules, machine \nlearning techniques, and context-aware system areas, involved in context-aware rule \nlearning. We also discuss about the future directions to overcome such issues. Thus, this \n\n\n\nPage 15 of 25Sarker J Big Data (2019) 6:95 \n\nsection examines the impact of learning context-aware rules on several perspectives dis-\ncussed in this paper, in the broad area of smartphone data analytics. In the following, the \nchallenges and corresponding future directions are discussed briefly.\n\nIn the area of context aware computing, a number of approaches exist in order to \nhandle the continuous contextual features like time-series and to develop time-based \ncontext-aware systems. They are mostly designed by taking into account several categor-\nical time periods with a particular interval either equal or unequal, and corresponding \ntemporal rule based system. Although, a static modeling of temporal context is easy to \nunderstand and can be useful to analyze population behavior comparing across users, \na machine learning based data-driven solution could be an effective way. The reason is \nthat we are now in the age of data science and have available real world contextual data-\nsets in time order due to the rapid growing of IoT and smartphones. Thus, time-series \nmodeling becomes an open problem for building a context-aware system. Although, a \nfew number of learning techniques are employed to create data-driven temporal seg-\nments, they can be improved with advanced data analysis like observing variations in \ntemporal patterns, relation with individuals and population behavior, data sparseness in \ntime-series, synchronizing temporal context with multiple data sources etc. Improved \nmachine learning techniques or hybrid methods could give better results for modeling \nsuch continuous contextual data. For instance, a dynamic behavior-oriented aggregation \nalgorithm [10], could produce better time-series segmentation results for the purpose of \nmodeling time-based user behavior. New machine learning solutions by considering the \nabove mentioned perspectives can be designed and developed to process and analyze \nreal-world time-series data, in order to build an intelligent time-based context-aware \nsystem.\n\nIn addition to temporal context, additional dimensions of contexts might have the \nimpact on context-aware system. Although, association analysis and rule-based classi-\nfication analysis are the well known approaches in machine learning to discover rules, \nstill there are some issues to learn context-aware rules using these techniques. For \ninstance, an association learning technique produces a large amount of redundant rules \nthat makes the context-aware system complex and ineffective. On the otherhand, clas-\nsification techniques produce rules for rigid decision making that becomes non-reliable \nin many context-aware cases. Thus, effectively learning rules based on multi-dimen-\nsional contexts becomes another challenge. Although, both the classification and asso-\nciation analysis are well established methods in the area of machine learning, improved \nmachine learning techniques or hybrid methods could give better results for learning \neffective rules based on multi-dimensional contexts. For instance, the problem of redun-\ndancy while generating the association rules can be minimized by taking into account \nthe precedence of contexts [119]. Thus, advanced functionality and their combinations \nin machine learning, like the precedence of contexts, optimum contextual feature selec-\ntion, users’ preference-oriented discovery, generalization, abnormality or  exceptional \ndiscovery etc. could produce more effective rules. New machine learning based solu-\ntions or potential hybrid methods by considering these functionalities can be designed \nand developed to process and analyze real-world contextual data, in order to build a \nrule-based intelligent context-aware system that behaves accordingly.\n\n\n\nPage 16 of 25Sarker J Big Data (2019) 6:95 \n\nIn recent days, rule-based context-aware systems become popular due to the rapid \ngrowing of IoT and smartphones. Some of them are static rule based system particularly \ndesigned and developed according to the current needs. A number of such rule based \ncontext-aware systems are designed by taking into account rules discovered from data \nusing association learning or classification learning techniques. Although, these rule-\nbased systems are capable to provide the relevant services, still there is a lack of system \neffectiveness in terms of prediction accuracy in a human-centric system. In that case, \na machine learning based data-driven solution by taking into account incremental data \nand corresponding learning could be an effective way. The reason is that human behavior \nchanges over time and the most recent pattern is likely to be more significant than older \nones, which can be found from incremental data. Thus, recent patterns based modeling \nbecomes another challenge for building a context-aware system. Although, a number \nof updating techniques are employed in the area of incremental data mining, they can \nbe improved by taking into account the freshness in behavioral analytics for a particu-\nlar context. For instance, a very recent work RecencyMiner [120], could produce bet-\nter prediction results by taking into account recency-based updation for the purpose of \nmodeling user behavior. Thus, new machine learning technique or hybrid learning based \nsolutions by considering advanced functionalities like analyzing dynamic log, behav-\nioral patterns changing, context-aware incremental learning, freshness in rules, can be \ndesigned and developed to build a human-centric intelligent context-aware system that \ntakes into account their recent activities.\n\nThe most important work for intelligent context-aware system is to develop an effec-\ntive framework that supports for learning context-aware rules. Thus, in such a frame-\nwork, we need to consider advanced data analysis based on contexts using machine \nlearning techniques, so that the rule learning framework is capble to resolve these issues. \nThus, a well designed context-aware rule learning framework for contextual smart-\nphone data and the experimental evaluation is a very important direction and a big \nchallenge as well. In a summary, this paper has uncovered several future directions in \nthe field of smartphone data analytics and context-aware rule learning. First, additional \nstudy must be performed on the characteristics of smartphone data in terms of asso-\nciated and relevant contexts, as the context-aware rules depend on surrounding differ-\nent contexts. Second, the scalability and efficacy of existing analytics techniques being \napplied to smartphone data must be empirically examined. Third, new techniques and \nalgorithms or potential hybrid methods are needed to be designed while learning con-\ntext-aware rules, particularly, in terms of time-series modeling, effective rule discovery \nbased on multi-dimensional contexts, and recency-based incremental learning for intel-\nligent decision making utilizing enormous amounts of smartphone data. Fourth, a range \nof empirical evaluation is necessary to measure the effectiveness and efficiency of these \nmachine learning techniques while comparing with existing techniques. Fifth, more \nwork is necessary on how to efficiently model context-aware rules in relevant application \nareas for the purpose of building intelligent context-aware applications.\n\n\n\nPage 17 of 25Sarker J Big Data (2019) 6:95 \n\nSuggested machine learning based framework\nAccording to the survey of contextual smartphone data and corresponding rule learn-\ning strategies, in this section, we suggest a context-aware rule learning framework based \non machine learning techniques. Figure 2 shows an overview of the suggested context-\naware rule learning framework highlighting various components starting from the bot-\ntom raw contextual data to real world applications and services. The framework typically \nconsists of four processing layers such as contextual data acquisition layer, context dis-\ncretization layer, rule discovery layer, and finally dynamic updating and management \nlayer, shown in Fig. 2. In the following, we briefly discuss about these layers and their \nroles in learning context-aware rules from smartphone data. These are:\n\nContextual data acquisition This represents the first layer of our context-aware rule \nlearning framework as collecting relevant data is the first step to build a data-driven sys-\ntem. Thus, this layer is responsible to collect individual’s smartphone data that includes \ntheir daily life activities with their phones and corresponding associated contextual \ninformation such as temporal context, spatial context, social context or others relevant \nto the particular usage. Such contextual data can be collected from various sources like \nsmartphone logs, sensors or external sources relevant to the application. Smartphone \ndata collected from these sources usually contains raw contexts that characterize indi-\nviduals’ daily life behavioral activities with their phones, and need to process effectively \nto use as the basis for learning context-aware rules.\n\nFig. 2 An overview of the suggested machine learning based context-aware rule learning framework\n\n\n\nPage 18 of 25Sarker J Big Data (2019) 6:95 \n\nContext discretization Machine learning based context discretization represents the \nsecond layer in our context-aware rule learning framework. Once we have available con-\ntextual raw data collected from the data acquisition layer, discretization of contexts is \nneeded to understand the actual meaning of data, which is also known as contextual data \nclustering, highlighted in Fig. 2. In other words, contextual data with similar character-\nistics are grouped in one cluster and dissimilar characteristics are grouped in another \ncluster. For instance, real-world smartphone data contains continuous raw contextual \ninformation like time-series data that represents individual’s diverse activities in differ-\nent data points in time order. Such particular data points separately may not represent \na meaningful behavior of users. A data-driven time-series segmentation using machine \nlearning techniques could give an effective discretization results according to the data \npatterns available in the source. Thus, the main purpose of this layer is to create contex-\ntual data clusters, e.g., segments or contextual groups according to similar data charac-\nteristics. The processed data in this layer helps to find the hidden patterns that are used \nas the basis of learning context-aware rules.\n\nRule discovery Machine learning based contextual rule discovery represents the third \nlayer in our context-aware rule learning framework, shown in Fig. 2. As different con-\ntexts might have different impacts on individuals’ usage behavior in the real world, the \nprecedence analysis of contexts can play a role to discover a set of effective rules, high-\nlighted in Fig. 2. Based on the precedence of contexts, this layer is responsible to gen-\nerate a set of users’ behavioral rules by taking into account relevant multi-dimensional \ncontexts such as temporal context, spatial context, social context or others relevant. The \ngenerated behavioral rules are effective and efficient in terms of reliability that repre-\nsents higher decision making accuracy, conciseness by taking into account the generali-\nzation and non-redundancy, context importance by taking into account the precedence \nof contexts, and lower training time by considering the computing resources in individu-\nals’ devices. Thus, this layer is responsible to generate a set of contextual rules based on \nrelevant multi-dimensional contexts by taking into account these aspects. After discov-\nering rules, this layer is also responsible to rank these rules according to their relevancy \nin terms of contexts and rule’s strength.\n\nDynamic updating and management layer Machine learning based dynamic updat-\ning and management of the discovered rules represents the final layer in our con-\ntext-aware rule learning framework, shown in Fig. 2. As individuals’ usage behavior \nare not static in the real world, may change over time, the recency analysis and min-\ning, and corresponding rule updation can play a role to dynamically update the dis-\ncovered rules over time, highlighted in Fig. 2. Based on the recent behavioral patters \nof individuals’ behavior, this layer outputs a set of users’ behavioral rules by taking \ninto account the relevant contexts. The main benefit of this layer is that it takes into \naccount the most recent pattern that represents the freshness in individuals’ behav-\nior in a particular context, which is likely to be more significant than older ones for \npredicting their future usage. Thus, this layer is one of the significant layers that is \nresponsible to identify the behavioral changing patterns over time, and to update \nand manage the rules dynamically according to their changes in behavioral activities.\n\nOverall, our suggested machine learning based framework is responsible to extract \na set of effective behavioral rules of individual mobile phone users based on relevant \n\n\n\nPage 19 of 25Sarker J Big Data (2019) 6:95 \n\nmulti-dimensional contexts utilizing their smartphone data. The extracted context-\naware rules can be used to build various rule-based intelligent systems, in order to \nnot only provide them the target personalized services that may vary from user to \nuser but also the population services in the relevant application areas.\n\nContext‑aware rule based applications\nA context-aware rule based smartphone application represents knowledge in terms of \na set of IF-THEN rules, (i.e., if contexts then user behavioral activities or preferences) \nthat tells what to do or what to conclude in different situations [121] and can act as \na software agent. According to [98], software agent is a new paradigm for developing \nsoftware applications in which an agent is capable of performing autonomous actions \nin a certain environment to achieve it’s goal. The target applications of this research \nare those context-aware personalized applications that have been studied widely in \nthe past few years. For instance, intelligent mobile interruptions management sys-\ntem in one of them. The most popular IoT device, smartphones, are considered to be \n‘always on, always connected’ device and they are always with their users; however \nthe users are not always able to response with the incoming communications because \nof their various day-to-day situations [122]. For this reason, sometimes people are \noften interrupted by incoming phone calls in a working environment [123]. Accord-\ning to the Basex BusinessEdge report [124], the mobile interruptions consume 28% \nof the knowledge worker’s day. It leads to a loss of $700 billion according to Bureau \nof Labor Statistics [125]. In order to manage such interruptions, a number of authors \n[24, 65–67, 126] have studied on static rule based systems. However, the machine \nlearning based context-aware rules can be used to make such system automated and \nintelligent.\n\nSmartphone apps management could be another useful applications for individual \nusers. According to the statistics in Google search, in March 2017, there were 2.8 mil-\nlion apps available at Google Play Store, and 2.2 million apps in the Apple’s App Store. \nThus, its very important to manage such kind of huge amount of available applications. \nMachine learning based context-aware rules can be used to manage these apps accord-\ning to individual’s preferences. Besides apps management, several notifications from dif-\nferent apps are potentially annoying to the users and causes disruptions [7, 127, 128]. \nThe reason is that the users might get irritated for such uninterested phone notifications \n[129, 130]. Thus, machine learning based context-aware rules can also be applicable to \nmanage such notifications intelligently.\n\nSmartphone recommendation system is one of the most", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNTM3LTAxOS0wMjU4LTQucGRm0", "metadata_author": "Iqbal H. Sarker ", "metadata_title": "Context-aware rule learning from smartphone data: survey, challenges and future directions", "metadata_creation_date": "2019-10-30T14:24:16Z", "keyphrases": [ "Context-aware rule", "smartphone data", "future directions", "survey", "challenges" ] }, { "@search.score": 1, "content": "\nLocal regression transfer learning with applications to users’\npsychological characteristics prediction\n\nZengda Guan • Ang Li • Tingshao Zhu\n\nReceived: 3 February 2015 / Accepted: 30 July 2015 / Published online: 14 August 2015\n\n� The Author(s) 2015. This article is published with open access at Springerlink.com\n\nAbstract It is important to acquire web users’ psycho-\n\nlogical characteristics. Recent studies have built computa-\n\ntional models for predicting psychological characteristics\n\nby supervised learning. However, the generalization of\n\nbuilt models might be limited due to the differences in\n\ndistribution between the training and test dataset. To\n\naddress this problem, we propose some local regression\n\ntransfer learning methods. Specifically, k-nearest-neigh-\n\nbour and clustering reweighting methods are developed to\n\nestimate the importance of each training instance, and a\n\nweighted risk regression model is built for prediction.\n\nAdaptive parameter-setting method is also proposed to deal\n\nwith the situation that the test dataset has no labels. We\n\nperformed experiments on prediction of users’ personality\n\nand depression based on users of different genders or dif-\n\nferent districts, and the results demonstrated that the\n\nmethods could improve the generalization capability of\n\nlearning models.\n\nKeywords Local transfer learning � Covariate shift �\nPsychological characteristics prediction\n\n1 Introduction\n\nIn recent decades, people spend more and more time on\n\nInternet, which implies an increasingly important role of\n\nInternet in human lives. To improve online user experience,\n\nonline services should be personalized and tailored to fit\n\nconsumer preference. Psychological characteristics, including\n\nconsistent traits (like personality [1]) and changeable status\n\n(like depression [2, 3]), are considered as key factors in\n\ndetermining personal preference. Therefore, it is critical to\n\nunderstand web user’s personal psychological characteristics.\n\nPersonal psychological characteristics can be reflected\n\nby behaviours. As one type of human behaviour, web\n\nbehaviour is also associated with individual psychological\n\ncharacteristics [4]. With the help of information technol-\n\nogy, web behaviours can be collected and analysed auto-\n\nmatically and timely, which motivates us to identify web\n\nuser’s psychological characteristics through web beha-\n\nviours. Many studies have confirmed that it is possible to\n\nbuild computational models for predicting psychological\n\ncharacteristics based on web behaviours [5, 6].\n\nMost studies build computational models by supervised\n\nlearning, which learns computational models on labelled\n\ntraining dataset and then applies the models on another\n\nindependent test dataset. Supervised learning assumes that\n\nthe distribution of the training dataset should be identical to\n\nthat of test dataset. However, the assumption might not be\n\nsatisfied in many cases, e.g. demographic variation (e.g.\n\nZ. Guan\n\nBusiness School, Shandong Jianzhu University, Jinan, China\n\ne-mail: guanzengda@sdjzu.edu.cn\n\nA. Li\n\nDepartment of Psychology, Beijing Forestry University, Beijing,\n\nChina\n\nA. Li\n\nBlack Dog Institute, University of New South Wales, Sydney,\n\nAustralia\n\ne-mail: ang.li@blackdog.org.au\n\nT. Zhu (&)\n\nInstitute of Psychology, Chinese Academy of Sciences, Beijing,\n\nChina\n\ne-mail: tszhu@psych.ac.cn\n\nT. Zhu\n\nInstitute of Computing Technology, Chinese Academy of\n\nSciences, Beijing, China\n\n123\n\nBrain Informatics (2015) 2:145–153\n\nDOI 10.1007/s40708-015-0017-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40708-015-0017-z&amp;domain=pdf\n\n\nvariation of gender and district), which results in the low\n\nperformance of trained models. Previous studies have paid\n\nlittle attention to this problem. In this paper, we build\n\nmodels based on an innovative approach, which do not\n\nneed to make the assumption of identical distribution.\n\nTransfer learning, or known as covariate shift, is intro-\n\nduced and investigated for this purpose.\n\nMost existing covariate shift methods compute the\n\nresampling weight of training dataset and then train a\n\nweighted risk model to predict on test dataset. Commonly,\n\nthese researches use the entire dataset to reweight in the\n\nwhole procedure. We notice that probability density of data\n\npoints is similar to each other in their local neighbour\n\nregion, and this motivates us to use only the local region\n\ninstead of the whole dataset to improve prediction accuracy\n\nand save computation cost. Therefore, we bring in some\n\nlocal learning views to improve covariate shift. In addition,\n\nthe situation can be encountered that people do not know\n\nany labels of the test dataset before they decide to predict\n\nthem, so it is difficult to learn the parameters of learning\n\nmodel. To cope with this problem, we propose an adaptive\n\nparameter-setting method which needs no test dataset label.\n\nBesides, we focus on the regression form of local transfer\n\nlearning since psychological characteristics labels are often\n\nused in the form of continual values.\n\nIn this paper, based on our previous work [7], we intend to\n\nwork on more domains of psychological characteristics pre-\n\ndictions and propose some new local regression transfer\n\nlearning methods, including training-test k-NN method and\n\nadaptive k-NN methods, which are more effective and can\n\nadaptively set the unknown parameter in prediction functions.\n\nThe rest of the paper is organized as follows: we present\n\nthe local regression transfer learning methods in Sect. 2;\n\nwe then introduce the background of covariate shift and\n\nlocal learning, and propose some local transfer learning\n\nmethods to reweight the training dataset and build the\n\nweighted risk regression model. We perform some exper-\n\niments of psychological characteristics prediction and\n\nanalyse the experiment results in Sect. 3. Finally, we\n\nconclude the whole work in the last section.\n\n2 Local regression transfer learning\n\n2.1 Covariate shift\n\nIn this paper, the input dataset is denoted by X and its labels\n\nare denoted by Y. The training dataset is defined as Ztr ¼\nfðxð1Þtr ; y\n\nð1Þ\ntr Þ; :::; ðxðntrÞ\n\ntr ; y\nðntrÞ\ntr Þg � X � Y with a probability\n\ndistribution PtrðX; YÞ, and the test dataset is defined as\n\nZte ¼ fðxð1Þte ; y\nð1Þ\nte Þ; :::; ðxðnteÞ\n\nte ; y\nðnteÞ\nte Þg � X � Y with a proba-\n\nbility distribution PteðX; YÞ.\n\nIt is quite often that the test dataset has a different distri-\n\nbution from the training dataset. We focus on simple covariate\n\nshift that only inputs of the training dataset and inputs of\n\nthe test dataset follow different distributions, i.e. only\n\nPtrðXÞ 6¼ PteðXÞ, while anything else does not change [8].\n\nThen, we will introduce a general solution framework to\n\ncope with covariate shift problems. The key point is to\n\ncompute probability of training data instances within the\n\ntest dataset population, so that people can use labels of the\n\ntraining dataset to learn a test dataset model. We illustrate\n\nthe process as [9, 10] did.\n\nFirstly, we represent the risk function in this situation\n\nand minimize its expected risk:\n\nmin\nh\n\nEðxtr;ytrÞ�Pte\nlðxtr; ytr; hÞ ; ð1Þ\n\nwhere lðxtr; ytr; hÞ is the loss function, which depends on an\n\nunknown parameter h, and ðxtr; ytrÞ�Pte denotes the\n\nprobability with which ðxtr; ytrÞ belongs to test dataset\n\npopulation.\n\nIt is usually difficult to compute the distribution of Pte, so\n\npeople turn to compute the empirical risk form as follows:\n\nmin\nh\n\nEðx;yÞ�Ptr\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ\n\n� min\nh\n\n1\n\nntr\n\nXntr\n\ni¼1\n\nPteðxtr; ytrÞ\nPtrðxtr; ytrÞ\n\nlðxtr; ytr; hÞ:\nð2Þ\n\nIt is usually assumed that PtrðyjxÞ ¼ PteðyjxÞ, i.e. the pre-\n\ndiction functions for both datasets are identical. Then,\nPteðxtr;ytrÞ\nPtrðxtr;ytrÞ is replaced by\n\nPteðxtrÞ\nPtrðxtrÞ. People usually directly com-\n\npute the ratio\nPteðxtrÞ\nPtrðxtrÞ but do not estimate Ptr and Pte inde-\n\npendently, which can avoid generating more errors.\n\nTo estimate the ratio\nPteðxtrÞ\nPtrðxtrÞ , also called the importance,\n\nresearchers construct many kinds of forms of formula 2.\n\nSugiyama et al. [11] computed the importance by mini-\n\nmizing the Kullback–Leibler divergence between training\n\nand test input densities and constructed the prediction\n\nmodel with a series of Gaussian kernel basis functions.\n\nKanamori et al. [12] proposed a method which minimizes\n\nsquares importance biases represented by Gaussian kernel\n\nfunctions centred at test points. Huang et al. [10] used a\n\nkernel mean matching method (KMM) which computed\n\nthe importance by matching test and training distributions\n\nin a reproducing-kernel Hilbert space. Dai et al. [13] and\n\nPardoe et al. [14] proposed a list of boosting-based algo-\n\nrithms for transfer learning.\n\n2.2 Local machine learning\n\nLocal machine learning has shown a comparative advan-\n\ntage in many machine learning tasks [15–17]. In some\n\nsituations, the size of local region of target data imposes a\n\n146 Z. Guan et al.\n\n123\n\n\n\nsignificant effect on prediction accuracy of model [17]. On\n\nthe one hand, too many neighbour points can over-estimate\n\nthe effects of long-distance points which may have little\n\nrelationship with target point. Thus, this may bring\n\nunnecessary interferences to learning process and produce\n\nmore computation cost. In another way, the predicted data\n\npoint can be thought to have similar property only to points\n\nin its small region but not to all points in a very big region.\n\nOn the other hand, too less neighbour points may introduce\n\nstrong noise to local learning.\n\nFor covariate shift, density estimation is important.\n\nThere are many density estimation methods including k-\n\nnearest-neighbour methods, histogram methods and kernel\n\nmethods, which are localized with only a small proportion\n\nof all points which contribute most to the density estima-\n\ntion of a given point [18]. The k-nearest-neighbour\n\napproximation method is represented as follows:\n\nPðxÞ ¼ k\n\nnV\n; ð3Þ\n\nwhere k is the number of nearest neighbours, n is the total\n\nnumber of all data and V is the region volume containing\n\nall nearest neighbours. If the training and test data are in\n\none volume, ratio between densities of both can be repre-\n\nsented as ktr=kte, which do not require to compute nV any\n\nmore. Moreover, Loog [19] proposed a local classification\n\nmethod which estimated the importance by using the\n\nnumber of test data falling in its neighbour region which\n\nconsisted of training and test data. All of these inspired us\n\nto further study local learning within covariate shift.\n\n2.3 Reweighting the importance\n\nA complete covariate shift process is divided into two\n\nstages: reweighting importance of training data, and\n\ntraining a weighted machine learning model for prediction\n\non the test dataset. In the first stage, we reweight the\n\nimportance of training instances by estimating the ratio\n\nPteðxtrÞ=PtrðxtrÞ.\nIn this work, we use local learning to improve the per-\n\nformance in covariate shift. The key point is to use the\n\nneighbourhood of training points to compute their impor-\n\ntance. In fact, this uses the knowledge of density similarity\n\nbetween the training point and its neighbour points.\n\nK-nearest-neighbour and clustering methods are used to\n\ndetermine the neighbourhood of training point and\n\nreweight the importance. Specifically, we first present k-\n\nNN reweighting method, which is simplest and can be seen\n\nas an origin form of all our k-NN methods. Training-test K-\n\nNN reweighting method is an extension of k-NN\n\nreweighting method, and adaptive K-NN reweighting\n\nmethod is an adaptation of training-test K-NN reweighting\n\nmethod to more common situations. Clustering-based\n\nreweighting method is another view about using local\n\nlearning to reweight the importance.\n\n2.3.1 K-NN reweighting method\n\nWe firstly introduce k-nearest-neighbour reweighting\n\nmethods [7], which uses k-nearest test set neighbours of\n\ntraining instance to compute its importance. Gaussian\n\nkernel is chosen to compute density distance between\n\ntraining data and test data. Then the importance can be\n\ncomputed as follows:\n\nWeigðxtrÞ ¼\nXk\n\ni¼1\n\nexp �cjjxtr � x\nðiÞ\nte jj22\n\n� �\n; ð4Þ\n\nwhere k represents the number of the nearest test set\n\nneighbours of training data xtr, which determines the size of\n\nthe local region, and c reflects the bandwidth of kernel\n\nfunction and c[ 0. Even though the exponential term in\n\nWeigðxtrÞ decreases according to an exponential law, the\n\nk value is helpful for obtaining an appropriate neighbour\n\nregion and then computing the importance. It is easy to\n\nknow that this k-nearest-neighbour reweighting method can\n\nsave much computation time when the size of dataset is\n\nvery large compared with k.\n\n2.3.2 Training-test K-NN reweighting method\n\nWhen we regard both the training and test neighbours of\n\ngiven training data in a local region, we develop a new k-\n\nnearest-neighbour reweighting method, called training-test\n\nk-NN reweighting method, which uses both training data\n\nand test data. The training-test k-NN reweighting method\n\ntries to use more training data points to balance the effect\n\nwhich is due to that the only training point does not have\n\ncomparable probability with the other test points in the k-\n\nNN reweighting method sometimes, which may reduce the\n\nperformance of the k-NN method. Simply, ktr=kte can be\n\nused as a reweighting formula if the training data and test\n\ndata in the local region are treated to have similar proba-\n\nbility. Further, we put forward the below formula to\n\ncompute the importance after combining Gaussian kernels.\n\nWeigðxtrÞ ¼\n1\nkte\n\nPkte\n\ni¼1 expð�cjjxtr � x\nðiÞ\nte jj22Þ\n\n1\nktr\n\nPktr\n\nj¼1 expð�cjjxtr � x\nðjÞ\ntr jj22Þ\n\n; ð5Þ\n\nwhere the neighbour region divides into two parts: the\n\ntraining data part with a total number of ktr and the test data\n\npart with a total number of kte. The total number of data in\n\nthe neighbour region is k ¼ ktr þ kte. When we determine\n\nthe k, ktr and kte will be determined automatically. Here,\n\nsince the training point itself is also defined as its neigh-\n\nbour, the denominator cannot be 0.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 147\n\n123\n\n\n\n2.3.3 Adaptive K-NN reweighting method\n\nFor covariate shift methods, how to determine appropriate\n\nparameters is an important issue. Cross validation tech-\n\nnique is used broadly for the problem. However, cross\n\nvalidation technique needs some labelled test data to be as\n\nvalidation dataset. When the prediction model is used in\n\nchanged situation where test data are completely not\n\nlabelled, people cannot apply cross validation. Here, we\n\ngive an empirical parameter estimation way to modify the\n\ntraining-test k-NN reweighting method. We call it adaptive\n\nk-NN reweighting method, which includes how to deter-\n\nmine k and how to determine c.\n\nFor k, we first assign k � n\n3\n8 in the way of Enas and Choi\n\n[20], where n is the population size. Then we reduce k to be\n\na smaller value nneig when Gaussian kernel function ratio\n\ngauðnneig þ 1Þ=gauðnneigÞ is less than a threshold, which\n\nmakes data in the region have similar probability. gau(i) is\n\ndefined as expð�cjjxtar � xðiÞjj22Þ. The reason is that, if a too\n\nsmall value gau(i) of nearest-neighbour point i is summed\n\nto compute the density together with other big values, that\n\nwould bring big bias, and thus the point should be gotten\n\nrid of.\n\nAs to the parameter c, we set it as an empirical way\n\nc ¼ 1\n2nneig\n\nPnneig\n\ni¼1 jjxtr � xðiÞjj22Þ. In fact, this way is somehow\n\nlike a way of computing an approximated empirical vari-\n\nance of a dataset.\n\n2.3.4 Clustering-based reweighting method\n\nFinally, we introduce clustering-based reweighting meth-\n\nods [7], which are somehow similar to data-adaptive his-\n\ntogram method [18]. This kind of methods use clustering\n\nalgorithm to generate histograms, whereas it uses training\n\nand test instances in one histogram to estimate the impor-\n\ntance. In detail, clustering is performed on the whole\n\ntraining and test dataset, and PteðxtrÞ=PtrðxtrÞ is estimated\n\nthrough computing the ratio between number of test data\n\nand number of training data in one cluster. The idea is\n\nsimple that training data and test data clustered in one\n\nsmall enough region can be thought to have the equal\n\nprobability and then the importance can be computed with\n\nthe ratio. Thus, we obtain the formula of clustering-based\n\nreweighting method as follows:\n\nWeigðxðiÞtr Þ ¼\njClusteðxðiÞtr Þj\njClustrðxðiÞtr Þj\n\n; ð6Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training data\n\nx\nðiÞ\ntr , and jClustrðxðiÞtr Þj and jClusteðxðiÞtr Þj denote, respectively,\n\nthe number of training data and the number of test data in\n\nthe same cluster which contains x\nðiÞ\ntr .\n\nLike the histogram method, this method may suffer from\n\nhigh-dimensional difficulty. Number of training data and\n\ntest data in their cluster affects the probability estimation,\n\nand it needs very many data in high-dimensional situation.\n\nClustering method also has a big influence on risk of\n\nimportance weighting, because common clustering meth-\n\nods are not accurate density-region division methods.\n\nClustering-based reweighting method can be taken as an\n\napproximate computation way.\n\n2.4 Weighted regression model\n\nWhen we get the importance of all training data in the\n\nprevious stage, we train the weighted learning model and\n\npredict on the test dataset. The importance of training data\n\nis taken as weight of data and is integrated into the fol-\n\nlowing formula:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� l y\n\nðiÞ\ntr ; f x\n\nðiÞ\ntr\n\n� �� �\n; ð7Þ\n\nwhere WeigðxðiÞtr Þ denotes the importance of training\n\ninstances x\nðiÞ\ntr and lðyðiÞtr ; f ðx\n\nðiÞ\ntr ÞÞ represents the bias between\n\nthe real value y\nðiÞ\ntr and the prediction value f ðxðiÞtr Þ which is a\n\nregression function. It can be seen that each instance in the\n\nweighted model has a different weight, while the weight in\n\nunweighted models is uniform.\n\nIn this work, we integrate multivariate adaptive regres-\n\nsion splines (MARS) method with local reweighting\n\nmethods. MARS is an adaptive stepwise regression method\n\n[21], and its weighted learning model has the following\n\nform:\n\nmin\nXntr\n\ni¼1\n\nWeig x\nðiÞ\ntr\n\n� �\n� y\n\nðiÞ\ntr � f x\n\nðiÞ\ntr\n\n� �� �2\n\nf ðxðiÞtr Þ ¼ b0 þ\nXm\n\nj¼1\n\nbjhj x\nðiÞ\ntr\n\n� �\n;\n\nð8Þ\n\nwhere hjðxÞ is a constant denoted by C, or a hinge function\n\nwith the form maxð0; x� CÞ or maxð0;C � xÞ, or a product\n\nof two or more hinge functions. m denotes the total steps to\n\nget optimal performance, and f ðxðiÞtr Þ and f ðxðiÞte Þ denote the\n\nprediction values of training data and test data, respec-\n\ntively. This model is trained for solving unknown coeffi-\n\ncients bj.\n\n3 Experiments\n\nOur experiments aim to predict microblog users’ psycho-\n\nlogical characteristics. They include three parts: predicting\n\nusers’ personality across different genders, predicting\n\n148 Z. Guan et al.\n\n123\n\n\n\nusers’ personality across different districts and predicting\n\nusers’ depression across different genders.\n\nIn this paper, personality is evaluated by the Big Five\n\npersonality framework, a wide accepted personality model\n\nin psychology. The Big Five personality model describes\n\nhuman personality with five dimensions as follows:\n\nagreeableness (A), conscientiousness (C), extraversion (E),\n\nneuroticism (N) and openness (O) [22]. Agreeableness\n\nrefers to a tendency to be compassionate and cooperative.\n\nConscientiousness refers to a tendency to be organized and\n\ndependable. Extraversion refers to a tendency to be\n\nsocialized and talkative. Neuroticism refers to a tendency\n\nto experience unpleasant emotions easily. Openness refers\n\nto the degree of intellectual curiosity, creativity and a\n\npreference for novelty. Besides, CES-T scale [23] is\n\nemployed to measure web users’ depression.\n\nWe test the local transfer methods among web users\n\nwith different genders and in different districts. There\n\nexists some relationship between users’ web behaviours\n\nand their personality/depression. Gender is an important\n\nfactor that can effect users’ behaviours, so we choose it as\n\nexample to test the local transfer methods. It is often\n\nencountered that users of the training set and the test set are\n\nin different districts, so we also study the suitability of the\n\nlocal transfer methods in this situation. Depression in male\n\nand female shows difference [24], so we also investigate it.\n\nIn detail, our experiments are to predict male users’ per-\n\nsonality based on female users, predict non-Guangdong\n\nusers’ personality based on Guangdong users and predict\n\nmale users’ depression degree based on female users.\n\n3.1 Experiment setup\n\nIn China, Sina Weibo (weibo.com) is one of the most\n\nfamous microblog service providers and has more than 503\n\nmillion registered users. In this research, we invited Weibo\n\nusers to complete online self-report questionnaire, includ-\n\ning personality and depression scales, and downloaded\n\ntheir digital records of online behaviours with their\n\nconsent.\n\nFor the prediction of personality, between May and\n\nAugust in 2012, we collected data from 562 participants\n\n(male: 215, female: 347; Guangdong: 175, non-Guang-\n\ndong: 387) and extracted 845 features from their online\n\nbehavioural data. The extracted features can be divided\n\ninto five categories: (a) profiles include features like reg-\n\nistration time and demographics (e.g. gender); (b) self-ex-\n\npression behaviours include features reflecting the online\n\nexpression of one’s personal image (e.g. screen name,\n\nfacial picture and self-statement on personal page);\n\n(c) privacy settings include features indicating the concern\n\nabout individual privacy online (e.g. filtering out pri-\n\nvate messages and comments sent by strangers);\n\n(d) interpersonal behaviours include features indicating the\n\noutcomes of social interaction between different users (e.g.\n\nnumber of friends whom a user follows, number of fol-\n\nlowers, categories of friends whom a user follows and\n\ncategories of forwarded microblogs); and (e) dynamic\n\nfeatures can be represented as time series data (e.g.\n\nupdating microblogs in a certain period or using apps in a\n\ncertain period).\n\nFor the prediction of depression, between May and June\n\nin 2013, we collected data from 1000 participants (male:\n\n426, female: 574). Compared with personality experiments,\n\nwe supplemented additional linguistic features in depres-\n\nsion experiments. These linguistic features included the\n\ntotal number of characters, the number of numerals, the\n\nnumber of punctuation marks, the number of personal\n\npronouns, the number of sentiment words, the number of\n\ncognitive words, the number of perceptual processing\n\nwords and so on.\n\nSince all these experiments have very many feature\n\ndimensions and high dimension curse would weaken the\n\nlearning model, we firstly use stepwisefit method in Matlab\n\ntoolbox to reduce dimensions and select the most relevant\n\nfeatures. For the gender-personality experiment, we pro-\n\ncess the female dataset and obtain 25, 14, 19, 25 and 20\n\nfeatures for predicting Big Five dimensions: A, C, E, N and\n\nO, respectively. For the district-personality experiment, the\n\nGuangdong dataset is processed and we obtain 19, 21, 18,\n\n22 and 20 features for A, C, E, N and O, respectively. For\n\nthe depression experiment, the female dataset is processed,\n\nand we obtain 20 features.\n\nIt also must be emphasized that we test whether the\n\ntraining set and the test set follow the same distribution\n\nbefore we do transfer learning. Both T test and Kol-\n\nmogorov–Smirnov test are performed in the two-sample\n\ntest. T test is fit to test dataset with Gaussian distribution,\n\nand Kolmogorov–Smirnov test can test dataset with\n\nunknown distribution. Specifically, we test the datasets\n\nalong each dimension.\n\nIn the experiments, our local transfer learning methods\n\nare compared with non-transfer method, global transfer\n\nmethod and other transfer learning methods. The local\n\ntransfer learning methods include k-NN transfer learning\n\nmethod, training-test k-NN transfer learning method,\n\nadaptive k-NN transfer learning methods and clustering\n\ntransfer learning methods. The non-transfer method does\n\nnot use a transfer learning way and is a traditional method.\n\nThe global transfer method is also a k-NN transfer learning\n\nmethod, but it has a k value equalling the number of all test\n\ndata, i.e. it takes all test data as neighbours. A famous\n\ntransfer learning method called KMM [10] is also used\n\nhere as a baseline method. After reweighting importance,\n\nwe integrate the importance into weighted risk models. We\n\nchoose weighted risk model MARS, which is open source\n\nLocal regression transfer learning with applications to users’ psychological characteristics 149\n\n123\n\n\n\nregression software for Matlab/Octave from (http://www.\n\ncs.rtu.lv/jekabsons/regression.html).\n\nIn all tables and figures of this paper, MARS denotes the\n\nmethod with no transfer learning, KMM denotes combi-\n\nnation of KMM reweighting method and MARS method in\n\na weighted risk form, GkNN denotes global k-NN\n\nreweighting method and MARS, kNN denotes k-NN\n\nreweighting method and MARS, TTkNN denotes training-\n\ntest k-NN reweighting method and MARS, and AkNN1\n\ndenotes adaptive k-NN reweighting method and MARS,\n\nwhere k value is determined as described in Sect. 2.3.3.\n\nAkNN2 denotes completely adaptive k-NN reweighting\n\nmethod and MARS, where k value and c value are both\n\ndetermined as described in Sect. 2.3.3. Clust denotes\n\nclustering-based reweighting method and MARS. KMM,\n\nGkNN, kNN, TTkNN, AkNN1 and Clust all showed the\n\nbest results where their parameter values are assigned the\n\nbest of a series of tried values. In all experiments, we use\n\nmean square error (MSE) for result comparisons.\n\n3.2 Predicting users’ personality across genders\n\nThis task is to predict male users’ personality based on\n\nfemale users’ labelled data and male users’ unlabelled data.\n\nWe firstly perform single-dimension T test and Kol-\n\nmogorov–Smirnov test to test whether male and female\n\ndatasets are drawn from the same distribution. As a result,\n\n3, 1, 2, 3 and 2 features of all 25, 14, 19, 25 and 20 features\n\nare shown to follow different distributions by T test, and 2,\n\n0, 0, 2 and 1 features by Kolmogorov–Smirnov test. All of\n\nthese test results are with probability more than 95 %\n\nconfidence. Thus, it can be thought that there exists some\n\ndistribution divergence between male and female datasets,\n\nthough the divergence is not big. Then, we examine the\n\nperformance of all the local transfer learning methods in\n\nthis experiment.\n\nFrom Table 1, it can be seen that all regression transfer\n\nlearning methods improve much on the prediction accuracy\n\ncompared with non-transfer learning method in all situa-\n\ntions. Local kNN reweighting methods beat global k-NN\n\nreweighting method GkNN in almost all situations. TTkNN\n\nmethod performs better than the others in 3 of 5 personality\n\ndimensions. AkNN1 performs nearly well with other k-NN\n\nreweighting methods, except in the dimension of C.\n\nEspecially, AkNN1 beats GkNN in 4 dimensions, and this\n\nshows the advantage of its fixed k value. For AkNN2, it\n\nperforms better only than MARS method. Clust also shows\n\ncomparable performance compared with other local trans-\n\nfer learning methods.\n\nTo investigate the impact of k value in k-NN\n\nreweighting methods, we take experiment on trait A as an\n\nexample. The results of GkNN, kNN and TTkNN are\n\nshown in Fig. 1. We can see that these methods perform the\n\nbest when the values of k range between 20 and 30. As\n\nk approximates to the total size of test dataset, the perfor-\n\nmances of kNN and TTkNN become equal to GkNN\n\nmethod. For TTkNN method, it performs worse than GkNN\n\nwhen k is 1, and that could be caused by noise. When k of\n\nTTkNN method is very small, i.e. close to 0, outlier point\n\ncan impose a strong influence. When k of TTkNN method is\n\n50, its performance shows an exception and the reason may\n\nbe that the local region caused by k experiences a shake-up.\n\nThus, the value of k can be recognized as a factor affecting\n\nthe prediction performance.\n\nWe then test how prediction accuracy of clustering\n\ntransfer methods is affected by the number of clusters in all\n\nfive personality traits. From Fig. 2, we can see that the\n\nnumber of clusters has a big influence on the prediction\n\naccuracy. There is no certain value of cluster number\n\nwhich achieves the best performance for all five traits. The\n\nmethod obtains the optimization result in C, E and O trait\n\nwhen the number of clusters is small. For these three traits,\n\nTable 1 Local regression transfer learning results for predicting\n\npersonality across different-gender datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 34.8431 45.9335 34.0655 29.5776 32.6700\n\nKMM 26.7654 30.8683 24.0116 27.9208 28.1425\n\nGkNN 25.2125 31.5119 23.1247 27.6345 30.6127\n\nkNN 24.3776 31.1357 23.1247 27.4160 28.2948\n\nTTkNN 24.3149 31.0282 22.8547 27.8493 28.1424\n\nAkNN1 24.3913 31.2013 24.5649 27.4419 28.2027\n\nAkNN2 29.8956 31.0112 24.0063 27.8779 28.1899\n\nClust 27.3070 30.4555 23.9003 27.7718 28.1425\n\n0 50 100 150 200 250 300\n24\n\n25\n\n26\n\n27\n\n28\n\n29\n\n30\n\n31\n\n32\n\n33\n\n34\n\nk\n\nM\nS\n\nE\n\nGkNN\nkNN\nTTkNN\n\nFig. 1 The impact of the number of nearest neighbours on the\n\nperformance of k-NN transfer methods in trait A\n\n150 Z. Guan et al.\n\n123\n\nhttp://www.cs.rtu.lv/jekabsons/regression.html\nhttp://www.cs.rtu.lv/jekabsons/regression.html\n\n\nit could also be seen that their MSE gradually increases as\n\nnumber of clusters increases, and the least k value (here,\n\nthe value is 1) may not be the optimised value because of\n\nnoise. Meanwhile, it seems to follow no regular rule for the\n\nother two traits. Thus, we can think that there is no constant\n\noptimal value for cluster number in clustering transfer\n\nmethods for all situations. The reasons are speculated that\n\ndistributions of the datasets are of diversity, and clustering\n\nmethod is not a stable density estimation method here.\n\n3.3 Predicting users’ personality across districts\n\nIn this experiment, we use Weibo data of Guangdong\n\nprovince of China to train the model and predict person-\n\nality of users in the other districts. Firstly, we still apply\n\nstepwisefit method to select 19, 21, 18, 22 and 20 features\n\nfrom a total of 845 features in A, C, E, N and O traits,\n\nrespectively. We then use T test and get 3, 1, 3, 3 and 2\n\nfeatures following different distributions and use Kol-\n\nmogorov–Smirnov test and get 3, 5, 6, 9 and 2 features\n\nfollowing different distributions, both with probability\n\nmore than 95% confidence. Finally, we perform our\n\nregression transfer methods on different-district datasets\n\nand compare all the methods as used in the above different-\n\ngender experiment.\n\nWe analyse performances of all methods. Table 2 shows\n\nthat all local transfer learning methods perform better than\n\nnon-transfer method MARS. GkNN behaves unstably: it\n\nperforms worse than MARS in 2 of all 5 traits, while it\n\nperforms best in O trait. kNN performs no worse than\n\nGkNN in all five traits. TTkNN is still the best method for\n\nmost situations and performs stably. AkNN1 performs\n\nmuch better than MARS, but much worse in O trait than\n\nother local transfer learning methods except AkNN2.\n\nAkNN2 behaves only a little better than MARS in four\n\ntraits and weaker in one trait. Clust also beats MARS\n\nmethod in all situations but behaves not so well in O trait.\n\n3.4 Predicting users’ depression across genders\n\nThis experiment is to predict male users’ depression level\n\nbased on female users’ labelled data. Still, stepwisefit\n\nmethod is performed and 20 features are selected. 3 feature\n\ndimensions in T test and 5 feature dimensions in Kol-\n\nmogorov–Smirnov test are thought as different-distribution\n\nfeature. This suggests that training and test data also follow\n\ndifferent distributions in this experiment.\n\nIn Table 3, the result shows that the transfer learning\n\nmethods perform much better than non-transfer method\n\nMARS. KMM and Clust behave a little better than other\n\ntransfer methods. AkNN1 and AkNN2 perform nearly\n\nequally well to other transfer learning methods.\n\n3.5 Discussion and conclusion\n\nIt can be concluded from the above experiments that all our\n\nlocal transfer learning methods work better than non-\n\ntransfer learning method, because they reduce the predic-\n\ntion bias of model which is trained and tested on different-\n\ndistribution datasets. Our local k-NN family transfer\n\nlearning methods perform better than the global k-NN\n\ntransfer learning method generally, and the reason may be\n\nthat an appropriate k value in k-NN methods could reflect\n\nmore subtle nature in density estimation. All our local\n\ntransfer learning methods show comparable performance\n\nwith KMM method in all situations. TTkNN method\n\nexceeds kNN and obtains the best performance among all\n\nthe methods in half of situations. It could be guessed that\n\nTTkNN uses both test and training data information, while\n\nkNN only uses test data. Clust method performs well in\n\n0 20 40 60 80 100 120 140 160 180 200\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\nNumber of Cluster\n\nM\nS\n\nE\n\nA\nC\nE\nN\nO\n\nFig. 2 The impact of cluster number in clustering regression transfer\n\nlearning in trait A, C, E, N and O\n\nTable 2 Local regression transfer learning results for predicting\n\npersonality across different-district datasets. MSE is used to measure\n\nthe test results\n\nCondition A C E N O\n\nMARS 43.6764 65.0172 44.3688 47.4115 229.8742\n\nKMM 42.1194 48.9055 39.3781 47.4057 59.7330\n\nGkNN 44.7136 45.8609 43.0928 49.2114 43.1696\n\nkNN 43.2840 42.0574 38.8104 42.9135 43.1696\n\nTTkNN 38.6335 40.8370 35.0338 41.8510 45.3623\n\nAkNN1 43.5722 41.3360 39.0398 41.4173 195.6540\n\nAkNN2 41.5186 62.8834 39.3683 52.2294 218.9917\n\nClust 39.1079 42.5235 37.7979 44.6171 113.6659\n\nLocal regression transfer learning with applications to users’ psychological characteristics 151\n\n123\n\n\n\nmost situations, this proves its applicability, and better\n\ndensity clustering methods may further enhance this\n\nmethod.\n\nFinally, we compare the performance of GkNN, AkNN1\n\nand AkNN2; AkNN1 is the best, GkNN is the second and\n\nAkNN2 is the worst of them. AkNN1 performs better than\n\nGkNN in most situations, and this demonstrates that\n\ndetermining k in an AkNN1 way, same as AkNN2, can\n\nwork well generally. We also note that AkNN1 and\n\nAkNN2 behave not well in O trait in Table 2, and it\n\nindicates that k in AkNN1 and AkNN2 is not an optimal\n\nchoice in some situation because of the change of distri-\n\nbution of data set. It is pointed that AkNN2 is inferior to\n\nAkNN1 and GkNN, because it does not choose the optimal\n\nvalue for parameter c in prediction function preliminary.\n\nSince no parameter in AkNN2 needs to be set artificially, it\n\ncould work in the situations where we completely have no\n\nidea about labels of predicted data, which can be of much\n\nsignificance.\n\n4 Conclusions\n\nIn this paper, we propose some local regression transfer\n\nlearning methods and apply them to predict users’ psy-\n\nchological characteristics when the training set and the test\n\nset follow different distributions. We present k-NN\n\nreweighting methods and clustering reweighting method to\n\nestimate the importance of training set in covariate shift\n\nprocess. Specifically, these methods utilize training and test\n\ndata in certain local neighbour region for importance\n\nestimations. We still apply them to psychological charac-\n\nteristics predictions including microblog users’ personality\n\nprediction across different genders and different districts,\n\nand microblog users’ depression prediction across different\n\ngenders. The experiments demonstrate that these methods\n\nimprove the accuracy of prediction models. Specially, the\n\ncomplete adaptive k-NN reweighting method is able to\n\nmake prediction even without knowing any label of test\n\ndata.\n\nAcknowledgments The authors gratefully acknowledge the gener-\n\nous support from National Basic Research Program of China\n\n(2014CB744600), National High-tech R&D Program of China\n\n(2013AA01A606), Strategic Priority Research Program (XDA060\n\n30800) and Key Research Program of CAS(KJZD-EW-L04).\n\nOpen Access This article is distributed under the terms of the\n\nCreative Commons Attribution 4.0 International License (http://crea\n\ntivecommons.org/licenses/by/4.0/), which permits unrestricted use, dis-\n\ntribution, and reproduction in any medium, provided you give\n\nappropriate credit to the original author(s) and the source, provide a link\n\nto the Creative Commons license, and indicate if changes were made.\n\nReferences\n\n1. Burger J (2008) Personality, 7th edn. Thomson Higher Education,\n\nBelmont\n\n2. Kessler RC, Angermeyer M, Anthony JC et al (2007) Lifetime\n\nprevalence and age-of-onset distributions of mental disorders in\n\nthe World Health Organization’s World Mental Health Survey\n\nInitiative. World Psychiatry 6(3):168–176\n\n3. Gurland BJ (1992) The impact of depression on quality of life of\n\nthe elderly. Clin Geriatr Med 8:377–386\n\n4. Amichai-Hamburger Y (2002) Internet and personality. Comput\n\nHuman Behav 18:1–10\n\n5. Li L, Li A, Hao B, Guan Z, Zhu T (2014) Predicting active users\n\npersonality based on micro-blogging behaviors. PLoS One\n\n9(1):e84997\n\n6. Zhang F, Zhu T, Li A, Li Y, Xu X (2011) A survey of web\n\nbehavior and mental health. 3rd International symposium of web\n\nsociety (SWS), Port Elizabeth\n\n7. Guan Z, Nie D, Hao B, Bai S and Zhu T (2014) Local regression\n\ntransfer learning for users’ personality prediction. Active media\n\ntechnology\n\n8. Storkey AJ (2009) When training and test sets are different:\n\ncharacterizing learning transfer. In: Quiñonero-Candela J,\n\nSugiyama M, Schwaighofer A, Lawrence N (eds) Dataset shift in\n\nmachine learning. The MIT Press, Cambridge, pp 3–28\n\n9. Pan S, Yang Q (2010) A survey on transfer learning. IEEE Trans\n\nKnowl Data Eng 22(10):1345–1359\n\n10. Huang J, Smola A, Gretton A, Borgwardt K, Scholkopf B (2007)\n\nCorrecting sample selection bias by unlabeled data. In: Pro-\n\nceedings of 19th annual conference neural information processing\n\nsystems\n\n11. Sugiyama M, Nakajima S, Kashima H, Buenau P, Kawanabe M\n\n(2008) Direct importance estimation with model selection and its\n\napplication to covariate shift adaptation. In: Proceedings of 20th\n\nannual conference neural information processing systems\n\n12. Kanamori T, Hido S, Sugiyama M (2008) Efficient direct density\n\nratio estimation for non-stationarity adaptation and outlier\n\ndetection. Adv Neural Inf Processing Syst 20:809–816\n\n13. Dai W, Yang Q, Xue G, Yu Y (2007) Boosting for transfer\n\nlearning. In: Proceedings of the 24th international conference on\n\nmachine learning\n\n14. Pardoe D, Stone P (2010) Boosting for regression transfer. In:\n\nProceedings of the 27th international conference on machine\n\nlearning\n\n15. Holte RC (1993) Very simple classification rules perform well on\n\nmost commonly used data sets. Mach. Learn. 11:63–90\n\n16. Loader C (1999) Local regression and likelihood. Springer, New\n\nYork\n\nTable 3 Local regression transfer learning results for predicting depression across different-gender dataset\n\nMARS KMM GkNN kNN TTkNN AkNN1 AkNN2 Clust\n\nMSE 126.9868 111.2089 113.5784 113.4111 113.1296 113.3482 113.3624 111.5430\n\n152 Z. Guan et al.\n\n123\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n17. Gupta M, Garcia E, Chin E (2008) Adaptive local linear\n\nregression with application to printer color management. IEEE\n\nTrans Image Processing 17:936–945\n\n18. Webb AR (2002) Statistical pattern recognition, 2nd edn. Wiley,\n\nNew York, pp 81–122\n\n19. Loog M (2012) Nearest neighbor-based importance weighting.\n\nIn: IEEE international workshop on machine learning for signal\n\nprocessing, Santander\n\n20. Enas GG, Choi SC (1986) Choice of the smoothing parameter\n\nand efficiency of k-nearest neighbor classification. Comput Math\n\nAppl 12A(2):235–244\n\n21. Hastie T, Tibshirani R, Friedman J (2008) The elements of sta-\n\ntistical learning, 2nd edn. Springer, New York\n\n22. Funder D (2001) Personality. Annu Rev Psychol 52:197–221\n\n23. Radloff LS (1977) The CES-D scale: a self-report depression\n\nscale for research in the general population. App Psychol Meas\n\n1:385\n\n24. Hankin B, Abramson L, Moffitt T, Silva P, McGee R, Angell K\n\n(1998) Development of depression from preadolescence to young\n\nadulthood: emerging gender differences in a 10-year longitudinal\n\nstudy. J Abnormal Psychol 107(1):128–140\n\nZengda Guan received his Ph.D. degree in computer science and\n\napplications from University of Chinese Academy of Sciences,\n\nBeijing, China, in 2014. He is currently a lecturer at the Department\n\nof E-commerce, Shandong Jianzhu University. His research interests\n\nlie in the area of transfer learning and data mining.\n\nAng Li received his Ph.D. degree at the Chinese Academy of\n\nSciences. He received postdoctoral training at the Black Dog Institute,\n\nUniversity of New South Wales and the NHMRC Centre of Research\n\nExcellence in Suicide Prevention (CRESP). Currently, he is the\n\nlecturer at the Department of Psychology, Beijing Forestry Univer-\n\nsity. His research efforts are focused on early detection and\n\nintervention of mental health problems via the Internet (eHealth).\n\nTingshao Zhu earned his second Ph.D. at the University of Alberta\n\nCanada in 2006, and he is now a full Professor at the Institute of\n\nPsychology CAS. He has published over 50 papers in major\n\ninternational academic conferences and journals, and the main foci\n\nof his current work are computational cyberpsychology and data\n\nmining.\n\nLocal regression transfer learning with applications to users’ psychological characteristics 153\n\n123\n\n\n\tLocal regression transfer learning with applications to users’ psychological characteristics prediction\n\tAbstract\n\tIntroduction\n\tLocal regression transfer learning\n\tCovariate shift\n\tLocal machine learning\n\tReweighting the importance\n\tK-NN reweighting method\n\tTraining-test K-NN reweighting method\n\tAdaptive K-NN reweighting method\n\tClustering-based reweighting method\n\n\tWeighted regression model\n\n\tExperiments\n\tExperiment setup\n\tPredicting users’ personality across genders\n\tPredicting users’ personality across districts\n\tPredicting users’ depression across genders\n\tDiscussion and conclusion\n\n\tConclusions\n\tAcknowledgments\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNzA4LTAxNS0wMDE3LXoucGRm0", "metadata_author": "Zengda Guan", "metadata_title": "Local regression transfer learning with applications to users’ psychological characteristics prediction", "metadata_creation_date": "2015-08-12T10:56:23Z", "keyphrases": [ "Local regression transfer learning", "psychological characteristics prediction", "applications" ] }, { "@search.score": 1, "content": "\nSänger et al. Journal of Trust Management (2015) 2:5 \nDOI 10.1186/s40493-015-0015-3\n\nRESEARCH Open Access\n\nReusable components for online reputation\nsystems\nJohannes Sänger*, Christian Richthammer and Günther Pernul\n\n*Correspondence:\njohannes.saenger@wiwi.\nuni-regensburg.de\nUniversity of Regensburg,\nUniversitätsstraße 31, 93053\nRegensburg, Germany\n\nAbstract\n\nReputation systems have been extensively explored in various disciplines and\napplication areas. A problem in this context is that the computation engines applied by\nmost reputation systems available are designed from scratch and rarely consider well\nestablished concepts and achievements made by others. Thus, approved models and\npromising approaches may get lost in the shuffle. In this work, we aim to foster reuse in\nrespect of trust and reputation systems by providing a hierarchical component\ntaxonomy of computation engines which serves as a natural framework for the design\nof new reputation systems. In order to assist the design process we, furthermore,\nprovide a component repository that contains design knowledge on both a\nconceptual and an implementation level. To evaluate our approach we conduct a\ndescriptive scenario-based analysis which shows that it has an obvious utility from a\npractical point of view. Matching the identified components and the properties of trust\nintroduced in literature, we finally show which properties of trust are widely covered by\ncommon models and which aspects have only rarely been considered so far.\n\nKeywords: Trust; Reputation; Reusability; Trust pattern\n\nIntroduction\nIn the last decade, trust and reputation have been extensively explored in various disci-\nplines and application areas. Thereby, a wide range of metrics and computation methods\nfor reputation-based trust has been proposed. While most common systems have been\nintroduced in e-commerce, such as eBay’s reputation system [1] that allows to rate sell-\ners and buyers, considerable research has also been done in the context of peer-to-peer\nnetworks, mobile ad hoc networks, social networks or ensuring data accuracy, relevance\nand quality in several environments [2]. Computation methods applied range from sim-\nple arithmetic over statistical approaches up to graph-based models involving multiple\nfactors such as context information, propagation or personal preferences. A general prob-\nlem is that most of the newly introduced trust and reputation models use computation\nmethods that are designed from scratch and rely on one novel idea which could lead to\nbetter solutions [3]. Only a few authors build on proposals of others. Therefore, approved\nmodels and promising approaches may get lost in the shuffle.\nIn this work, we aim to encourage reuse in the development of reputation systems by\n\nproviding a framework for creating reputation systems based on reusable components.\nDesign approaches for reuse have been given much attention in the software engineering\n\n© 2015 Sänger et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nmailto: johannes.saenger@wiwi.uni-regensburg.de\nhttp://creativecommons.org/licenses/by/4.0\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 2 of 21\n\ncommunity. The research in trust and reputation systems could also profit from ben-\nefits like effective use of specialists, accelerated development and increased reliability.\nToward this goal, we propose a hierarchical taxonomy for components of computation\nengines used in reputation systems. Thereto, we decompose the computation phase of\ncommon reputation models to derive single building blocks. The classification based on\ntheir functions serves as a natural framework for the design of new reputation systems.\nMoreover, we set up a component repository containing artifacts on both a conceptual\nand an implementation level to facilitate the reuse of the identified components. On the\nconceptual level, we describe each building block as a design pattern-like solution. On\nthe implementation level, we provide already implemented components by means of web\nservices.\nThe rest of this paper is based on the design science research paradigm involving the\n\nguidelines for conducting design science research by Hevner et al. [4] and organized as\nfollows: Firstly, we give an overview of the general problem context as well as the relevance\nand motivation of our work. Thereby, we identify the research gap and define the objec-\ntives of our research. In the following section, we introduce our hierarchical component\ntaxonomy of computation engines used in reputation systems. After that, we point out\nhow our component repository is conceptually designed and implemented. Subsequently,\nwe carry out a descriptive scenario-based analysis of our approach. At the same time, we\nmatch all components identified with the properties of trust introduced in literature. We\nshow which properties of trust are widely covered by common models and which aspects\nhave only rarely been considered so far. Finally, we summarize the contribution and name\nour plans for future work.\n\nProblem context andmotivation\nWith the success of the Internet and the increasing distribution and connectivity, trust\nand reputation systems have become important artifacts to support decision making in\nnetwork environments. To impart a common understanding, we firstly provide a defi-\nnition of the notion of trust. At the same time, we explain the properties of trust that\nare important with regard to this work. Then, we point out how trust can be established\napplying computational trust models. Focusing on reputation-based trust, we explain how\nand why the research in reputation models could profit from reuse. Thereby, we identify\nthe research gap and define the objectives of this work.\n\nThe notion of trust and its properties\n\nThe notion of trust is a topic that has been discussed in research for decades. Although\nit has been intensively examined in various fields, it still lacks a uniform and generally\naccepted definition. Reasons for this circumstance are the multifaceted terms trust is\nassociated with like credibility, reliability or confidence as well as the multidimension-\nality of trust as an abstract concept that has a cognitive, an emotional and a behavioral\ndimension. As pointed out by [5], trust has been described as being structural in\nnature by sociologists while psychologists viewed trust as an interpersonal phenomenon.\nEconomists, however, interpreted trust as a rational choice mechanism. The definition\noften cited in literature regarding trust and reputation online that is referred to as relia-\nbility trust was proposed by Gambetta in 1988 [6]: “Trust (or, symmetrically, distrust) is\na particular level of the subjective probability with which an agent assesses that another\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 3 of 21\n\nagent or group of agents will perform a particular action, both before he can monitor such\naction (or independently of his capacity ever to be able to monitor it) and in a context in\nwhich it affects his own action.”\nMultiple authors furthermore include security and risk which can lead to more com-\n\nplex definitions. Anyway, it is generally agreed that trust is multifaceted and dependent\non a variety of factors. Moreover, there are several properties of trust described in lit-\nerature (see Table 1). These properties are important with respect to this work because\nthey form the basis for many applied computation techniques in trust and reputation\nsystems described in Section ‘Hierarchical component taxonomy’. Reusable components\ncould extend current models by the ability to gradually include these properties.\n\nReputation-based trust\n\nIn recent years, several trust models have been developed to establish trust. Thereby,\ntwo common ways can be distinguished, namely policy-based and reputation-based trust\n\nTable 1 Overview of properties of trust described in literature [14,41-46]\n\nDynamic Trust can increase or decrease through gathering new experiences. Moreover,\ntrust is said to decay with time (time-based aging [45]). Because of these char-\nacteristics, trust values strongly depend on the time they are determined. The\ngreater importance of new experiences compared to old experiences has been\nwidely studied and considered in many trust models such as [32,47] or [30].\n\nContext-dependent Trust is bound to a specific context. For example, Alice trusts Bob as her doctor.\nHowever, she might not trust him as a cook to prepare a delicious meal for her.\n\nMulti-faceted Even in the same context, a trust value may not reflect all aspects of this context\n[43]. For example, a customer may trust a particular restaurant for its quality of\nfood but not for its quality of service. The overall trust on this restaurant depends\non the combination of the amount of trust in the specific aspects.\n\nPropagative One property of trust made use of in several models is its propagativity. If Alice\ntrusts Bob, who in turn trusts Claire, Alice can derive trust on Claire from the rela-\ntionships between her and Bob as well as between Bob and Claire. Because of\nthis propagative nature, it is possible to create trust chains passing trust from\none agent to another agent. As clarified by Christianson and Harbison [48], trust\nis not automatically transitive although trust transitivity was assumed proven for\na long time. If Alice trusts Bob, who in turn trusts Claire, it does not inherently\nmean that Alice trusts Claire. It follows from the foregoing that transitivity implies\npropagation. The reverse, though, is not the case.\n\nComposable When trust is propagated, a particular agent may be connected to multiple\ntrust chains. To come up with a final decision whether to trust or distrust this\nagent, the trust information received from the different chains need to be com-\nposed in order to build one aggregated picture. In this context, trust statements\npropagated from nodes close to oneself should have greater influence on the\naggregated value than the ones from distant nodes (distance-based aging [45]).\nComposition is potentially difficult if the trust statements are contradictory [14].\n\nSubjective The subjective nature of trust becomes clear if one thinks about a review on Ama-\nzon [26]. A book review that totally reflects Alice’s opinion will probably resolve\nin a high level of trust against the reviewer Rachel. Bob, however, who disagrees\nwith the review, will have a lower trust in Rachel although it bases on the same\nevidence.\n\nFine-grained Although trust is sometimes modeled in a binary manner (i.e. either trust or dis-\ntrust), it is possible that Alice trusts both Bob and Claire but that she trusts Bob\nmore than Claire. Hence, there may be multiple discrete levels of trust such as\nhigh, medium and low [41]. Mapped to numbers, trust may also be a continuous\nvariable taking values within a certain interval (e.g. between 0 and 1).\n\nEvent-sensitive It can take a long time to build trust. One negative experience, though, can\ndestroy it [23].\n\nReflexive Trust in oneself is always at the maximum value.\n\nSelf-reinforcing It is human nature to preferentially interact with other agents that are trusted.\nAnalogously, agents will avoid interacting with untrustworthy agents. Thus, the\ntrustworthiness of other agents is inherently taken into consideration.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 4 of 21\n\nestablishment [7]. Policy-based trust is often referred to as a hard security mechanism due\nto the exchange of hard evidence (e.g. credentials). Reputation-based trust, in contrast, is\nderived from the history of interactions. Hence, it can be seen as an estimation of trust-\nworthiness (soft security). In this work, we focus on reputation-based trust. Reputation\nis defined as follows: “Reputation is what is generally said or believed about a person’s or\nthing’s character or standing.” [8].\nIt is based on referrals, ratings or reviews from members of a community. Therefore,\n\nit can be considered as a collective measure of trustworthiness [8]. Trustworthiness as a\nglobal value is objective. However, the trust an agent puts in someone or something as a\ncombination of personal experience and referrals is subjective.\n\nResearch gap: design of reputation systems with reuse\n\nIt has been argued (e.g. by [3]) that most reputation-based trust models proposed in the\nacademic community are built from scratch and do not rely on existing approaches. Only\na few authors continue their research on the ideas of others. Thus, many approvedmodels\nand promising thoughts go unregarded. The benefits of reuse, though, have been rec-\nognized in software engineering for years. However, there are only very few works that\nproposed single components to enhance existing approaches. Rehak et al. [9], for instance,\nintroduced a generic mechanism that can be combined with existing trust models to\nextend their capabilities by efficiently modeling context. The benefits of such a compo-\nnent that can easily be combined with existing systems are obvious. Nonetheless, research\nin trust and reputation still lacks in sound and accepted principles to foster reuse.\nTo gradually close this gap, we aim to provide a framework for the design of new\n\nreputation systems with reuse. As described above, we thereto propose a hierarchical\ncomponent taxonomy of computation engines used in reputation systems. Based on this\ntaxonomy, we set up a repository containing design knowledge on both a conceptual\nand an implementation level. On the one hand, the uniform and well-structured artifacts\ncollected in this repository can be used by developers to select, understand and apply\nexisting concepts. On the other hand, they may encourage researchers to provide novel\ncomponents on a conceptual and an implementation level. In this way, the reuse of ideas,\nconcepts and implemented components as well as the communication of reuse knowledge\nshould be achieved. Furthermore, we argue that the reusable components we identify in\nthis work could extend current reputation models by the ability to gradually include the\nproperties of trust described above. To evaluate whether our taxonomy/framework can\ncover all aspects of trust, we finally provide a table matching our component classes with\ntrust properties.\n\nA hierarchical component taxonomy for computationmethods in reputation\nsystems\nTo derive a taxonomy from existing models, our research includes two steps: (1) the\nanalysis of the generic process of reputation systems and (2) the identification of logical\ncomponents of the computation methods used in common trust and reputation models.\nA critical question is how to determine and classify single components. Thereto, we follow\nan approach to function-based component classification, which means that the taxonomy\nis derived from the functions the identified components fulfill.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 5 of 21\n\nThe generic process of reputation systems\n\nThe generic process of reputation systems, as depicted in Figure 1, can be divided into\nthree steps: (1) collection & preparation, (2) computation and (3) storage & communica-\ntion. These steps are adapted from the three fundamental phases of reputation systems\nidentified by [10] and [11]: feedback generation/collection, feedback aggregation and\nfeedback distribution. Feedback aggregation as the central part of every trust and repu-\ntation system is furthermore divided into the three process steps filtering, weighting and\naggregation taken together as computation. The context setting consists of a trustor who\nwants to build a trust relation toward a trustee by providing context and personalization\nparameters and receiving a trustee’s reputation value.\n\nCollection and preparation\n\nIn the collection and preparation phase, the reputation system gleans information about\nthe past behavior of a trustee and prepares it for subsequent computing. Although per-\nsonal experience is the most reliable, it is often not sufficiently available or nonexistent.\nTherefore, data from other sources needs to be collected. These can be various, ranging\nfrom public or personal collections of data centrally stored to data requested from dif-\nferent peers in a distributed network. After all available data is gathered, it is prepared\nfor further use. Preparation techniques include normalization, for instance, which brings\nthe input data from different sources into a uniform format. Once the preparation is\ncompleted, the reputation data serves as input for the computation phase.\n\nComputation\n\nThe computation phase is the central part of every reputation system and takes the rep-\nutation information collected as input and generates a trust/reputation value as output.\nThis phase can be divided into the three generic process steps filtering, weighting and\naggregation. Depending on the computation engine, not all steps have to be implemented.\nThe first two steps (filtering and weighting) preprocess the data for the subsequent aggre-\ngation. The need for these steps is obvious: The first question to be answered is which\ninformation is useful for further processing (filtering). The second process step concerns\nthe question of how relevant the information is for the specific situation (weighting). In\nline with this, Zhang et al. [12] pointed out that current trust models can be classified into\nthe two broad categories filtering-based and discounting-based. The difference between\nfiltering and weighting is that the filtering process reduces the information amount while\n\nFigure 1 Generic process of a reputation system, inspired by [10].\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 6 of 21\n\nit is enriched by weight factors in the second case. Therefore, filtering can be seen as\nhard selection while weighting is more like a soft selection. Finally, the reputation values\nare aggregated to calculate one or several reputation scores. Depending on the algo-\nrithm, the whole computation process or single process steps can be run through for\nmultiple times.\n\nStorage and communication\n\nAfter reputation scores are calculated, they are either stored locally, in a public storage\nor both depending on the structure (centralized/decentralized/hybrid) of the reputation\nsystem. Common reputation systems not only provide the reputation scores but also offer\nextra information to help the end-users understand the meaning of a score. They should\nfurthermore reveal the computation process to accomplish transparency.\nIn this work, we focus on the computation phase, since the first phase (collection &\n\npreparation) and the last phase (storage & communication) strongly depend on the struc-\nture of the reputation system (centralized or decentralized). The computation phase,\nhowever, is independent of the structure and can look alike for systems implemented in\nboth centralized and decentralized environments. Therefore, it works well for design with\nreuse.\n\nHierarchical component taxonomy\n\nIn this section, the computation process is examined in detail. We introduce a novel\nhierarchical component taxonomy that is based on the functional blocks of common rep-\nutation systems identified in this work. Thereto, we clarify the objectives of the identified\nclasses (functions) and name common examples. Our analysis and selection of reputa-\ntion systems is based on different surveys [2,3,8,13,14]. Figure 2 gives an overview of the\nprimary and secondary classes identified.\n\nFigure 2 Classes of filtering-, weighting- and aggregation-techniques.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 7 of 21\n\nBeginning with the filtering phase, the three broad classes attribute-based, statistic-\nbased and clustering-based filtering can be identified:\n\n1. Attribute-based filtering: In several trust models, input data is filtered based on a\nconstraint-factor defined for the value of single attributes. Attribute-based filters\nmostly implement a very simple logic, in which an attribute is usually compared to\na reference value. Due to their lightweight, they are proper for reducing huge\namounts of input data to the part necessary for the reputation calculation. Besides\nthe initial filtering of input data, it is often applied after the weighting phase in\norder to filter referrals that have been strongly discounted. Time is an example of\nan attribute that is often constrained because it is desirable to disregard very old\nratings. eBay’s reputation system, for instance, only considers transactions having\noccurred in the last 12 months for their overview of positive, neutral and negative\nratings. Other models such as Sporas [15] ignore every referral but the latest, if one\nparty rated another party more than once. In this way, simple ballot stuffing attacks\ncan be prevented. In ballot stuffing attacks, parties improve their reputation by\nmeans of positive ratings after fake transactions.\n\n2. Statistic-based filtering: Further techniques that are used to enhance the\nrobustness of trust models against the spread of false rumors apply statistical\npatterns. Whitby et al. [16], for example, proposed a statistical filter technique to\nfilter out unfair ratings in Bayesian reputation systems applying the majority rule.\nThe majority rule considers feedback that is far away from the majority’s referrals as\ndishonest. In this way, dishonest or false feedback can easily be detected and filtered.\n\n3. Clustering-based filtering: Clustering-based filter use cluster analysis approaches\nto identify unfair ratings. These approaches are comparatively expensive and\ntherefore rarely used as filtering techniques. An exemplary procedure is to analyze\nan advisor’s history. Since a rater never lies to himself, an obvious way to detect\nfalse ratings is to compare own experience with the advisor’s referrals. Thus, both\nfair and unfair ratings can be identified. iCLUB [17], for example, calculates\nclusters of advisors whose evaluations against other parties are alike. Then, the\ncluster being most similar to the own opinion is chosen as fair ratings. If there is no\ncommon experience (e.g. bootstrapping), the majority rule will be applied. Another\nexample for an approach using cluster filtering was proposed by Dellarocas [18].\n\nOnce all available information is reduced to those suitable for measuring trust and\nreputation in the current situation, it becomes clear that various data differ in their\ncharacteristics (e.g. context, reliability). Hence, the referrals are weighted in the second\nprocess step based on different factors. In contrast to the filtering step, applied techniques\ndiffer strongly. For that reason, our classification of weighting techniques is based on the\nproperties of referrals that are analyzed for the discounting. We distinguish between the\nfollowing classes:\n\n1. Context comparability: Reputation data is always bound to the specific context in\nwhich it is created. Ratings that are generated in one application area might not be\nautomatically applicable in another application area. In e-commerce, for instance,\ntransactions are accomplished involving different prices, product types, payment\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 8 of 21\n\nmethods, quality or time. The non-consideration of this context leads to the value\nimbalance problem where a malicious seller can build a high reputation by selling\ncheap products while cheating on expensive ones. To increase comparability and\navoid such situations, context has become a crucial attribute for many current\napproaches like [19] or [9].\n\n2. Criteria comparability: Besides the context in which feedback is created, the\ncriteria that underlie the evaluation are important. Particularly, if referrals from\ndifferent application areas or communities are integrated, criteria comparability\ncan be crucial. In file-sharing networks, for instance, a positive rating is often\ngranted with a successful transaction independent of the quality of service. On\ne-commerce platforms, in contrast, quality may be a critical factor for customer\nsatisfaction. Other distinctions could be the costs of reviews, the level of\nanonymity or the number of peers in different communities or application\nareas. Weighting based on criteria comparability can compensate these\ndifferences.\n\n3. Credibility/propagation: In network structures such as in the web-of-trust, trust\ncan be established along a recommendation or trust chain. Obviously, referrals that\nhave first-hand information about the trustworthiness of an agent are more\ncredible than referrals received at second-hand (with propagation degree of two) or\nhigher. Therefore, several models apply a propagation (transitivity) rate to discount\nreferrals based on their distance. The biometric identity trust model [20], for\ninstance, derives the reputation-factor from the distance of nodes in a web-of-trust.\n\n4. Reliability: Reliability or honesty of referrals can strongly affect the weight of\nreviews. The concept of feedback reputation that measures the agents’ reliability in\nterms of providing honest feedback is often applied. As a consequence, referrals\ncreated by agents having a low feedback reputation have a low impact on the\naggregated reputation. The bases for this calculation can be various. Google’s\nPageRank [21], for instance, involves the position of every website connected to the\ntrustee in the web graph in their recursive algorithm. Epinions [22], on the other\nhand, allows users to directly rate reviews and reviewers. In this way, the effects of\nunfair ratings are diminished.\n\n5. Rating value: Trust is event sensitive. For stronger punishment of bad behavior,\nthe weight of positive ratings compared to negative ratings can be calculated\nasymmetrically. An example for a model using an “adaptive forgetting scheme” was\nproposed by Sun et al. [23], in which good reputation can be built slowly through\ngood behavior but easily be ruined through bad behavior.\n\n6. Time: Due to the dynamic nature of trust, it has been widely recognized that time\nis one important factor for the weighting of referrals. Old feedback might not be as\nrelevant for reputation scoring as new referrals. An example measure for\ntime-based weighting is the “forgetting factor” proposed by Jøsang [24].\n\n7. Personal preferences: Reputation systems are used by various end-users (e.g.\nhuman decision makers, services). Therefore, a reputation system must allow the\nadaptation of its techniques to subjective personal preferences. Different actors\nmight have different perceptions regarding the importance of direct experience\nand referrals, the significance of distinct information sources or the rating of\nnewcomers.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 9 of 21\n\nThe tuple of reputation data and weight-factor(s) serve as input for the third step of\nthe computation process - the aggregation. In this phase, one or several trust/reputa-\ntion values are calculated by composing the available information. In some cases, the\nweighting and the aggregation process are run through repetitively in an iterative manner.\nHowever, the single steps can still be logically separated. The list of proposed algorithms\nto aggregate trust and reputation values has become very long during the last decade.\nHere, we summarize the most common aggregation techniques and classify them into the\nfour blocks simple arithmetic, statistic, fuzzy and graph-based models:\n\n1. Simple arithmetic: The first class includes simple aggregation techniques like\nranking, summation or average. Ranking is a very basic way to measure\ntrustworthiness. In ranking algorithms, ratings are counted and organized in a\ndescending order based on that value. This measure has no exact reputation score.\nInstead, it is frequently used as a proxy for the relative importance/trustworthiness.\nExamples for systems using ranking algorithms are message boards like Slashdot\n[25] or citation counts used to calculate the impact factor in academic literature.\nOther aggregation techniques that are well known due to the implementation on\neBay or Amazon [26] are the summation (adding up positive and negative ratings)\nor the average of ratings. Summation, though, can easily be misleading, since a\nvalue of 90 does not reveal the composition of positive and negative ratings (e.g.\n+100,-10 or +90,0). The average, on the other hand, is a very intuitive and easily\nunderstandable algorithm.\n\n2. Statistic: Many of the prominent trust models proposed in the last years use a\nstatistical approach to provide a solid mathematical basis for trust management.\nApplied techniques range from Bayesian probability over belief models to Hidden\nMarkov Models. All models based on the beta probability density function (beta\nPDF) are examples for models simply using Bayesian probability. The beta PDF\nrepresents the probability distributions of binary events. The a priori reputation\nscore is thereby gradually updated by new ratings. The result is a reputation score\nthat is described in a beta PDF function parameter tuple (α, β), whereby α\n\nrepresents positive and β represents negative ratings. A well known model using\nthe beta PDF is the Beta Reputation system [24]. A weakness of Bayesian\nprobabilistic models, however, is that they cannot handle uncertainty. Therefore,\nbelief models extend the probabilistic approach by Dempster-Shafer theory (DST)\nor subjective logic to include the notion of uncertainty. Trust and reputation\nmodels involving a belief model were proposed by Jøsang [27] or Yu and Singh [28].\nMore complex solutions that are based on machine learning, use the Hidden\nMarkov Model, a generalization of the beta model, to better cope with the dynamic\nbehavior. An example was introduced by Malik et al. [29].\n\n3. Fuzzy: Aggregation techniques classified as fuzzy models use fuzzy logic to\ncalculate a reputation value. In contrast to classical logic, fuzzy logic allows to\nmodel truth or falsity within an interval of [0,1]. Thus, it can describe the degree to\nwhich an agent/resource is trustworthy or not trustworthy. Fuzzy logic has been\nproven to deal well with uncertainty and mimic the human decision making\nprocess [30]. Thereby, a linguistic approach is often applied. REGRET [31] is one\nprominent example of a trust model making use of fuzzy logic.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 10 of 21\n\n4. Graph-based: A variety of trust models employ a graph-based approach. They rely\non different measures describing the position of nodes in a network involving the\nflow of transitive trust along trust chains in network structures. As online social\nnetworks have become popular as a medium for disseminating information and\nconnecting people, many models regarding trust in social networks have lately\nbeen proposed. Graph-based approaches use measures from the field of graph\ntheory such as centrality (e.g. Eigenvector, betweenness), distance or node-degree.\nReputation values, for instance, grow with the number of incoming edges (in-\ndegree) and increase or decrease with the number of outgoing edges (out-degree).\nThe impact of one edge on the overall reputation can depend on several factors like\nthe reputation of the node an edge comes from or the distance of two nodes.\nPopular algorithms using graph-based flow model are Google’s PageRank [21] as\nwell as the Eigentrust Algorithm [32]. Other examples are the web-of-trust or trust\nmodels particularly designed for social networks as described in [14]. As mentioned\nabove, the weighting and aggregation phases are incrementally run through for\nseveral times due to the incremental nature of these algorithms.\n\nThe classification of the computation engine’s components used in different trust mod-\nels in this taxonomy is not limited to one component of each primary class. Depending\non the computation process, several filtering, weighting and aggregation techniques can\nbe combined and run through more than once. Malik et al. [29], for instance, introduced\na hybrid model combining heuristic and statistical approaches. However, our taxonomy\ncan reveal the single logical components a computation engine is built on. Moreover,\nit serves as an overview of existing approaches. Since every currently known reputa-\ntion system can find its position, to the best of our knowledge, this taxonomy can be\nseen as complete. Though, an extension by new classes driven by novel models and\nideas is possible. Our hierarchical component taxonomy currently contains 3 primary\ncomponent classes, 14 secondary component classes, 23 component terms and 29 sub-\nsets. Table 2 shows an excerpt of the hierarchical component taxonomy with building\nblocks of the primary class “weighting”. The full taxonomy is provided in Additional file 1:\nTable S1.\n\nThe component taxonomy as a framework for design with reuse\nThe hierarchical component taxonomy introduced in the former section serves as a nat-\nural framework for the design of reputation systems with reuse. To support this process,\nwe set up a component repository combining a knowledge and a service repository.\nThus, it does not only contain information about software components on implementa-\ntion level but also provides extensive descriptions of the ideas applied on a conceptual\nlevel. This comprehensive set of fundamental component concepts and ideas combined\nwith the related implementation allows the reuse of both ideas and already implemented\ncomponents.\nIn this section, we firstly describe the conceptual design of our component reposi-\n\ntory in detail. Then, we elaborate on the implementation of a web application employing\nour thorough repository to provide design knowledge for reuse on a conceptual and an\nimplementation level.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 11 of 21\n\nTable 2 Excerpt of the hierarchical component taxonomywith descriptions\n\nPrimary\ncomponent\nclass\n\nSecondary\ncomponent\nclass\n\nComponent term Subset Description\n\ncredibility/\npropagation\n\npropagation discount Discount referrals along\ntrust chains\n\nsubjective reliability property similarity Discount based on\nsimilarity of personal\nproperties\n\nrating similarity Discount referrals based\non similarity of ratings\ntoward other agents\n\nweighting reliability Explicit Discount based on explicit\nreputation information like\nreferrals or certificates\n\nobjective reliability Implicit Discount based on implicit\nreputation information like\nprofile age, number of\nreferrals or position\n\nrating value asymmetric rating Strongly discount positive\nratings compared to\nnegative ratings (event\nsensitive)\n\n. . . . . . . . . . . . . . .\n\nConceptual design of the component repository\n\nReuse-based software engineering can be implemented on different levels of abstraction,\nranging from the reuse of ideas to the reuse of already implemented software components\nfor a very specific application area. In this work, we want to apply our taxonomy for reuse\non two levels – a conceptual level and an implementation level. Therefore, the developed\nrepository provides design knowledge for reuse on two logical layers (see Figure 3).\n\nFigure 3 Logical layers of the component repository for design with reuse.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 12 of 21\n\nReuse on conceptual level\n\nWhen reusing an implemented component, one is unavoidably constrained by design\ndecisions that have been made by the developer. A way to prevent this is to conceive more\nabstract designs that do not specify the implementation. Thus, we provide an abstract\nsolution to a problem by means of design pattern-like concepts. Design patterns are\ndescriptions of commonly occurring problems and a generic solution to the problems that\ncan be used in different settings [33]. Our design pattern-like concepts consist of essential\nelements that are exemplary depicted in Table 3.\n\nReuse on implementation level\n\nOn implementation level, we provide fully implemented reusable components by means\nof web services in a service-orientated architecture. These services encapsulate the con-\ncepts’ logic and functionality in independent and interchangeable modules to achieve the\nseparation of concerns. The web services are incorporated via well-defined interfaces. All\nservices provided are registered as artifacts in the service repository. An artifact contains\nessential information about one live reachable service such as ID, type (REST or ws), URL,\ndescription, parameters, example calls, example output, the design pattern that is imple-\nmented by the service, and tags describing the functionality. Table 4 shows an example\nartifact for the design pattern described above.\n\nTable 3 Design pattern on the conceptual level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nDescription This component uses an absolute congruence metric as similarity measure to\nidentify context similarity.\n\nProblem description Reputation data is always bound to the specific context in which it was created.\nRatings that were generated in one application area might not be automatically\napplicable in another application area which can result in the value imbalance\nproblem.\n\nSolution description Apply similarity measurement between context ci (reference context) and con-\ntext cj of referrals in the referral set to deliver a weight-factor for each item of the\nreferral set using the following formula:\n\nw(c1, c2) := k(ci) ∩ k(cj)\n\nk(ci) ∪ k(cj)\n\nk(ci) denotes the total number of keywords describing context ci .\n\nApplicability Set of nominal context attributes.\n\nCode example (php)\n\nfunction calculate_values($reference, $context_sets) {\n$reference_context = $reference[’context_attributes’];\n$return_values = array();\nwhile(!empty($context_sets)) {\n\n...shortened...\n}\nreturn $return_values;\n\n}\n\nImplementation Context similarity-based weighting service (absolute congruence)\n\nLiterature\n• Mohammad Gias Uddin, Mohammad Zulkernine, and Sheikh Iqbal\n\nAhamed. 2008. CAT: a context-aware trust model for open and dynamic\nsystems. In Proceedings of the 2008 ACM symposium on Applied\ncomputing (SAC ’08). ACM, New York, NY, USA, 2024–2029.\n\nTags weighting, context, similarity, congruence\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 13 of 21\n\nTable 4Web service description on implementation level (example)\n\nComponent term Context similarity\n\nSubset Absolute congruence\n\nType REST\n\nDemo http://trust.bayforsec.de/ngot/webservice/Client/?=Weighting-congruence-absolute-\ncall.php\n\nDescription This service provides an absolute similarity measurement between a reference context\nand a context-set of referrals.\nExample: The sets ’registered’,’charged’, ’verified’ and ’registered’, ’costless’, ’unverified’\nhave a similarity of 1/3.\n\nParameters\n\n// define words that describe the quality of a referall\n$reference_context : array(\"words\" => array (TEXT));\n\n$referral_sets = array( $context_set );\n\n$referral_set = array( \"id\" => NUMBER, \"words\" => array (TEXT));\n\nExample call\n\nrequire_once(’WebserviceCallHelper.php’);\n\n$arguments = array(\"words\" => array (\"registered\",\"charged\",\"verified\"));\n$referral_set = array();\n$referral_set[0] = array( \"id\" => \"10000\", \"words\" =>\n\narray (\"registered\",\"costless\", \"unverified\"));\n$referral_set[1] = array( \"id\" => \"10001\", \"words\" =>\n\narray (\"registered\",\"charged\", \"verified\"));\n$referral_set[2] = array( \"id\" => \"10002\", \"words\" =>\n\narray (\"registered\",\"costless\", \"verified\"));\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Weighting\\CongruenceAbsolute\"\n\n));\n$webservice_call->get_result($arguments, $referral_set);\n\nExample output\n\nArray\n(\n\n[status] => 200\n[data] => Array\n\n(\n[0] => Array\n\n(\n[0] => 10000\n[1] => 0.2\n\n)\n[1] => Array\n\n(\n[0] => 10001\n[1] => 1\n\n)\n[2] => Array\n\n(\n[0] => 10002\n[1] => 0.5\n\n)\n)\n\n)\n\nPattern Implemented Context similarity (Absolute congruence)\n\nTags weighting, context, similarity, congruence\n\nImplementation of the repository\n\nTo demonstrate the feasibility of our approach, we have prototypically implemented the\nrepository as a web-based application in a three-tier client-server-architecture [34]. To\ngive an overview of the chosen architecture, we distinguish between server-side and\nclient-side implementation.\n\nServer-side\n\nOn server-side, the logic is implemented in PHP on an Apache server (logic layer) con-\nnecting to a MySQL database (persistent layer). The MySQL database contains all data\nregarding the design patterns as described in Table 2. Each of these design patterns is also\nimplemented in a web service. To enable a standardized realization of new web services\nand a flawless call via standardized interfaces, we employ an abstract classComponent. All\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 14 of 21\n\ncomponents (implemented as web services) must inherit from this class, which particu-\nlarly requires overwriting the function calculate_values. To make the generic component\nindependent of the input data, developers are advised to make use of the PHP function\nfunc_get_args(). In this way, distinct components can receive a variety of arguments. To\nconsistently handle client calls, our architecture is extended by aWebserviceCallHandler.\nFigure 4 depicts the schematic layout.\nAll web services implementing the trust pattern currently described in our knowledge\n\nrepository have been created and registered as artifacts in our service repository [34].\nFurthermore, these artifacts are described in detail including a definition of input, output\nand example calls as defined in Table 4.\n\nClient-side\n\nOn client-side (presentation layer), we employ the current web standards HTML5,\nJavaScript and CSS (Bootstrap). The front end is divided into three main pages –\n“overview”, “knowledge repository” and “service repository” – which provide information\non the general concept, the trust patterns and the web services. To enable a standard-\nized call of a web service from client-side, a WebserviceCallHelper allows a simple call of\neach component by configuration and provides all functions necessary to establish a con-\nnection to the repository. The configuration details are passed to the constructor, which\nrequires a base_url, an output format (HTML, XML or JSON) and a unique component\nname as illustrated below.\n\nExample call of a filtering component via theWebserviceCallHelper\n\n$webservice_call = new WebserviceCallHelper(array(\n’base_url’ => \"http://trust.bayforsec.de/ngot/webservice/\",\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n$webservice_call->get_result($arguments, $referral_set);\n\nFigure 4 Schematic view on the service architecture.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 15 of 21\n\nEvaluation\nTo rigorously demonstrate the proper functioning and quality of our approach, we carry\nout a two-part evaluation of our artifact in this section. As there is currently no compara-\nble framework, we firstly perform a descriptive scenario-based evaluation. According to\nHevner et al. [4], this is a standard approach for innovative artifacts like ours. To demon-\nstrate the completeness of our taxonomy, we secondly conduct a static analysis [4], in\nwhich we match all components to the trust properties described in Section ‘The notion\nof trust and its properties’.\n\nScenario analysis: Reputation system development\n\nThe fictitious web developer John Gray runs an electronic marketplace platform for\nphilatelists and numismatics. The platform has been launched with his friends as the first\nusers but has been growing fast. Meanwhile, most users do not know each other in person\nanymore. As a result, many of the initial users have stopped interacting with the newcom-\ners as they do not trust them. After realizing this problem, John decides to introduce an\nonline reputation system in order to establish trust among the strangers. In the following,\nwe describe how our knowledge and service repository can help him to build a reputation\nsystem that perfectly meets his requirements.\nHaving read the basics on our component model, John concludes that he wants to build\n\na computation engine thatmakes use of components of all three phases – filtering, weight-\ning and aggregation. Thinking about the experiences made with sellers on the platform,\nhe recognizes that most of them do not deliver the same quality all the time. Thus, an\nage-based filter should be employed to make old referrals less important than new ones.\nFurthermore, there are sellers that usually deliver high quality stamps while offering poor\nquality coins. Therefore, a weighting component based on context similarity (absolute\ncongruence) should be selected. Regarding the aggregation alternatives, John decides to\nmake use of the average component as the simple average is probably the most intuitive\nand most transparent aggregation technique for the users. Finally, the single components\nare combined in sequence to a fully functional computation engine as depicted in Figure 5.\nThe code listed below shows an example for the implementation of John’s computation\n\nengine. Here, the WebserviceCallHelpers for each of the selected components have to be\ninstantiated first as described in Section ‘Client-side’. Secondly, the referral set needs to\nbe loaded and prepared according to input parameter descriptions provided for each web\nservice in the component repository. In its current form, the framework does not pro-\nvide any classes to automatically plug single components together (glue class). Thus, the\ndeveloper has to ensure that the output of one component is correctly provided as an\ninput for the following component. This lose coupling, however, allows for more flexibil-\nity. All details on the input and output format can be found in the artifact description of\neach component. While the input data varies from component to component, the output\nis alike for components of each primary class.\n\nFigure 5 Sequence of components for John’s computation engine.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 16 of 21\n\nExample code for the computation engine described above\n\n//create helper for filtering component\n$webservice_filter = new WebserviceCallHelper(array(\n\n’base_url’ => WEBSERVICE_URL,\n’format’ => \"html\",\n’component’ => \"Filtering\\AgeBasedAbsolute\"\n\n));\n\n//create helper for weighting component\n$webservice_weight = ...as above...\n\n//create helper for aggregation component\n$webservice_aggregate = ...as above...\n\n//define referral_set\n$referral_set = array(\"id\" => NUMBER, \"time\" => DATE, \"context\" => array (TEXT), \"\n\nrating\" => NUMBER);\n\n//call computation\n$reputation_value = $webservice_aggregate(\n\n$glue->prepare_for_aggregation($webservice_weight(\n$glue->prepare_for_weighting($webservice_filter->get_result(\n\n$referral_set))\n))\n\n);\n\nThis scenario elucidates that our knowledge and service repository has an obvious util-\nity from a practical point of view since developers can easily access it and gain knowledge\nabout online reputation systems. Thereby, we may help to better spread innovative ideas\nand allow developers to experiment with different computation techniques. However,\nour approach requires specific knowledge on the structure of the repository, the func-\ntioning of each component and details on how to plug components together. Developers\nneed to manually combine components and take care, whether they use valid input data\nand a feasible combination of reputation system components. In its current form, our\nframework is not very “developer friendly”. Therefore, further research will be necessary\nin order to improve the practical usability of our component repository. In Section ‘Con-\ntribution and future work’, we discuss open issues in more detail.\n\nStatic analysis: Matching components and trust properties\n\nTo guarantee the proper generation of computational trust, trust-enforcing mechanisms\nsuch as reputation systems should be able to consider and address all properties of trust.\nAs our component taxonomy serves as a framework for the design of new reputation sys-\ntems with reuse, it should enable developers to extend current reputation models by the\nability to gradually include the various properties of trust. Therefore, a way to evaluate\nthe completeness of our solution is to review whether a trust system that is built accord-\ning to our framework could meet this standard. In Table 5, we match the computation\ncomponents identified above to the properties of trust introduced in Section ‘The notion\nof trust and its properties’. Since our taxonomy is based on reputation systems analyzed\nin various surveys, this approach also enables us to identify aspects of trust that have only\nrarely been considered in research so far.\nExamining Table 5, we find that all trust properties listed are widely covered. There is at\n\nleast one component addressing each single characteristic. Going intomore detail, we find\nthat there are many proposals that have developed components to personalize reputation\nsystems, thus covering the subjective property of trust. This reflects a general trend to an\nenhanced personalization of reputation systems. Furthermore, it becomes clear that all\nof our weighting and aggregation components follow the fine-grained property of trust,\ni.e. that trust can be modeled as a continuous variable. For the filtering components, the\nfine-grained property is not entirely applicable in the same meaning as filtering has no\ndirect influence on the trust value of referrals. Since the effect of filtering is that a referral\n\n\n\nSängeretal.JournalofTrustM\nanagem\n\nent\n (2015) 2:5 \n\nPage\n17\n\nof21\n\nTable 5Matching reputation system components and trust properties\n\nTrust properties\n\nPrimary Secondary Dynamic Context- Multi- Propagative Composable Subjective Fine- Event- Reflexive Self-\n\ncomponent component dependent faceted grained sensitive reinforcing\n\nclass class\n\nComputation\ncomponents\n\nFiltering\n\nattribute-based � � � � �� � � � � �\nstatistic-based � � � � � � � � � ��\nclustering � � � � � � � � � ��\n\nWeighting\n\ncontext comparability �� � �� � � � � � � �\ncriteria comparability � �� � � � � � � � �\ncredibility � � � � � � � � �� �\nreliability � � � �� �� � � � �� �\nrating value � � � � � � � � � ��\ntime � � � � � � � � � �\npersonal preferences � � � � � � � � �� �\n\nAggregation\n\nsimple arithmetic � � � � � � � � � ��\nstatistic � � � � � � � � � ��\nfuzzy � � � � � � � � � ��\ngraph-based � � � � � �� � � � ��\n\n� not addressed.�� partly addressed.� completely addressed.\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 18 of 21\n\neither is further considered or not, it shows a binary character rather than being fine-\ngrained. In contrast to the subjective and fine-grained properties of trust, other properties\nsuch as context-dependent, multi-faceted and event-sensitive are particularly addressed\nby only one or two components. Note that this does not automatically mean that there\nis an increased necessity for future research concerning these properties. It may also be\npossible that one component is enough to cover one trust property. More detailed studies\non this could be part of future work.\nOverall, we can say that computational trust can be represented quite accurately when\n\nusing our taxonomy and the provided components as a basis for the development of new\nreputation systems or the extension of existing models. Note, however, that this is only\none view on our taxonomy. Conducting a comparable analysis from the viewpoint of\nattacks and defense mechanisms, for instance, the outcomes may vary greatly.\n\nContribution and future work\nMany surveys of trust and reputation systems give an overview of existing trust and repu-\ntation systems by means of a classification of existing models and approaches. In contrast\nto this, we provide a collection of ideas and concepts classified by their functions. Further-\nmore, these ideas are not only named but also clearly described in well-structured design\npattern-like artifacts which can easily be adapted to a specific situation. Therewith, we\nreorganized the design knowledge for computation techniques in reputation systems and\ntranslated the most common ideas into a uniform format. To directly make use of novel\ncomponents, the web services created on implementation level can instantly be reused\nand integrated in existing reputation systems to extend their capabilities. This approach\n(i.e. publicly providing implemented computation components as web services) may help\nto better spread innovative ideas in trust and reputation systems and give system builders\na better choice allowing to experiment with different computation techniques. Moreover,\nwe encourage researchers to focus on the design of single components by providing a\nplatform on which concepts and their prototypical implementation can be made publicly\navailable.\nNonetheless, there are still some unexplored areas regarding the design with reuse in\n\ntrust and reputation systems. Firstly, reusability could play a role in process steps other\nthan the computation phase. To clarify the opportunities, further research is necessary in\nthis area. Secondly, our hierarchical taxonomy is currently limited to a functional view on\nthe identified components but developersmay also benefit from additional views. Because\nof the importance of the robustness of trust and reputation systems [35], we are particu-\nlarly interested in an attack view. In [36], we present first ideas on this issue. We propose\na taxonomy of attacks on reputation systems and then refer to the single components of\nour repository as solutions to the specific attack classes. In this way, we not only support\nreputation system designers in the development of more reliable and more robust rep-\nutation systems with already existing components but also help to identify weaknesses\nthat have not been addressed so far. Thirdly, the selection and interpretation of adequate\ncomponents for new reputation systems in a particular application area requires time,\neffort and – to some extent – knowledge of this research area. To increase usability, a\nsoftware application is needed to support a user in this development process. Ultimately,\nthe application may even be able to automatically find the most qualified composition\nfor specific requirements and input data. This, in turn, demands for generic testbeds that\n\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 19 of 21\n\nenable objective evaluations of reputation systems because so far, researchers have mainly\nbeen developing their own testing scenarios favoring their own work [37]. The most well-\nknown proposals regarding independent testbeds are ART [38] and TREET [37]. Recently,\nIrissappane and Zhang [39,40] made another important step forward by introducing a\npublicly available testbed that is able to reflect real environmental settings. We plan to\nuse their tool in future studies. Finally, we need to observe the usage of our repository in\npractice to learn from how users deal with it. This can either be done through conducting\nexperimental user studies or by interviewing developers who use our repository in a real\nenvironment. In this way, we can run through a continuous improvement process.\n\nConclusion\nThe research in trust and reputation systems is still growing. In this paper, we presented\nconcepts to foster reuse of existing approaches. We provided a hierarchical taxonomy of\ncomputation components from a functional view and described the implementation of\na component repository that serves as both a knowledge base and a service repository.\nIn this way, we communicate design knowledge for reuse, support the development of\nnew reputation systems and encourage researchers to focus on the development of single\ncomponents that can be integrated in various reputation systems to easily extend their\ncapabilities by new features. Matching the identified components and the properties of\ntrust, we found that integrating existing ideas and concepts can lead to a reputation sys-\ntem that widely reflects computational trust by addressing all properties of trust described\nin literature.\n\nAdditional file\n\nAdditional file 1: Table S1.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthors’ contributions\nJS proposed the initial idea of this paper. He developed the hierarchical component taxonomy and implemented the\ncomponent repository. CR and JS conducted the evaluation of the proposed ideas. CR and JS furthermore revised this\npaper according to reviewers’ comments. GP supervised the research, contributed to the paper writing and made\nsuggestions. All authors read and approved the final manuscript.\n\nAcknowledgment\nThe research leading to these results was supported by the Bavarian State Ministry of Education, Science and the Arts\" as\npart of the FORSEC research association.\n\nReceived: 9 December 2014 Accepted: 1 April 2015\n\nReferences\n1. Electronics, Cars, Fashion, Collectibles, Coupons and More | eBay. http://www.ebay.com\n2. Yao Y, Ruohomaa S, Xu F (2012) Addressing common vulnerabilities of reputation systems for electronic commerce.\n\nJ Theor Appl Electron Commerce Res 7(1):1–20\n3. Tavakolifard M, Almeroth KC (2012) A taxonomy to express open challenges in trust and reputation systems. J\n\nCommun 7(7):538–551\n4. Hevner AR, March ST, Park J, Ram S (2004) Design science in information systems research. MIS Quarterly 28(1):75–105\n5. McKnight DH, Chervany NL (1996) The Meanings of Trust. Technical report. University of Minnesota, Management\n\nInformation Systems Research Center\n6. Gambetta D (1988) Can we trust trust? In: Gambetta D (ed). Trust: making and breaking cooperative relations. Basil\n\nBlackwell, Oxford. pp 213–237\n7. Artz D, Gil Y (2007) A survey of trust in computer science and the semantic web. Web Semantics 5(2):58–71\n8. Jøsang A, Ismail R, Boyd C (2007) A survey of trust and reputation systems for online service provision. Decis Support\n\nSyst 43(2):618–644\n\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.journaloftrustmanagement.com/content/supplementary/s40493-015-0015-3-s1.pdf\nhttp://www.ebay.com\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 20 of 21\n\n9. Rehak M, Gregor M, Pechoucek M, Bradshaw J (2006) Representing context for multiagent trust modeling. In:\nSkowron A, Barthès JP, Jain LC, Sun R, Morizet-Mahoudeaux P, Liu J, Zhong N (eds). Proceedings of the 2006\nIEEE/WIC/ACM International Conference on Intelligent Agent Technology, Hong Kong, China. IEEE Computer\nSociety, Washington, DC. pp 737–746\n\n10. Swamynathan G, Almeroth KC, Zhao BY (2010) The design of a reliable reputation system. Electron Commerce Res\n10(3–4):239–270\n\n11. Resnick P, Kuwabara K, Zeckhauser R, Friedman E (2000) Reputation systems. Commun ACM 43(12):45–48\n12. Zhang L, Jiang S, Zhang J, Ng WK (2012) Robustness of trust models and combinations for handling unfair ratings. In:\n\nDimitrakos T, Moona R, Patel D, McKnight DH (eds). Trust Management VI: Proceedings of the 6th IFIP WG 11.11\ninternational conference (IFIPTM). Springer, Berlin, Heidelberg, Surat, India. pp 36–51\n\n13. Noorian Z, Ulieru M (2010) The state of the art in trust and reputation systems: a framework for comparison. J Theor\nAppl Electron Commerce Res 5(2):97–117\n\n14. Sherchan W, Nepal S, Paris C (2013) A survey of trust in social networks. ACM Comput Surv 45(4):1–33\n15. Zacharia G, Moukas A, Maes P (2000) Collaborative reputation mechanisms for electronic marketplaces. Decis\n\nSupport Syst 29(4):371–388\n16. Whitby A, Jøsang A, Indulska J (2004) Filtering out unfair ratings in Bayesian reputation systems. In: Falcone R, Barber\n\nS, Sabater J, Singh M (eds). Proceedings of the third international joint conference on autonomous agents and multi\nagent systems, New, York, USA. IEEE Computer Society, Washington, DC. pp 106–117\n\n17. Liu S, Zhang J, Miao C, Theng Y-L, Kot AC (2011) iCLUB: an integrated clustering-based approach to improve the\nrobustness of reputation systems. In: Sonenberg L, Stone P, Tumer K, Yolum P (eds). Proceedings of the 10th\ninternational conference on Autonomous Agents and Multiagent Systems (AAMAS), Taipei, Taiwan. IFAAMAS,\nRichland, SC. pp 1151–1152\n\n18. Dellarocas C (2000) Immunizing online reputation reporting systems against unfair ratings and discriminatory\nbehavior. In: Jhingran A, MacKie J, Tygar D (eds). Proceedings of the 2nd ACM conference on electronic commerce,\nMinneapolis, MN. ACM, New York. pp 150–157\n\n19. Zhang H, Wang Y, Zhang X (2012) A trust vector approach to transaction context-aware trust evaluation in\ne-commerce and e-service environments. In: Shih C, Son S, Kuo T, Huemer C (eds). Proceedings of the 5th IEEE\ninternational conference on Service-Oriented Computing and Applications (SOCA). IEEE Computer Society\nWashington, DC, Taipei, Taiwan. pp 1–8\n\n20. Obergrusberger F, Baloglu B, Sänger J, Senk C (2013) Biometric identity trust: toward secure biometric enrollment in\nweb environments. In: Yousif M, Schubert L (eds). Proceedings of the 3rd international conference on Cloud\nComputing (CloudComp), Vienna, Austria. Springer, Berlin, Heidelberg. pp 124–133\n\n21. Brin S, Page L (1998) The anatomy of a large-scale hypertextual web search engine. Comput Networks\n30(1-7):107–177\n\n22. Epinions.com: Read expert reviews on Electronics, Cars, Books, Movies, Music and More. http://www.epinions.com/\n23. Sun Y, Han Z, Yu W, Ray Liu K (2006) Attacks on trust evaluation in distributed networks. In: Proceedings of Th 40th\n\nannual Conference on Information Sciences and Systems (CISS), Princeton, NJ, USA, IEEE Computer Society\nWashington, DC. pp 1461–1466\n\n24. Jøsang A, Ismail R (2002) The beta reputation system. In: Proceedings of the 15th bled conference on electronic\ncommerce, Bled, Slovenia. pp 41–55\n\n25. Slashdot: News for nerds, stuff that matters. http://www.slashdot.org/\n26. Amazon.com: Online Shopping for Electronics, Apparel, Computers, Books, DVDs & more. http://www.amazon.com\n27. Jøsang A (2001) A logic for uncertain probabilities. Int J Uncertainty Fuzziness Knowledge-Based Syst 9(3):279–311\n28. Yu B, Singh MP (2002) An evidential model of distributed reputation management. In: Proceedings of the first\n\nInternational Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 294–301\n\n29. Malik Z, Akbar I, Bouguettaya A (2009) Web services reputation assessment using a Hidden Markov Model. In: Baresi\nL, Chi CH, Suzuki J (eds). Service-oriented computing: Proceedings of the 7th International Joint Conference on\nService-Oriented Computing (ICSOC-ServiceWave), Stockholm, Sweden. Springer Berlin, Heidelberg. pp 576–591\n\n30. Song S, Hwang K, Zhou R, Yu-Kwong K (2005) Trusted P2P transactions with fuzzy reputation aggregation, Vol. 9\n31. Sabater J, Sierra C (2002) Reputation and social network analysis in multi-agent systems. In: Proceedings of the first\n\nInternational joint conference on Autonomous Agents and Multiagent Systems (AAMAS), Bologna, Italy. ACM, New\nYork, NY. pp 475–482\n\n32. Kamvar SD, Schlosser MT, Garcia-Molina H (2003) The Eigentrust algorithm for reputation management in P2P\nnetworks. In: Hencsey G, White B, Chen YF, Kovács L, Lawrence S (eds). Proceedings of the 12th International\nConference on World Wide Web (WWW), Budapest, Hungary. ACM, New York, NY. pp 640–651\n\n33. Gamma E (1995) Design patterns: elements of reusable object-oriented software. Addison-Wesley, Reading\n34. Next Generation Online Trust. http://trust.bayforsec.de\n35. Jøsang A (2012) Robustness of trust and reputation systems: does it matter? In: Dimitrakos T, Moona R, Patel D,\n\nMcKnight DH (eds). Trust management VI: Proceedings of the 6th IFIP WG 11.11 International Conference (IFIPTM),\nSurat, India. Springer, Berlin, Heidelberg. pp 253–262\n\n36. Sänger J, Pernul G (2015) Reusable defense components for online reputation systems. In: Marsh S, Jensen CD,\nMurayma Y, Dimitrakos T (eds). Trust management IX: Proceedings of the 9th IFIP WG 11.11 International\nConference (IFIPTM), Hamburg, Germany. Springer, Berlin, Heidelberg\n\n37. Kerr R, Cohen R (2010) TREET: The Trust and Reputation Experimentation and Evaluation Testbed. Electron\nCommerce Res 10(3–4):271–290\n\n38. Fullam KK, Voss M, Klos TB, Muller G, Sabater J, Schlosser A, Topol Z, Barber KS, Rosenschein JS, Vercouter L (2005) A\nspecification of the Agent Reputation and Trust (ART) Testbed. In: Dignum F, Dignum V, Koenig S, Kraus S, Singh MP,\nWooldridge M (eds). Proceedings of the 4th international joint conference on Autonomous Agents and Multiagent\nSystems (AAMAS), Utrecht, Netherlands. ACM, New York, NY, USA. pp 512–518\n\nhttp://www.epinions.com/\nhttp://www.slashdot.org/\nhttp://www.amazon.com\nhttp://trust.bayforsec.de\n\n\nSänger et al. Journal of Trust Management (2015) 2:5 Page 21 of 21\n\n39. Irissappane AA, Jiang S, Zhang J (2012) Towards a comprehensive Testbed to evaluate the robustness of reputation\nsystems against unfair rating attacks. In: Herder E, Yacef K, Chen L, Weibelzahl S (eds). Workshop and Poster\nProceedings of the 20th conference on User Modeling, Adaptation, and Personalization (UMAP), Montreal, Canada.\nSpringer Berlin, Heidelberg\n\n40. Ir", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNDkzLTAxNS0wMDE1LTMucGRm0", "metadata_author": null, "metadata_title": null, "metadata_creation_date": "2015-05-19T12:42:36Z", "keyphrases": [] }, { "@search.score": 1, "content": "\nJ Braz Comput Soc (2013) 19:573–587\nDOI 10.1007/s13173-013-0117-7\n\nSURVEY PAPER\n\nA systematic review on keystroke dynamics\n\nPaulo Henrique Pisani · Ana Carolina Lorena\n\nReceived: 18 March 2013 / Accepted: 24 June 2013 / Published online: 10 July 2013\n© The Brazilian Computer Society 2013\n\nAbstract Computing and communication systems have\nimproved our way of life, but have also contributed to an\nincreased data exposure and, consequently, to identity theft.\nA possible way to overcome this issue is by the use of biomet-\nric technologies for user authentication. Among the possible\ntechnologies to be analysed, this work focuses on keystroke\ndynamics, which attempts to recognize users by their typ-\ning rhythm. In order to guide future researches in this area,\na systematic review on keystroke dynamics was conducted\nand presented here. The systematic review method adopts\na rigorous procedure with the definition of a formal review\nprotocol. Systematic reviews are not commonly used in arti-\nficial intelligence, and this work contributes to its use in the\narea. This paper discusses the process involved in the review\nalong with the results obtained in order to identify the state\nof the art of keystroke dynamics. We summarized main clas-\nsifiers, performance measures, extracted features and bench-\nmark datasets used in the area.\n\nKeywords Behavioral intrusion detection · Biometrics ·\nKeystroke dynamics · Systematic review\n\nP. H. Pisani (B)\nInstituto de Ciências Matemáticas e de Computação (ICMC),\nUniversidade de São Paulo (USP), São Carlos, SP, Brazil\ne-mail: phpisani@icmc.usp.br\n\nA. C. Lorena\nInstituto de Ciência e Tecnologia (ICT),\nUniversidade Federal de São Paulo (UNIFESP),\nSão José dos Campos, SP, Brazil\ne-mail: aclorena@unifesp.br\n\n1 Introduction\n\nThe wider dissemination of digital identities has contributed\nto greater worries regarding information exposure [47].\nRecently, in view of the increased dissemination of the inter-\nnet in several activities (e.g. online banking, e-commerce,\ne-mail), security problems became more evident [24]. As a\nresult, identity theft has gained new momentum. The term\nidentity theft is commonly used to refer to the crime of using\npersonal information of someone else to illegally pretend to\nbe a certain person [38].\n\nIn view of this scenario, more sophisticated methods for\nuser authentication have been developed. Authentication is\nthe process used to confirm the identity of a user. In the case of\nworkstations, for example, the authentication usually occurs\nin the system initialization, known as initial authentication.\nNevertheless, even more secure authentication methods do\nnot provide an entirely effective security mechanism, as the\ncomputer may be vulnerable to intruders when the user leaves\nthe workstation and does not end the session. Consequently,\nan intruder could use the computer masquerading as the legit-\nimate user, resulting in identity theft [38]. One of the ways to\nmitigate this problem is by using intrusion detection systems\nthat act on the workstation (host-based).\n\nMore recently, the concept of detecting intrusions by the\nbehavioral analysis of the user of the computer [39] has\nemerged, also known as Behavioral Intrusion Detection [49];\nseveral aspects of this method have yet to be explored. This\nconcept is grounded on the fact that, by observing the behav-\nior of a user, it is possible to define models that represent\nthe regular behavior (profile) of this user, thus allowing the\nidentification of deviations that are potential intrusions. The\nprocess of defining these models is known as user profil-\ning [46]. There is a great variety of features that can be\nused to define the model of a user. This work focuses on\n\n123\n\n\n\n574 J Braz Comput Soc (2013) 19:573–587\n\nkeystroke dynamics, classified as a behavioral biometric\ntechnology.\n\nThis paper adopts a rigorous method to perform a review\non intrusion detection with keystroke dynamics, known as\nsystematic review. As the name suggests, a systematic review\nadopts a formal and systematic procedure for the conduction\nof the bibliographic review, with the definition of explicit\nprotocols for obtaining information. Consequently, by using\nthese protocols, the results attained by the systematic review\ncan be reproduced by other researchers as a way of validation,\ndecreasing the incidence of bias in the review, a problem\nboosted in non-systematic bibliographic reviews [33].\n\nSystematic reviews are commonly applied in other areas,\nmainly in medicine, and have a number of reported benefits\n[33]. In the area of computing, this review method is more\ndisseminated in software engineering [7]. This paper con-\ntributes to the use of systematic review in computing, partic-\nularly in artificial intelligence. Here, we discuss how the sys-\ntematic review was applied and the achieved results, which\nare valuable information for the area of intrusion detection\nwith keystroke dynamics.\n\nThis paper presents a systematic review carried out with\nthe aim of identifying the state of the art in keystroke dynam-\nics applied to intrusion detection. Preliminary results of this\nreview are shown in [42] and [41]. The remaining sections are\norganized as follows: in Sect. 2, basic concepts of keystroke\ndynamics are introduced; in Sect. 3, the process of system-\natic review is presented; Sect. 4 discusses how the systematic\nreview was applied in this work, specifying the review proto-\ncol and the steps adopted; in Sect. 5, the results obtained\nby the systematic review are summarized; and, finally,\nSect. 6 presents our conclusions.\n\n2 Background\n\nIn information security, intrusion detection is the process of\nmonitoring events in a computer or network and analyse them\nto detect signals of possible incidents, which are violations\nor threats of violations of security policies, acceptable use or\nsecurity practices [45]. An intrusion detection system (IDS)\nautomatizes this process.\n\nAs previously discussed, more recently, a new concept\nof detecting intrusions by the analysis of the user behaviour\nin the computer has emerged [39], which is performed by\nthe behavioural IDS [49]. This type of system is grounded\non a concept known as user profiling, which consists of\nobserving the behaviour of a user in order to generate mod-\nels that represent its normal behaviour. Observed events\nare then compared to these models and possible devia-\ntions are classified as potential intrusions [46]. An IDS\nthat applies user profiling is a system based on anomaly\ndetection, as it generates alarms for events that deviates\n\nKeystroke dynamics,\nApplication usage, etc.\n\nUser\n\nTraining\n\nRecognition\n\nGet profile\n\nYes/No\n\nTraining\nphase?\n\nS\n\nN\n\nUser profile\n\nStore profile\n\nFig. 1 Behavioural intrusion detection (adapted from [42])\n\nfrom a behaviour pattern. Figure 1 represents the basic\nflow of a behavioural IDS, which involves two major steps\n[16,21]:\n\n– Training: obtaining features for the definition of the user\nbehavior pattern;\n\n– Recognition: matching observed features against user\nbehavior pattern.\n\nA key issue in the application of user profiling is how to\ndefine the profile, that is, which aspects will be observed.\nThe process of choosing these aspects is one of the major\nquestions when applying user profiling. Ideally, the chosen\naspects should allow the identification of a user within a\ngroup of users and, at the same time, maintain similar values\nthrough the time for the same user [21]. There is a number of\naspects that can be used for the definition of the user profile,\nsuch as keystroke dynamics, system audit logs, e-mail and\ncommand line use [46].\n\nThis work studies keystroke dynamics as an aspect to\nbe analysed by the behavioural intrusion detection sys-\ntem. Keystroke dynamics analyzes how users type from\nthe monitoring of the keyboard input. As a result, mod-\nels that represent the regular typing rhythm of the user are\ndefined. Afterwards, these models are used for the recogni-\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 575\n\ntion [28], in such a way that typing rhythms deviating from\nthis model are classified as being from intruders. Here, we\nhave chosen keystroke dynamics instead of other aspects\nbecause it may be used either in the initial authentication\nof a system or as continuous authentication after the ini-\ntial authentication. It makes this technology more flexible\nthan an analysis of systems audit logs or e-mail behav-\niour.\n\nKeystroke dynamics can be applied in two ways: static\ntext or dynamic text. Static text only performs an analysis\nof fixed expressions as, for example, a password. While, in\ndynamic text, the analysis occurs for any text that is typed by\nthe user. Keystroke dynamics in static text requires less effort\nto be implemented and it also reached lower error rates in\nliterature [11].\n\nTwo distinctive processes are involved in keystroke dynam-\nics: feature extraction and classification of the extracted\nfeatures. In the first process, a number of features are\nextracted for the recognition of a user. These features\nshould represent how the user behaves in terms of keystroke\ndynamics.\n\nIn the second process, which corresponds to the feature\nclassification, several algorithms can be used. For instance,\nmachine learning algorithms, like neural networks [48] and\nsupport vector machines [19], were applied in this classifica-\ntion, which consists of verifying whether the typing features\nbelong or not to a specific user.\n\n3 Systematic review\n\nSystematic literature review (called just systematic review\nin this paper) is a method for conducting bibliographic\nreviews in a formal way, following well defined steps, which\nallows the results to be reproducible. In addition, the pro-\ntocol adopted for the conduction of the review must assure\nits completion. This review method is commonly used in\nother areas, mainly in Medicine [7] and has several reported\nbenefits, like less susceptibility to bias [33]. In the area of\nComputing, this method of review is more disseminated in\nSoftware Engineering.\n\nThe application of the systematic review involves three\nmajor phases: planning, conduction and presentation of\nresults. In the first phase, a review protocol is defined, in\nwhich research questions are specified along with search\nstrategies. After that, in the second phase, the review pro-\ntocol is applied and the information is extracted from the\nreturned references. References used for the extraction of\ninformation are called primary studies, while the review\nis a secondary study. Finally, the third phase defines the\nway to present the results and the final report is done.\nThe items comprehended in each of the three phases are\n[33]:\n\n3.1 Planning\n\n– Identification of the review need: a systematic review has\nthe goal of summarizing all information regarding a spe-\ncific topic. However, before starting a systematic review,\nthe need of this review has to be checked. This check-\ning, for instance, should verify the existence of previ-\nously published systematic reviews that deal with the\ntopic under investigation and whether the protocol of\nthese reviews meet the requirements of the research.\n\n– Commissioning (optional): in some cases, due to the lack\nof time or specific knowledge, one may need to request\nthat other researchers conduct the systematic review.\n\n– Specification of the research questions: this is considered\nto be the most important part of the systematic review,\nas these questions will guide all the following steps, as\nthe search for primary studies, extraction and analysis of\ninformation.\n\n– Development of the review protocol: this step defines\nstrategies to be used for the search, selection and eval-\nuation of the references. In addition, the information to\nbe extracted from each of the selected references is also\ndefined.\n\n– Protocol evaluation (optional): as the review protocol is\nan essential part of the systematic review, it is recom-\nmended to be reviewed by other researches.\n\n3.2 Conduction\n\n– Reference search: search for the greatest possible number\nof references which can answer the research question in\norder to avoid bias. In the systematic review, the search is\nperformed with increased rigour, with the pre-definition\nof search expressions and databases, making it different\nfrom traditional reviews.\n\n– Selection of primary studies: after reference search, the\nstudies that are in fact relevant for the research must be\nselected, by the use of inclusion/exclusion criteria.\n\n– Quality evaluation: each of the selected references\nundergo a quality evaluation. This evaluation may be\nused with diverse aims, like contributing for the inclu-\nsion/exclusion criteria or supporting the summary results,\nby measuring the importance of each study.\n\n– Information extraction: the information extraction from\nthe references must be done with the support of forms\ndefined during the planning phase of the systematic\nreview.\n\n– Data synthesis: this step corresponds to summarizing the\nresults attained during the review. This summary may\ninvolve qualitative and quantitative aspects. For quanti-\ntative aspects, a meta-analysis may also be applied.\n\n123\n\n\n\n576 J Braz Comput Soc (2013) 19:573–587\n\n3.3 Reporting the review\n\n– Specification of the dissemination mechanisms and for-\nmulation of the report: dissemination of the results\nattained by the systematic review. This can be done by\npublishing in academic journals and conferences or even\nin web sites.\n\n– Report evaluation (optional): this evaluation can be\nrequested to experts in the area of the research. If the\nreview is submitted to a journal or conference, the review\nprocess of the publication can be considered an evalua-\ntion of the report.\n\nThe explicit definition of the review protocol allows the\nresults to be reproduced. The review presented in this paper\nwas performed by two researchers in the planning phase,\nbut by just one in the conduction phase. Due to that, this\nreview can be called a quasi-systematic review, as it follows\nthe principles of a systematic review, but was not conducted\nby two researchers in all phases. This term, quasi-systematic\nreview, was also used in previous work [35]. More details on\nhow to carry out each of the phases are discussed in the next\nsections, in which the systematic review process is applied to\nthe topic of keystroke dynamics for intrusion detection.\n\n4 How the systematic review was applied\n\nIn this work, the application of the systematic review has the\ngoal of studying the state of the art in keystroke dynamics in\norder to identify:\n\n1. Advantages and disadvantages of using keystroke dynam-\nics in intrusion detection;\n\n2. Extracted features;\n3. Classification algorithms applied;\n4. Performance measures commonly adopted;\n5. Benchmarking datasets, which are useful for conducting\n\ncomparative experiments in the area.\n\nBefore presenting details of how the systematic review\nwas applied in this work, it is important to highlight that we\nonly considered references indexed by reference databases\navailable on the Internet and written in English.\n\n4.1 Planning\n\nAccording to a research carried by the authors, there are\nno published systematic reviews that meet the goals of this\nwork. Besides, the newer review article on keystroke dynam-\nics known by the authors was submitted for publication in\n2009 [28]. Moreover, part of our aims was not met in that\n\npublication, as the identification of benchmarking datasets.\nHence, the conduction of the review in this work is justified.\n\n4.1.1 Research questions\n\nIn view of the need of the systematic review, we defined a\nresearch question and some respective sub-questions to meet\nthe established goals:\n\nHow keystroke dynamics is used for intrusion\ndetection?\n\n– What are the advantages and disadvantages of using\nkeystroke dynamics for intrusion detection?\n\n– What features are extracted from the typing data?\n– What classification algorithms are applied? What algo-\n\nrithms are used in the performance comparisons?\n– What measures were used to evaluate the performance?\n\nWhat was the performance achieved?\n– What datasets are used to measure the performance of\n\nthe classifier? How many users took part in the tests\nperformed?\n\n4.1.2 References search\n\nAfter defining the research question, we enumerated a list\nof terms related to papers that could answer it: keystroke\ndynamics, typing dynamics, keystroke biometric(s), key-\nstroke authentication, keystroke pattern(s), typing pattern(s),\nbehaviour intrusion detection, behavior intrusion detection,\nbehavioral IDS, biometric intrusion detection, user profil-\ning, behavioural biometrics, behavioral biometrics, contin-\nuous authentication, typing biometric(s), keypress biomet-\nric(s), keystroke analysis. The use of various terms for the\nsame topic, sometimes even synonyms, contributes to the\ncompleteness of the search [1]. From this list of terms, we\nbuilt search expressions for each database of references. The\nbasic search expression is the conjunction of each term in the\nlist using the logical connective O R.\n\nNevertheless, after some tests with this search expres-\nsion, we observed that many of the returned references dealt\nwith topics not related to the research question, as personal-\nization systems and recommender systems. For this reason,\nsome terms that could exclude these unrelated topics were\nidentified: web search, personalized information, personal-\nized content, content delivery, recommendation system, rec-\nommendations system, information retrieval, personalizing,\npersonalization, recommender. The basic search expression\nwas then modified to consider the exclusion terms with the\nuse of the logic connective AN D and N OT together, as\nfollows:\n\n(‘‘behavioural intrusion detection’’\nOR ‘‘behavioral intrusion detection’’\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 577\n\nOR ‘‘behavioral IDS’’\nOR ‘‘behavioural IDS’’\nOR ‘‘biometric intrusion detection’’\nOR ‘‘user profiling’’\nOR ‘‘keystroke dynamics’’\nOR ‘‘typing dynamics’’\nOR ‘‘keystroke biometrics’’\nOR ‘‘keystroke biometric’’\nOR ‘‘continuous authentication’’\nOR ‘‘keystroke authentication’’\nOR ‘‘behavioural biometrics’’\nOR ‘‘behavioral biometrics’’\nOR ‘‘keystroke pattern’’\nOR ‘‘keystroke patterns’’\nOR ‘‘typing pattern’’\nOR ‘‘typing patterns’’\nOR ‘‘typing biometric’’\nOR ‘‘typing biometrics’’\nOR ‘‘keypress biometric’’\nOR ‘‘keypress biometrics’’\nOR ‘‘keystroke analysis’’)\n\nAND NOT\n\n(‘‘web search’’\nOR ‘‘personalized information’’\nOR ‘‘personalized content’’\nOR ‘‘content delivery’’\nOR ‘‘recommendation system’’\nOR ‘‘recommendations system’’\nOR ‘‘information retrieval’’\nOR ‘‘personalizing’’\nOR ‘‘personalization’’\nOR ‘‘recommender’’)\n\nThis search expression was applied in several data-bases\nthat included references in the computing area. As each data-\nbase has differences in its syntax for search expression, the\nbasic search expression presented here was adapted to each\ndatabase, as specified in Appendix A. The following data-\nbases were considered in this work:\n\n– ACM Digital Library\n(http://dl.acm.org/)\n\n– IEEE Xplore\n(http://ieeexplore.ieee.org/)\n\n– Science Direct\n(http://www.sciencedirect.com/)\n\n– Web of Science\n(http://isiknowledge.com/)\n\n– Scopus\n(http://www.scopus.com/)\n\n4.1.3 Selection criteria\n\nThe last part of the planning phase is the definition of\nthe selection criteria (inclusion and exclusion) that will be\napplied to the returned references. In this systematic review,\nall the returned references are included for analysis in the\nnext steps, except the ones that meet the following exclusion\ncriteria:\n\n1. Publications that do not deal with keystroke dynamics\nfor intrusion detection: the aim of this review is to work\nwith intrusion detection, which comprehends authentica-\ntion systems. Therefore, references that do not meet this\nrequirement were not included.\n\n2. Publications with one page, posters, presentations, abstra-\ncts and editorials, texts in magazines/newspaper and\nduplicate publications in terms of results, except the most\ncomplete version: references without enough informa-\ntion to answer the research question. This criterion also\navoids unnecessary work for the cases in which the same\nstudy is published in different versions.\n\n3. Publication hosted in services with restricted access and\nnot accessible or publications not written in English.\n\nIn this phase, we also created a quality score to be applied\nto the returned references. This score was determined to high-\nlight references that better answer our research question. The\nvalue of the quality score is the sum of the score reached in\neach of the assessed items. For each of these items, the ref-\nerence scores 1 if fully meets it, 0.5 if partially meets it and\n0 if does not meet the assessed item. As there are nine items,\nthe possible scores ranges between 0 and 9, in such a way\nthat higher values indicate better publications according to\nthe established research criteria. The items are:\n\n1. Were the goals clearly presented in the beginning of the\nwork?\n\n2. Were the advantages/disadvantages of keystroke dynam-\nics discussed?\n\n3. Is the dataset available to be reused?\n4. Was it detailed how the feature vector is generated?\n5. Were the values of the algorithm parameters presented?\n6. Were the applied approaches detailed so as to allow them\n\nto be replicated?\n7. Were experimental tests conducted?\n8. Were the results compared to previous researches in the\n\narea?\n9. Were the limitations of the study presented?\n\nThe quality criteria were defined considering that researc-\nhes may present problems in the following steps: design,\nconduction, analysis and conclusion [33]. The items 1 and\n\n123\n\nhttp://dl.acm.org/\nhttp://ieeexplore.ieee.org/\nhttp://www.sciencedirect.com/\nhttp://isiknowledge.com/\nhttp://www.scopus.com/\n\n\n578 J Braz Comput Soc (2013) 19:573–587\n\n2 refer to the design step, the items 3–6 to the conduction\nstep, the items 7–8 to the analysis step and the item 9 to the\nconclusion step. Part of the items used to assess the quality\nwas based on the list in [33], which presents several items to\nbe evaluated in references.\n\n4.1.4 Information extraction\n\nStill in the planning phase of the systematic review, we\ndefined a set of information to be extracted from each selected\nreference (after the application of the exclusion criteria), as\nfollows:\n\n– Basic information about the publication (title, authors,\nname and year of publication)\n\n– Were performance tests conducted?\n– Type of device (e.g. PC, mobile)\n– Best performance achieved: algorithm, measure and\n\nperformance\n– Number of users in the tests\n– Algorithms used in the tests\n– Extracted features\n– Is the test dataset available to be reused? Where?\n– Type of verification: static text or dynamic text?\n– Observations\n\nThese items were defined in line with the research question,\nin order to answer it and guide the information extraction in\nthe conduction phase of this review.\n\n4.2 Conduction\n\nFrom the review protocol defined in the planning phase, the\nconduction of the systematic review was started.\n\n4.2.1 Application of the search expressions\n\nThe first step was to apply the search expressions in each\ndatabase of references and save the returned results. Apart\nfrom the returned references, we also included a reference\npreviously known by the authors, but not indexed by the data-\nbases used in this review: [15]. This reference is mentioned\nin several papers as being one of the first publications about\nkeystroke dynamics. Table 1 shows the number of references\nreturned by each database on 18/February/2013.\n\nThese results were centralized in order to continue the\nreview, using a tool called Mendeley (available in: http://\nwww.mendeley.com/). We used this tool to import the results\nexported from the databases. Mendeley has a series of use-\nful features that can be used for systematic reviews, such as\nsearch for duplicates, organization of references by category\nand associations of the entries with PDF files stored in the\ncomputer.\n\nTable 1 Number of returned references\n\nDatabase Number of references\n\nACM Digital Library 71\n\nIEEE Xplore 308\n\nScience Direct 104\n\nWeb of Science 596\n\nScopus 943\n\nGaines et al. [15] 1\n\nTotal 2, 023\n\n4.2.2 Selection of references\n\nAfter the centralization of the information returned from the\nsearch databases, duplicate references were removed. Dupli-\ncate references may appear since databases can have some\nintersection in the indexed data, as in the case of Scopus and\nWeb of Science.\n\nOnce the removal of duplicates was finished, a fast read-\ning of the text of the remaining references was performed.\nBefore starting this step, we needed to download the com-\nplete text of each publication. However, it was not possible\nto download 27 of them, which were hosted in services not\navailable from our university (exclusion criterion 3). Conse-\nquently, the number of eligible references was again reduced.\nIn the end, another fast reading of the eligible references was\nperformed to revalidate the exclusion criteria 1 and 2. A great\nnumber of references that do not deal with keystroke dynam-\nics for intrusion detection has been eliminated just by the title\nand abstract, nevertheless, some references were eliminated\nonly after reading their full text. Once the exclusion crite-\nria 1 to 3 were applied, secondary studies were removed,\nwhich were only three: [11,28,40]. Secondary studies are\nthose commonly known as reviews or surveys. Table 2 shows\nthe number of references returned after the application of\neach step.\n\nWith the application of all exclusion criteria, 200 refer-\nences (Table 2) were left for the next steps: information\nextraction and quality assessment. Aiming at accelerating\nthese tasks, we created a spreadsheet with all the items for\ninformation extraction and quality assessment discussed in\n\nTable 2 Number of references after each step\n\nStep Number\n\nTotal of references 2,023\n\nAfter elimination of duplicates and exclusion\ncriteria 1 and 2\n\n230\n\nAfter exclusion criterion 3 203\n\nAfter exclusion of secondary studies 200\n\n123\n\nhttp://www.mendeley.com/\nhttp://www.mendeley.com/\n\n\nJ Braz Comput Soc (2013) 19:573–587 579\n\nthe planning phase (Sect. 4.1). This spreadsheet was then\nfilled with the information from the references.\n\nThis was the part of the systematic review that consumed\nmore time due to the need to read in detail several texts. In\naddition, sometimes the information to be extracted were not\npresent in a direct way in the text. For example, in some pub-\nlications, there were tables summarizing tested algorithms\nand their performance [19] or it was even possible to extract\nalmost all information from the abstract [22]. However, this\nwas not the case of some publications, which needed to be\nread more deeply to find the desired information. Actually,\nthis observation may be related to the one mentioned in [7],\nwhich highlights the fact that abstracts in Computing are usu-\nally not well structured, making it difficult to get informa-\ntion about the publication only by the abstract. According to\n[7], the scenario is different in medicine, area in which the\nabstracts are, in general, better structured and usually contain\nmore information about the publication.\n\n4.2.3 Quality assessment\n\nDue to the high number of selected references, they were\nsorted in descending order of quality score and only the ones\nwith the highest scores are discussed in details here. For the\npurpose of this review, only those papers with quality score\nequals or higher than 7.5 were considered, resulting in 16\npublications. The focus on references with higher scores has\nthe goal of spending greater efforts on references more rel-\nevant to the research question, as the quality scores were\nspecially designed with this purpose.\n\nThe graph in Fig. 2 shows the number of publications for\neach quality score. The average score among those different\nfrom zero was 5.54 and, as shown in Fig. 2, the scores follow\nan approximate normal distribution. The maximum reached\nscore was 8.5.\n\nAnother aspect analysed was the number of selected publi-\ncations by year, as shown in the graph in Fig. 3. In this graph,\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n35\n\n0 1 2 3 4 5 6 7 8 9\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nQuality Score\n\nFig. 2 Publications by quality score\n\n0\n\n5\n\n10\n\n15\n\n20\n\n25\n\n30\n\n1998 2000 2002 2004 2006 2008 2010 2012\n\nN\nu\n\nm\nb\n\ner\n o\n\nf \nP\n\nu\nb\n\nlic\nat\n\nio\nn\n\ns\n\nPublication Year\n\nFig. 3 Publications by year in keystroke dynamics. The growth trend\nillustrates that the field is gaining new momentum, justifying additional\nresearch efforts\n\nit is important to highlight the growth trend in the number of\npublications by year in the area of keystroke dynamics. This\ntrend was higher between 2002 and 2006. Such a growth\ntrend indicates that the area has been receiving more atten-\ntion from the scientific community. This may justify addi-\ntional research efforts in keystroke dynamics.\n\nBoth graphs consider only the references with available\ntexts.\n\n5 Results\n\nIn this section, we focus on the 16 publications with highest\nquality score and on some papers referenced by them. The\nfollowing subsections are organized in such a way to answer\neach of the research sub-questions: advantages and disadvan-\ntages of keystroke dynamics, feature extraction, classifica-\ntion algorithms, performance evaluation and benchmarking\ndatasets.\n\n5.1 Advantages and disadvantages\n\nAuthentication of users is done by the use of credentials, also\nknown as authentication factors, which can be [47]:\n\n1. what the user knows (e.g. password);\n2. what the user has (e.g. access card, token);\n3. what the user is/does (e.g. biometrics: recognition by fin-\n\ngerprint, iris, keystroke dynamics, voice recognition);\n4. some combination of the above items.\n\nThe primary method of authentication, be it for\ne-commerce or for military purposes, is a simple login and\npassword [12]. The use of this method is based on the fact that\nthe secrecy of the password will be held [40]. However, this\nis not always the case, implying in a number of weaknesses\n[10]:\n\n123\n\n\n\n580 J Braz Comput Soc (2013) 19:573–587\n\n– Passwords may be shared by several users, resulting in\nunauthorized access;\n\n– Passwords may be copied without authorization;\n– Passwords may be guessed, particularly for easy pass-\n\nwords, as when someone uses his/her birthday as a pass-\nword [43].\n\nMoreover, even in scenarios in which the user authenti-\ncation is performed by the use of access cards, the security\nis compromised. This is because the card ownership can be\nshared with an unauthorized user and it may also be stolen\n[26].\n\nThese problems, along with widespread use of the Web,\ncontributed to expansion of identity theft, which occurs when\na person uses personal information of someone else to ille-\ngally pretend to be this person [38]. In recent years, identity\ntheft has become a crime with the rate of greatest growth in\nthe USA [6]. Furthermore, the sum of losses in the world due\nto identity theft have been estimated to be around US$ 221\nbillion in 2003 [25]. According to research, [29], weaknesses\nof passwords was the most exploited factor by insiders (users\nfrom the same institution which is the victim of the attack).\n\nOne way to mitigate this problem is the use of biometric\ntechnologies to enhance the security provided by passwords.\nIn the security context, biometrics is a science which studies\nmethods for the determination of user identity based on phys-\niological and behavioral features [26]. Keystroke dynamics,\nwhich is considered a biometric technology, can be used with-\nout any additional cost with hardware, in contrast to other\nbiometric technologies (e.g., iris, fingerprint), which need\nspecific devices for the capture of biometric data [24,37].\nIn addition, the level of transparency in the use of keystroke\ndynamics is high [40]. This means that there is no need to\nperform specific operations for the authentication by key-\nstroke dynamics [3]. This factor contributes for an increased\nacceptance of keystroke dynamics among users.\n\nRecognition precision by keystroke dynamics may be\naffected in the presence of keyboards with different charac-\nteristics in the same environment. Nevertheless, it is expected\nthat such differences does not significantly impair the recog-\nnition performance and, consequently, still enable proper\nuser identification [24]. This can be compared to the sig-\nnature recognition biometrics in which, regardless of the pen\nused, the system is still able to differentiate between legiti-\nmate and illegitimate users [24].\n\nFurthermore, false alarm rates (when a legitimate user\nis classified as an intruder) in keystroke dynamics are usu-\nally high and do not meet standards in some access con-\ntrol systems, such as the European. Additionally, differences\namong systems, like precision in the capture of typing times,\nmay negatively affect the performance of the classifier by\nintroducing noise [30]. Another issue raised in the area of\nbehavioral biometrics is the adaptation to changing profiles.\n\nTime\n\nKey 1\n\nKey 2\n\nKey 3\n\nUU\n\nDD\n\nUD\n\nDU2\n\nDU1\n\nFig. 4 Typing data and features (adapted from [42])\n\nA person may change the behavior over time as a result of\nlearning and such a change should be included in the profile\nstored in the security system, otherwise performance may be\nimpaired. However, this task is far from being simple and\nrepresents a challenge in the area [27].\n\n5.2 Extracted features\n\nApart from the text itself, the keyboard provides the instants\nin which each key is pressed and released. From these basic\ndata, features are extracted and used as input for the classi-\nfication algorithm. In this paper, we adopted the following\nnotation to represent the extracted features (Fig. 4 shows\nthese features in a graphical way, in which the down and up\narrows represent, respectively, the instants of pressing and\nreleasing of each key):\n\n– DU1: time difference between the instants in which a key\nis pressed and released. This feature represents the time\nthat the key keeps pressed and is also named by some\nauthors as dwell time [38].\n\n– DU2: time difference between the instants in which a key\nis pressed and the next key is released.\n\n– UD: time difference between the instants in which a key\nis released and the next is pressed. This feature is also\nknown as flight time [38].\n\n– DD: time difference between the instants in which a key\nis pressed and the next key is pressed.\n\n– UU: time difference between the instants in which a key\nis released and the next key is released.\n\nThe feature vector is then generated based on these fea-\ntures. An example of a feature vector for an expression of\nfour keys is shown in Fig. 5.\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 581\n\nTime\n\nKey 1 Key 2 Key 3\n\nUD1 UD2 UD3\n\nGenerated feature vector\n\nKey 4\n\nFig. 5 Example of a feature vector (adapted from [41])\n\nA summary of the features used in each of the selected\nreferences is shown in Table 3. From the data on this table,\nwe generated the histogram shown in Figure 6. As can be\nobserved, features DU1 (dwell time) and UD (flight time) are\nthe most used.\n\nAnother feature used in previous researches was the pres-\nsure over the keys [8,13], but the extraction of this feature\nrequires the use of specialized hardware. However, in view\nof the increasing availability of touch screen devices, costs\nto use this feature may decrease over time. In a recent work\n[8], the pressure of a touch-screen smartphone was evaluated\nin a keystroke dynamics scenario. Error rates decreased from\n12.2 to 6.9 % when the pressure was also considered.\n\nIn [37], a process of equalization over the feature vec-\ntor was applied. The authors argue that this transformation\nmay highlight important aspects of the feature vector, as\nobserved in other areas, like digital communications and\nimage processing. According to the reported results, the\napplication of this equalization improved the performance\n(lower error rate) attained by several algorithms from previ-\nous researches.\n\nStudies from [17,19] evaluated the use of discretization\nover the feature vectors. Each value in the feature vector is\ndiscretized in five ranges. Discretized data is then classified\nby a two-class SVM, using both negative and positive sam-\nples for training. According to the authors, the application\nof the SVM together with this discretization obtained lower\nerror rates than other approaches seen in the literature (e.g.,\nneural networks and distance-based classifiers).\n\nIn [24], the authors performed a comparative analysis of\nseven feature sets. All combinations using DU1, DD and UU\nwere considered. the best performance was achieved by the\nset DU1, UU. However, the feature UD was not considered in\ntheir analysis. UD is one of the most used feature in previous\npapers, according to our review, as shown in Fig. 6.\n\nAnother study on extracted features was conducted by [3].\nIn addition to considering “character” keys, this study also\ninvestigated the Shift key. In passwords containing a mixture\n\nTable 3 Extracted features in keystroke dynamics\n\nReference Extracted features\n\nMontalvao et al. [37] DD\n\nDD with equalization\n\nGiot et al. [17] UU, DD, UD, DU2\n\nGiot et al. [19] UU, DD, UD, DU2 and\ntotal typing time\n\nKillourhy and Maxion [30] DU1, UD\n\nRodrigues et al. [43] UD, DU1\n\nUD, DU1, UU, DD\n\nHosseinzadeh and Krishnan [24] DU1\n\nDD\n\nUU\n\nUU, DD\n\nDU1, DD\n\nDU1, UU\n\nDU1, UU, DD\n\nKillourhy and Maxion [31] DU1, DD, UD\n\nDU1, DD\n\nDU1, UD\n\nBartlow and Cukic [3] DU1, UD (average,\nstandard deviation,\nsum, minimum and\nmaximum), including\nthe Shift key\n\nChang [9] DU1, UD\n\nMontalvao Filho and Freire [14] DD\n\nDD with equalization\n\nGunetti and Piccardi [22] DU1, UD\n\nMonrose and Rubin [36] DU1, UD\n\nYu e Cho [48] DU1, UD\n\nGiot et al. [20] UU, DD, UD, DU1\n\nChang et al. [8] DU1, UD, DD, pressure\n\nKillourhy and Maxion [32] DU1, DD\n\n0\n\n2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\nDU1 DU2 UD DD UU\n\nFeature\n\nNumber of Works\n\nFig. 6 Number of references that employed each feature\n\n123\n\n\n\n582 J Braz Comput Soc (2013) 19:573–587\n\nof lower case and upper case letters, the Shift key is normally\nused. Consequently, the analysis of the Shift key may be an\nadditional factor to classify users. According to their tests,\nanalysing the Shift key reduces the error rates of the classifier.\n\nAn important factor in keystroke dynamics is the resolu-\ntion of the captured data. In the MS Windows operating sys-\ntem, for example, the notification of keyboard events, such as\nkey press and release, does not distinguish differences lower\nthan 15.625 ms. In [30], the effect of different resolutions\nwas evaluated. This evaluation used an external device with\n\na resolution of 100 µs. High resolution data was then used\nto derive lower resolution samples. As expected, higher res-\nolution data implies in better classification accuracy. Low\nresolutions (e.g., 100 ms) resulted in error rates of 50 %,\nwhich is a very low performance.\n\n5.3 Classification algorithms\n\nA number of algorithms have been used to classify users\nin keystroke dynamics. Table 4 shows the algorithms studied\n\nTable 4 Classifiers used in\nkeystroke dynamics Reference Classifier\n\nMontalvao et al. [37] Bleha [4]\n\nMonrose and Rubin [36]\n\nGunetti and Picardi [22]\n\nGiot et al. [17] SVM\n\nStatistical\n\nNeural network\n\nClassifier based on distance\n\nGiot et al. [19] SVM\n\nStatistical\n\nClassifier based on Euclidean distance\n\nClassifier based on Hamming distance\n\nKillourhy and Maxion [30] Nearest neighbour\n\nNeural network\n\nMean-based classifier\n\nRodrigues et al. [43] Hidden Markov Model (HMM)\n\nStatistical\n\nHosseinzadeh and Krishnan [24] Gaussian Mixture Model (GMM) + Leave one out method\n\nKillourhy and Maxion [31] Nearest neighbour\n\nOutlier count (z-score)\n\nManhattan distance\n\nBartlow and Cukic [3] Random Forests\n\nChang [9] Tree-based with Euclidean distance\n\nMontalvao Filho and Freire [14] Bleha [4]\n\nMonrose and Rubin [36]\n\n1D-Histogram and 2D-Histogram\n\nGunetti and Piccardi [22] Proposed Methods: R Measure and A Measure\n\nMonrose and Rubin [36] Euclidean distance\n\nWeighted and non-weighted probability\n\nBayes\n\nYu e Cho [48] SVM 1\n\n2-layer and 4-layer Auto Associative Multi-layer Perceptron\n(AAMLP)\n\nGiot et al. [20] Based on Gaussian distribution [23]\n\nChang et al. [8] Statistical [5]\n\nKillourhy and Maxion [32] Statistical\n\nDisorder-based\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 583\n\nin the 16 selected publications. It is important to highlight\nthat, apart from algorithms known from Machine Learning\nliterature, such as Support Vector Machines (SVM) [19] and\nNearest Neighbour [30], some authors proposed some new\nalgorithms [22,36]. These new algorithms were also used in\ncomparisons performed by later researches [37].\n\nThe use of static and dynamic text was tested in [36]. At\nthe time the work was published, the concept of recogniz-\ning users by keystroke dynamics was relatively new. There-\nfore, the authors carried out experiments to validate the idea\nof classifying users by their typing rhythm. Their experi-\nments validate the approach, achieving an accuracy rate of\n92.14 %.\n\nAs discussed in previous works [19,31], the amount of\ntraining samples may affect the classifier performance. In\ngeneral, the greater their representativity, the higher is the\nclassification accuracy. In [9], a method to generate new train-\ning samples based on the legitimate user was proposed. The\nsamples are generated using re-sampling in time domain and\nby the use of discrete wavelet transform (DWT). Although\nthe this method generate more samples, a question still not\nanswered is whether these new samples actually imply in\ngreater representativity.\n\nThe use of numeric keypads was analysed by [43].\nAn advantage of using numeric keypads is that it would\nbe easier to implement keystroke dynamics technology in\nmobile devices, such as cell phones, which usually only\nhave a numeric keypad. The authors conducted experi-\nments using eight number passwords, obtaining an ERR of\n3.6 %.\n\nNovelty detectors were tested in [48], namely an auto-\nassociative multilayer percetron (AAMLP) and a one-class\nsupport vector machine (one-class SVM). According to their\nexperiments, error rates were similar for both novelty detec-\ntors. Nevertheless, the one-class SVM was more efficient in\nterms of computational resources usage.\n\nSeveral tools were used to carry out the tests of the clas-\nsification algorithms in these papers. In the case of neural\nnetworks, two tools were identified: the library ffnet and the\npackage AMORE, which were employed by [19] and [30]\nrespectively. For the other algorithms, we identified the fol-\nlowing tools: [19] applied the library libsvm for a SVM and\n[43] applied the Hidden Markov Toolkit (HTK) for training a\nHMM. Some classification algorithms were implemented by\nthe authors using programming languages, such as Java in\nthe Net Beans development environment [3] and C++ with\nthe library xview [36].\n\n5.4 Performance evaluation\n\nWith regard to the performance evaluation, through the\nreview, we found four main measures:\n\nFAR FRR\n\nSensitivity level\n\nE\nrr\n\nor\ns\n\nEER\n\nFig. 7 FAR, FRR and EER (adapted from [11])\n\nFRR\n\nF\nA\n\nR\nIntegrated\n\nError\n\nFig. 8 Example of integrated error (adapted from [34])\n\n– FAR and FRR: the false acceptance rate (FAR) measures\nthe percentage of times that an intruder is erroneously\naccepted as being legitimate and the false rejection rate\n(FRR) measures the percentage of times that a legitimate\nuser is wrongly rejected [40]. Hypothetically, these two\nrates vary according to the graph in Fig. 7, depending\non the sensitivity level of the algorithm: when one rate\ndecreases, the other increases.\n\n– EER: the equal error rate (EER) represents the error value\nwhen both FAR and FRR assume the same value [11]. In\ncontrast to FAR and FRR, this measure does not depend\non the level of sensibility of the classification algorithm.\n\n– Accuracy rate: only measures the percentage of correct\nclassifications attained by the algorithm.\n\n– Integrated error: is the area under the curve plotted with\nFAR and FRR rates, as shown Fig. 8. The value of the\nshaded area is the integrated error. Smaller areas repre-\nsent better performance.\n\nSeveral aspects may affect the performance of a biomet-\nric system based on keystroke dynamics. In [31], the authors\nstudied which aspects have the major influence on keystroke\n\n123\n\n\n\n584 J Braz Comput Soc (2013) 19:573–587\n\nTable 5 Best performance achieved by classifiers (EER)\n\nClassifier Users EER (%)\n\nGunetti and Picardi [37] 205 13\n\nSVM [19] 100 6.95\n\nNearest neighbor [30] 51 9.96\n\nHidden Markov Model [43] 20 3.6\n\nBleha (with equalization) [14] 47 6.2\n\nManhattan distance [31] 51 7.1\n\nGMM [24] 41 4.4\n\nBased on Gaussian distribution [20] 83 8.87\n\nStatistical [8] 100 6.9\n\nTable 6 Best performance achieved by classifiers (FAR and FRR)\n\nClassifier Users FAR (%) FRR (%)\n\nRandom Forests [3] 53 1 14\n\nTree-based with 12 0 3.47\n\nEuclidean distance [9]\n\nGunetti and Piccardi: 205 0.005 5\n\nR Measure [22]\n\n4-layer AAMLP [48] 21 0 0.25\n\ndynamics performance. Their study showed that the classifi-\ncation algorithm, the amount of training samples and meth-\nods to update the user model play a key role in the system\nperformance. Other aspects, such as the set of extracted fea-\ntures and the user typing experience had minor effects on the\noverall performance.\n\nAnother fundamental issue in performance evaluation is\nregarding the way keystroke data is collected. For instance, a\nuser may type a predefined text (transcription) or just freely\ntype something (free composition). Most papers in keystroke\ndynamics adopt the transcription method as it is easier to\napply. However, does it have an impact on the classifier per-\nformance? A recent study showed that there are no significant\ndifference between the two methods [32]. Thus, the authors\nencourage researches to continue using transcription.\n\nTables 5 and 6 summarize the best results reached in the\nselected references. The first table shows the papers that used\nEER to measure the performance and the second table shows\nthe papers that evaluated the performance using FAR and\nFRR. One of the returned papers reported the results in terms\nof accuracy rates and, therefore, it is not shown in Tables 5\nand 6. Based on a Bayesian classifier, the accuracy rate\nattained was 92.14 % in a dataset containing 63 users [36].\n\nNonetheless, the comparison of studies just by the reported\nperformance values cannot be done directly, as there is a num-\nber of differences between them, like dataset and evaluation\nmeasures used. According to Tables 5 and 6, the number of\nusers that took part in the tests was quite different among the\n\nselected studies, ranging from 12 to 205. Moreover, even\nwhen the same algorithm is applied by some papers, the\ncomparison is still complex as the parameter values may\nbe different. This difficulty in performing comparisons in\nthe area of keystroke dynamics due to the non-uniformity\nbetween researches was also mentioned in [40]. The use\nof benchmarking datasets can improve this scenario, as it\nwould allow a more reliable comparison between studies in\nkeystroke dynamics.\n\n5.5 Benchmarking datasets\n\nIn view of the fact that performance in keystroke dynamics is\nhighly dependent on the dataset, the identification of bench-\nmarking datasets turns out to be fundamental. Furthermore,\nthe use of readily available datasets save research time and\nallows greater focus on the development of the classification\nalgorithm [18].\n\nAs there are few benchmarking datasets in keystroke\ndynamics, all the 200 references were considered to answer\nthis item of the research question. In these references, we\nidentified five datasets (items 1–5) and another one (item 6)\nwas found in [44].\n\n1. GREYC [18]: 133 users typed the text “greyc labora tory”\nin two different keyboards, in which 100 of the users\nprovided samples in at least five sessions. Samples were\ncolected in a period of two months. Link: http://www.\necole.ensicaen.fr/~rosenber/keystroke.html.\n\n2. Web-GREYC [20]: 118 users typed imposed and free\nlogin/passwords during one year. The authors claim\nthat this dataset has the biggest number of differ-\nent passwords in a public dataset. Link: http://www.\nepaymentbiometrics.ensicaen.fr/index.php/app/resources/\n84.\n\n3. BioChaves [37]: 47 users formed four datasets: A (10\nusers), B (8 users), C (14 users) and D (15 users). In\ndatasets A and B, users typed four fixed expressions\n(“chocolate”, “zebra”, “banana” and “taxi”), while in\ndatasets C and D users typed the expression “computador\ncalcula’. Link: http://www.biochaves.com/en/download.\nhtm.\n\n4. CMU [31]: 51 users typed the text “.tie5Ronal” in eight\nsessions. Link: http://www.cs.cmu.edu/keystroke/.\n\n5. CMU-2 [32]: 20 users provided keystroke data for free\ntext and transcribed text. Link: http://www.cs.cmu.edu/\nkeystroke/laser-2012/\n\n6. Pressure sensitive [2]: 104 users typed three differ-\nent texts: “pr7q1z”, “jeffrey allen” and “drizzle”. Link:\nhttp://jdadesign.net/2010/04/pressure-sensitive-keystroke-\ndynamics-dataset/\n\n123\n\nhttp://www.ecole.ensicaen.fr/~rosenber/keystroke.html\nhttp://www.ecole.ensicaen.fr/~rosenber/keystroke.html\nhttp://www.epaymentbiometrics.ensicaen.fr/index.php/app/resources/84\nhttp://www.epaymentbiometrics.ensicaen.fr/index.php/app/resources/84\nhttp://www.epaymentbiometrics.ensicaen.fr/index.php/app/resources/84\nhttp://www.biochaves.com/en/download.htm\nhttp://www.biochaves.com/en/download.htm\nhttp://www.cs.cmu.edu/keystroke/\nhttp://www.cs.cmu.edu/keystroke/laser-2012/\nhttp://www.cs.cmu.edu/keystroke/laser-2012/\nhttp://jdadesign.net/2010/04/pressure-sensitive-keystroke-dynamics-dataset/\nhttp://jdadesign.net/2010/04/pressure-sensitive-keystroke-dynamics-dataset/\n\n\nJ Braz Comput Soc (2013) 19:573–587 585\n\nAll datasets presented here contain basic data for the fea-\nture extraction (instants in which each key is pressed and\nreleased), with the exception of the dataset 2, which does not\nprovide the instant in which each key is released. Addition-\nally, the last dataset (item 6) also stored the pressure over\neach key.\n\n6 Conclusion\n\nIntrusion detection systems based on the user behavior are a\npromising alternative to curb identity theft. Among the fea-\ntures to be analysed in order to define the user behavior, this\nwork considered a biometric technology known as keystroke\ndynamics.\n\nThe quasi-systematic review we conducted here may be\nused to guide future researches in this area. A systematic\nreview involves a formal definition of the review protocol\nbefore starting the review. Consequently, the results attained\nby the review may be reproduced by other researches as way\nof validation.\n\nHere, the main goal was to identify the state of the art\nin keystroke dynamics. In order to perform this task, this\nreview identified advantages and disadvantages of the use of\nkeystroke dynamics, features extracted from keystroke data,\nclassification algorithms, ways of evaluating the performance\nand datasets for benchmarking.\n\nA possible trend in keystroke dynamics is its use in touch\nscreen devices due to their increasing availability. These\ndevices may provide additional features to increase accu-\nracy. Although we cite a fair amount of datasets, some of\nthem have few samples per user (around 10). Consequently,\nmore public datasets on key-stroke dynamics are needed.\nThis would allow studies on specific aspects of keystroke\ndynamics, such as influence of age, typing skills, keyboard,\netc on the authentication performance. Additionally, the use\nof more datasets would increase the confidence of classifier\nperformance comparisons drawn in the literature.\n\nIn addition to summarizing key information in the area\nof keystroke dynamics, this paper also detailed the process\ninvolved in the application of the systematic review. This may\nlead to an increased dissemination of this review method in\nComputing, particularly in the area of Artificial Intelligence.\n\nAcknowledgments The authors would like to thank Universidade\nFederal do ABC (UFABC), Coordenação de Aperfeiçoamento de Pes-\nsoal de Nível Superior (CAPES), Conselho Nacional de Desenvolvi-\nmento Científico e Tecnológico (CNPq) and Fundação de Amparo à\nPesquisa do Estado de São Paulo (FAPESP) for financial support.\n\nAppendix: search expressions\n\nThe search expressions used in each of the databases are\nshown here.\n\nACM Digital Library\n\nIn the case of ACM Digital Library, the expression had to be\nsplit, as the complete version exceeded the size limit.\n\n((Title:(”behavioural intrusion detection” OR ”behav-\nioral intrusion detection” OR ”behavioral IDS” OR ”behav-\nioural IDS” OR ”biometric intrusion detection” OR ”user\nprofiling” OR ”keystroke dynamics” OR ”typing dynamics”\nOR ”keystroke biometrics” OR ”keystroke biometric” OR\n”continuous authentication” OR ”keystroke authentication”\nOR ”behavioural biometrics” OR ”behavioral biometrics”\nOR ”keystroke pattern” OR ”keystroke patterns” OR ”typ-\ning pattern” OR ”typing patterns”) AND NOT Title:(”web\nsearch” OR ”personalized information” OR ”personal-\nized content” OR ”content delivery” OR ”recommendation\nsystem” OR ”recommendations system” OR ”information\nretrieval” OR ”personalizing” OR ”personalization” OR\n”recommender”)) OR (Abstract:( ”behavioural intrusion\ndetection” OR ”behavioral intrusion detection” OR ”behav-\nioral IDS” OR ”behavioural IDS” OR ”biometric intrusion\ndetection” OR ”user profiling” OR ”keystroke dynamics”\nOR ”typing dynamics” OR ”keystroke biometrics” OR ”key-\nstroke biometric” OR ”continuous authentication” OR ”key-\nstroke authentication” OR ”behavioural biometrics” OR\n”behavioral biometrics” OR ”keystroke pattern” OR ”key-\nstroke patterns” OR ”typing pattern” OR ”typing patterns”)\nAND NOT Abstract:(”web search” OR ”personalized infor-\nmation” OR ”personalized content” OR ”content delivery”\nOR ”recommendation system” OR ”recommendations sys-\ntem” OR ”information retrieval” OR ”personalizing” OR\n”personalization” OR ”recommender”)))\n\n((Title:(”typing biometric” OR ”typing biometrics” OR\n”keypress biometric” OR ”keypress biometrics”) AND NOT\nTitle:(”web search” OR ”personalized information” OR\n”personalized content” OR ”content delivery” OR ”rec-\nommendation system” OR ”recommendations system” OR\n”information retrieval” OR ”personalizing” OR ”personal-\nization” OR ”recommender”)) OR (Abstract:(”typing bio-\nmetric” OR ”typing biometrics” OR ”keypress biometric”\nOR ”keypress biometrics” OR ”keystroke analysis”) AND\nNOT Abstract:(”web search” OR ”personalized informa-\ntion” OR ”personalized content” OR ”content delivery” OR\n”recommendation system” OR ”recommendations system”\nOR ”information retrieval” OR ”personalizing” OR ”per-\nsonalization” OR ”recommender”)))\n\nIEEE Xplore\n\n((”behavioural int rusion detection” OR ”behavioral intru-\nsion detection” OR ”behavioral IDS” OR ”behavioural\nIDS” OR ”biometric intrusion detection” OR ”user pro-\nfiling” OR ”keystroke dynamics” OR ”typing dynamics”\nOR ”keystroke biometrics” OR ”keystroke biometric” OR\n”continuous authentication” OR ”keystroke authentication”\n\n123\n\n\n\n586 J Braz Comput Soc (2013) 19:573–587\n\nOR ”behavioural biometrics” OR ”behavioral biometrics”\nOR ”keystroke pattern” OR ”keystroke patterns” OR ”typ-\ning pattern” OR ”typing patterns” OR ”typing biomet-\nric” OR ”typing biometrics” OR ”keypress biometric” OR\n”keypress biometrics” OR ”keystroke analysis”) AND NOT\n(”web search” OR ”personalized information” OR ”person-\nalized content” OR ”content delivery” OR ”recommenda-\ntion system” OR ”recommendations system” OR ”informa-\ntion retrieval” OR ”personalizing” OR ”personalization”\nOR ”recommender”))\n\nScience Direct\n\nTITLE-ABSTR-KEY((”behavioural intrusion detection” OR\n”behavioral intrusion detection” OR ”behavioral IDS” OR\n”behavioural IDS” OR ”biometric intrusion detection”\nOR ”user profiling” OR ”keystroke dynamics” OR ”typ-\ning dynamics” OR ”keystroke biometrics” OR ”keystroke\nbiometric” OR ”continuous authentication” OR ”keystroke\nauthentication” OR ”behavioural biometrics” OR ”behav-\nioral biometrics” OR ”keystroke pattern” OR ”keystroke\npatterns” OR ”typing pattern” OR ”typing patterns” OR\n”typing biometric” OR ”typing biometrics” OR ”keypress\nbiometric” OR ”keypress biometrics” OR ”keystroke analy-\nsis”) AND NOT (”web search” OR ”personalized informa-\ntion” OR ”personalized content” OR ”content delivery” OR\n”recommendation system” OR ”recommendations system”\nOR ”information retrieval” OR ”personalizing” OR ”per-\nsonalization” OR ”recommender”))\n\nWeb of Science\n\nTS=((”behavioural intrusion detection” OR ”behavioral\nintrusion detection” OR ”behavioral IDS” OR ”behav-\nioural IDS” OR ”biometric intrusion detection” OR ”user\nprofiling” OR ”keystroke dynamics” OR ”typing dynam-\nics” OR ”keystroke biometrics” OR ”keystroke biometric”\nOR ”continuous authentication” OR ”keystroke authentica-\ntion” OR ”behavioural biometrics” OR ”behavioral bio-\nmetrics” OR ”keystroke pattern” OR ”keystroke patterns”\nOR ”typing pattern” OR ”typing patterns” OR ”typing bio-\nmetric” OR ”typing biometrics” OR ”keypress biometric”\nOR ”keypress biometrics” OR ”keystroke analysis”) NOT\n(”web search” OR ”personalized information” OR ”person-\nalized content” OR ”content delivery” OR ”recommenda-\ntion system” OR ”recommendations system” OR ”informa-\ntion retrieval” OR ”personalizing” OR ”personalization”\nOR ”recommender”))\n\nScopus\n\nTITLE-ABS-KEY((”behavioural intrusion detection” OR\n”behavioral intrusion detection” OR ”behavioral IDS” OR\n”behavioural IDS” OR ”biometric intrusion detection”\nOR ”user profiling” OR ”keystroke dynamics” OR ”typ-\ning dynamics” OR ”keystroke biometrics” OR ”keystroke\nbiometric” OR ”continuous authentication” OR ”keystroke\n\nauthentication” OR ”behavioural biometrics” OR ”behav-\nioral biometrics” OR ”keystroke pattern” OR ”keystroke\npatterns” OR ”typing pattern” OR ”typing patterns” OR\n”typing biometric” OR ”typing biometrics” OR ”keypress\nbiometric” OR ”keypress biometrics” OR ”keystroke analy-\nsis”) AND NOT (”web search” OR ”personalized informa-\ntion” OR ”personalized content” OR ”content delivery” OR\n”recommendation system” OR ”recommendations system”\nOR ”information retrieval” OR ”personalizing” OR ”per-\nsonalization” OR ”recommender”))\n\nReferences\n\n1. Afzal W, Torkar R (2011) On the application of genetic program-\nming for software engineering predictive modeling: a systematic\nreview. Expert Syst Appl 38(9):11984–11997\n\n2. Allen JD (2010) An analysis of pressure-based keystroke dynamics\nalgorithms. Master’s thesis, Southern Methodist University, Dallas\n\n3. Bartlow N, Cukic B (2006) Evaluating the reliability of credential\nhardening through keystroke dynamics. In: Software Reliability\nEngineering, ISSRE ’06. 17th International Symposium on IEEE,\npp 117–126\n\n4. Bleha S, Slivinsky C, Hussien B (1990) Computer-access security\nsystems using keystroke dynamics. IEEE Trans Pattern Anal Mach\nIntell 12(12):1217–1222\n\n5. Boechat G, Ferreira J, Carvalho Filho E (2007) Authentication\npersonal. In: International conference on intelligent and advanced\nsystems, 2007. ICIAS 2007, pp 254–256\n\n6. Bose R (2006) Intelligent technologies for managing fraud and\nidentity theft. In: Information technology: new generations, 2006.\nITNG 2006. Third International Conference on IEEE, pp 446–451\n\n7. Brereton P, Kitchenham BA, Budgen D, Turner M, Khalil M (2007)\nLessons from applying the systematic literature review process\nwithin the software engineering domain. J Syst Softw 80(4):571–\n583\n\n8. Chang TY, Tsai CJ, Lin JH (2012) A graphical-based password\nkeystroke dynamic authentication system for touch screen hand-\nheld mobile devices. J Syst Softw 85(5):1157–1165\n\n9. Chang W (2006) Reliable keystroke biometric system based on a\nsmall number of keystroke samples, 3995th edn. Springer, Berlin /\nHeidelberg\n\n10. Conklin A, Dietrich G, Walz D (2004) Password-based authenti-\ncation: a system perspective. In: Proceedings of the 37th annual\nHawaii international conference on system sciences, 2004, IEEE,\npp 1–10\n\n11. Crawford H (2010) Keystroke dynamics: Characteristics and\nopportunities. In: Eighth annual international conference on\nprivacy security and trust (PST), pp 205–212\n\n12. Desouza KC, Vanapalli GK (2005) Securing knowledge assets\nand processes: lessons from the defense and intelligence sectors.\nHawaii Int Conf Syst Sci 1:1–11\n\n13. Elftmann P (2006) Diploma thesis: secure alternatives to password-\nbased authentication mechanisms. Master’s thesis, Laboratory for\nDependable Distributed Systems, RWTH Aachen University\n\n14. Filho JRM, Freire EO (2006) On the equalization of keystroke\ntiming histograms. Pattern Recogn Lett 27(13):1440–1446\n\n15. Gaines R, Lisowski W, Press S, Shapiro N (1980) Authentication\nby keystroke timing: some preliminary results, technical report.\nRand Corporation\n\n16. Galassi U (2008) Learning behavior profiles from noisy sequences.\nIn: Intrusion detection systems, 38th edn. Springer, US\n\n123\n\n\n\nJ Braz Comput Soc (2013) 19:573–587 587\n\n17. Giot R, El-Abed M, Hemery B, Rosenberger C (2011) Uncon-\nstrained keystroke dynamics authentication with shared secret.\nComput Secur 30(6–7):27–445\n\n18. Giot R, El-Abed M, Rosenberger C (2009) Greyc keystroke: a\nbenchmark for keystroke dynamics biometric systems. In: IEEE\ninternational conference on biometrics: theory, applications and\nsystems (BTAS). IEEE Computer Society, Washington, District of\nColumbia, USA(2009)\n\n19. Giot R, El-Abed, M, Rosenberger C (2009) Keystroke dynamics\nwith low constraints SVM based passphrase enrollment. In: IEEE\n3rd International Conference on biometrics: theory, applications,\nand systems, 2009. BTAS 2009, pp 1–6\n\n20. Giot R, El-Abed M, Rosenberger C (2012) Web-based benchmark\nfor keystroke dynamics biometric systems: a statistical analysis.\nIn: Intelligent information hiding and multimedia signal processing\n(IIH-MSP), pp 11–15\n\n21. Goldring T (2003) User profiling for intrusion detection in windows\nnt. In: Proceedings of the 35th Symposium on the Interface\n\n22. Gunetti D, Picardi C (2005) Keystroke analysis of free text. ACM\nTrans Inf Syst Secur 8:312–347\n\n23. Hocquet S, Ramel J, Cardot H (2006) Estimation of user specific\nparameters in one-class problems. In: 18th International Confer-\nence on Pattern Recognition, 2006. ICPR 2006. vol 4, pp 449–452\n\n24. Hosseinzadeh D, Krishnan S (2008) Gaussian mixture modeling\nof keystroke patterns for biometric applications. IEEE Trans Syst\nMan Cybernetics Part C: Appl Rev 38(6):816–826\n\n25. Jain A, Pankanti S (2006) A touch of money [biometric authenti-\ncation systems]. Spectrum IEEE 43(7):22–27\n\n26. Jain AK, Flynn P, Ross AA (2007) Handbook of biometrics.\nSpringer, New York\n\n27. Kang P, Hwang Ss, Cho S (2007) Continual retraining of key-\nstroke dynamics based authenticator, 4642nd edn. Springer, Berlin /\nHeidelberg\n\n28. Karnan M, Akila M, Krishnaraj N (2011) Biometric personal\nauthentication using keystroke dynamics: a review. Appl Soft Com-\nput 11:1565–1573\n\n29. Keeney M, Kowalski E, Cappelli D, Moore A, Shimeall T, Rogers\nS (2005) Insider threat study: computer system sabotage in critical\ninfrastructure sectors. Carnegie Mellon University, Pittsburgh\n\n30. Killourhy K, Maxion R (2008) The effect of clock resolution on\nkeystroke dynamics. In: Lippmann R, Kirda E, Trachtenberg A\n(eds) Recent advances in intrusion detection, lecture notes in com-\nputer science, vol 5230. Springer, Berlin/Heidelber, pp 331–350\n\n31. Killourhy K, Maxion R (2010) Why did my detector do that?!\npredicting keystroke-dynamics error rates. In: Jha S, Sommer R,\nKreibich C (eds) Recent advances in intrusion detection, lecture\nnotes in computer science, vol 6307. Springer, Berlin/Heidelberg,\npp 256–276\n\n32. Killourhy KS, Maxion RA (2012) Free vs. transcribed text for\nkeystroke-dynamics evaluations. In: Proceedings of the 2012 work-\nshop on learning from authoritative security experiment results,\nLASER ’12, pp 1–8. ACM, New York\n\n33. Kitchenham B, Charters S (2007) Guidelines for performing sys-\ntematic literature reviews in software engineering, technical report\n2007–001. Keele University and Durham University Joint Report\n\n34. joo Lee H, Cho S (2007) Retraining a keystroke dynamics-based\nauthenticator with impostor patterns. Comput Security 26(4):300–\n310\n\n35. Magdaleno AM, Werner CML, de Araujo RM (2012) Reconciling\nsoftware development models: a quasi-systematic review. J Syst\nSoftw 85(2):351–369\n\n36. Monrose F, Rubin AD (2000) Keystroke dynamics as a biometric\nfor authentication. Future Gener Comp Syst 16(4):351–359\n\n37. Montalvao J, Almeida C, Freire E (2006) Equalization of key-\nstroke timing histograms for improved identification performance.\nIn: Telecommunications symposium, 2006 International, pp 560–\n565\n\n38. Moskovitch R, Feher C, Messerman A, Kirschnick N, Mustafic T,\nCamtepe A, Lohlein B, Heister U, Moller S, Rokach L, Elovici\nY (2009) Identity theft, computers and behavioral biometrics. In:\nIEEE International conference on intelligence and security infor-\nmatics, 2009. ISI ’09. pp 155–160\n\n39. Pannell G, Ashman H (2010) User modelling for exclusion and\nanomaly detection: a behavioural intrusion detection system. In: De\nBra P, Kobsa A, Chin D (eds) User modeling, adaptation, and per-\nsonalization, lecture notes in computer science, vol 6075. Springer,\nBerlin/Heidelberg, pp 207–218\n\n40. Peacock A, Ke X, Wilkerson M (2004) Typing patterns: a key to\nuser identification. Secur Privacy IEEE 2(5):40–47\n\n41. Pisani PH (2012) Algoritmos imunológicos aplicados na detecção\nde intrusões com dinâmica da digitação (in Portuguese). Master’s\nthesis, Universidade Federal do ABC\n\n42. Pisani PH, Lorena AC (2011) Detecção de intrusões com din", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczEzMTczLTAxMy0wMTE3LTcucGRm0", "metadata_author": null, "metadata_title": null, "metadata_creation_date": "2013-10-11T02:33:25Z", "keyphrases": [] }, { "@search.score": 1, "content": "\nQER: a new feature selection method \nfor sentiment analysis\nTuba Parlar1* , Selma Ayşe Özel2 and Fei Song3\n\nIntroduction\n“What other people think” has always been an important piece of information for most \nof us during the decision making process [1]. The Internet and social media provide a \nmajor source of information about people’s opinions. Due to the rapidly-growing num-\nber of online documents, it becomes both time-consuming and hard to obtain and ana-\nlyze the desired opinionated information. Turkey is among the top 20 countries with the \nhighest numbers of Internet users according to the Internet World Stats.1 The exploding \ngrowth in the Internet users is one of the main reasons that sentiment analysis for differ-\nent languages and domains becomes an actively-studied area for many researchers \n[2–6].\n\nSentiment analysis (SA) is a natural language processing task that classifies the senti-\nments expressed in review documents as “positive” or “negative”. In general, SA is con-\nsidered as a two-class classification problem. However, some researchers use “neutral” as \n\n1 http://www.internetworldstats.com/.\n\nAbstract \n\nSentiment analysis is about the classification of sentiments expressed in review docu-\nments. In order to improve the classification accuracy, feature selection methods are \noften used to rank features so that non-informative and noisy features with low ranks \ncan be removed. In this study, we propose a new feature selection method, called \nquery expansion ranking, which is based on query expansion term weighting meth-\nods from the field of information retrieval. We compare our proposed method with \nother widely used feature selection methods, including Chi square, information gain, \ndocument frequency difference, and optimal orthogonal centroid, using four classi-\nfiers: naïve Bayes multinomial, support vector machines, maximum entropy model-\nling, and decision trees. We test them on movie and multiple kinds of product reviews \nfor both Turkish and English languages so that we can show their performances for \ndifferent domains, languages, and classifiers. We observe that our proposed method \nachieves consistently better performance than other feature selection methods, and \nquery expansion ranking, Chi square, information gain, document frequency difference \nmethods tend to produce better results for both the English and Turkish reviews when \ntested using naïve Bayes multinomial classifier.\n\nKeywords: Sentiment analysis, Feature selection, Machine learning, Text classification\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nParlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \nhttps://doi.org/10.1186/s13673-018-0135-8\n\n*Correspondence: \ntparlar@mku.edu.tr \n1 Department \nof Mathematics, Mustafa \nKemal University, Antakya, \nHatay, Turkey\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0002-8004-6150\nhttp://www.internetworldstats.com/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-018-0135-8&domain=pdf\n\n\nPage 2 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nthe third class label. There are a number of studies about sentiment analysis that use dif-\nferent approaches for data preprocessing, feature selection, and sentiment classification \n[1, 3, 4, 6–10]. The statistical methods such as Chi square (CHI2) and information gain \n(IG) are used to eliminate unnecessary or irrelevant features so that the classification \nperformance can be improved [11]. Supervised learning methods including naïve Bayes \n(NB), support vector machines (SVM), decision trees (DT), and maximum entropy mod-\nelling (MEM) are used to classify the sentiments of the reviews.\n\nAlthough SA can be considered as a text classification task, it has some differences \nfrom the traditional topic-based text classification. For example, instead of saying: “This \ncamera is great. It takes great pictures. The LCD screen is great. I love this camera” in a \nreview document, people are more likely to write: “This camera is great. It takes breath-\ntaking pictures. The LCD screen is bright and clear. I love this camera.” [8]. As can be \nseen, sentiment-expressing words like “great” are not so frequent within a particular \nreview, but can be more frequent across different reviews, and a good feature selection \nmethod for SA should take this observation into account.\n\nIn this paper, we propose a new feature selection method, called query expansion rank-\ning (QER) which is especially developed for reducing dimensionality of feature space of \nSA problems. The aim of this study is to show that our proposed method is effective for \nSA from review texts written in different languages (e.g., Turkish, English) and domains \n(e.g., movie reviews, book reviews, kitchen appliances reviews, etc.). QER is based on \nquery expansion term weighting methods used to improve the search performance of \ninformation retrieval systems [12, 13] and to evaluate its effectiveness as a feature selec-\ntor in SA, we compare it with other common feature selection methods, including CHI2, \nIG, document frequency difference (DFD), and optimal orthogonal centroid (OCFS), \nalong with four text classifiers: naïve Bayes multinomial (NBM), SVM, DT, and MEM, \nover ten different review documents datasets. Our goal is to examine whether these fea-\nture selection methods can reduce the feature sizes and improve the classification accu-\nracy of sentiment analysis with respect to different document domains, languages, and \nclassifiers.\n\nThe rest of the paper is organized as follows. “Related work” reviews the related work \non sentiment analysis. “Methods” presents the methods that we used for our study, \nincluding the new feature selection method we proposed. “Experiments and results” \ndescribes the experimental settings, datasets, performance measures, and testing results. \nFinally, “Conclusion” concludes the paper.\n\nRelated work\nSA is an important topic in Natural Language Processing and Artificial Intelligence. \nAlso known as opinion mining, SA mines people’s opinions, sentiments, evalua-\ntions, and emotions about entities such as products, services, organizations, individu-\nals, issues, and events, as well as their related attributes. This kind of analysis has many \nuseful applications. For example, it determines a product’s popularity according to \nthe user’s reviews. If the overall sentiments are negative, further analysis may be per-\nformed to identify which features contribute to the negative ratings so companies can \nreshape their businesses. Numerous studies have been done for sentiment analysis in \ndifferent domains, languages, and approaches [3–5, 8–10, 14–17]. Among these studies, \n\n\n\nPage 3 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nthe machine learning approaches are more popular since the models can be automati-\ncally trained and improved with the training datasets. Pang et al. [4] apply supervised \nmachine learning methods such as NB and SVM to sentiment classification. NB, SVM, \nMEM, and DT are some of the commonly used machine learning approaches [4, 7–9, \n14]. Feature selection methods are used to rank features so that non-informative features \ncan be removed to improve the classification performance [18]. Some researchers have \ninvestigated the effects of feature selection for sentiment analysis [3, 8–10, 19–25]. For \nexample, Yang and Yu [3] examine IG for feature selection and evaluate its performance \nusing NB, SVM, and C4.5 (popular implementation for DT) classifiers. Nicholls et al. [8] \ncompare their proposed DFD feature selection method against other feature selection \nmethods, including CHI2, OCFS [26], and count difference using the MEM classifier. \nAgarwal et al. [9] investigate minimum redundancy maximum relevancy (mRMR) and \nIG methods for sentiment classification using NBM and SVM classifiers. The results \nshow that mRMR performs better than IG for feature selection, and NBM performs bet-\nter than SVM in accuracy and execution time. Abbasi et al. [22] examine a new feature \nselection method called entropy weighted genetic algorithm (EWGA) and compare the \nperformance of this method using information gain feature selection method. EWGA \nachieves a relatively high accuracy of 91.7% using SVM classifier. Xia et al. [24] design \ntwo types of feature sets: POS based and word relation based. Their word relation based \nmethod improves an accuracy of 87.7 and 85.15% on movie and product datasets. Bai \n[25] proposes a Tabu heuristic search-enhanced Markov blanket model that provides a \nvocabulary to extract sentiment features. Their method achieves an accuracy of 92.7% \nfor the movie review dataset. Mladenovic et al. [16] propose a feature selection method \nthat is based on mapping of a large number of related features to a few features. Their \nproposed method improves the classification performance using unigram features \nwith 95% average accuracy. Zheng et al. [27] perform comparative experiments to test \ntheir proposed improved document frequency feature selection method. Their method \nachieves significant improvement in sentiment analysis of Chinese online reviews with \nan accuracy of 97.3%.\n\nMost of the SA studies listed above focus on the English language. Only few studies \nhave been done on SA for the Turkish language [6, 10, 19, 28–31]. The Turkish language \nbelongs to the Altaic branch of the Ural-Altaic family of languages and is mainly used in \nthe Republic of Turkey. Turkish is an agglutinative language similar to Finnish and Hun-\ngarian, where a single word can be translated into a relatively longer sentence in English \n[32]. For instance, word “karşılaştırmalısın” in Turkish can be expressed as “you must \nmake (something) compare” in English. As Turkish and English have different charac-\nteristics, methods developed for SA in English need to be tested for Turkish. Among \nthe few researchers who investigate the effects of feature selection on the SA of Turkish \nreviews, Boynukalın [29] applies Weighted Log Likelihood Ratio (WLLR) to reduce fea-\nture space with NB, Complementary NB, and SVM classifiers for the emotional analysis \nusing the combinations of n-grams where sequences of n words are considered together. \nIt is shown that WLLR helps to improve the accuracy with reduced feature sizes. Akba \net al. [19] implement and compare the performance of reduced feature sizes using two \nfeature selection methods: CHI2 and IG with NB and SVM classifiers. They show that \nfeature selection methods improve the classification accuracy.\n\n\n\nPage 4 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nOur aim is to propose a new feature selection method for the SA of Turkish and Eng-\nlish reviews. We presented an initial version of this method in [10] where we employ \nonly product review dataset in Turkish and compare our method with CHI2 and DFD \nby using only one classifier. We now extend it to more datasets for Turkish, and also \ninvestigate the performance of our method in English datasets to show that our method \nis language independent. We further include more feature selection methods especially \ndeveloped for SA and compare the performance of our proposed method using NBM, \nSVM, MEM, and DT classifiers along with statistical analysis to prove that our method is \nclassifier independent.\n\nMethods\nMachine learning algorithms\n\nFor sentiment classification, we use the Weka [33] data mining tool, which contains the \nfour classifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR \nfor MEM. We choose NBM, SVM, LR, and J48 classification methods due to the follow-\ning reasons: (i) many researchers use NBM for text classification because it is computa-\ntionally efficient [9, 10, 14] and performs well for large vocabulary sizes [34]; (ii) SVM \ntends to perform well for traditional text classification tasks [3, 4, 7, 14, 35]; (iii) LR is \nknown to be equivalent to MEM which is another method used in SA studies [8]; (iv) J48 \nis a well-known decision tree classifier for many classification problems and is used for \nSA [3, 30].\n\nFeature selection\n\nFeature Selection methods have been shown to be useful for text classification in general \nand sentiment analysis in specific [11, 18]. Such methods rank features according to cer-\ntain measures so that non-informative features can be removed, and at the same time, \nthe most valuable features can be kept in order to improve the classification accuracy \nand efficiency. In this study, we consider several feature selection methods, including \ninformation gain, Chi square, document frequency difference, optimal orthogonal cen-\ntroid, and our new query expansion ranking (QER) so that we can compare their effec-\ntiveness for the sentiment analysis.\n\nFeature sizes are selected in the range from 500 to 3000 with 500 increments, com-\npared with the total feature sizes ranging from 8000 to 18,000 for the Turkish review \ndatasets and from 8000 to 38,000 for English review datasets. In our previous study [10], \nwe observed that feature sizes up to 3000 tend to give good classification performance \nimprovement; therefore we choose these feature sizes in our experiments.\n\nInformation gain\n\nInformation gain is one of the most common feature selection methods for sentiment \nanalysis [3, 9, 19, 35], which measures the content of information obtained after knowing \nthe value of a feature in a document. The higher the information gain, the more power \nwe have to discriminate between different classes.\n\nThe content of information can be calculated by the entropy that captures the uncer-\ntainty of a probability distribution for the given classes. Given m number of classes: \nC = {c1,c2,…,cm} the entropy can be given as follows:\n\n\n\nPage 5 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nwhere P(ci) is the probability of how many documents in class ci. If an attribute A has n \ndistinct values: A = {a1,a2,…,an}, then the entropy after the attribute A is observed can be \ndefined as follows:\n\nwhere P(aj) is the probability of how many documents contain the attribute value aj, and \nP(ci|aj) is the probability of how many documents in class ci that contain the attribute \nvalue aj. Based on the definitions above, the information gain for an attribute is simply \nthe difference between the entropy values before and after the attribute is observed:\n\nFor sentiment analysis, we normally classify the reviews into positive and negative cat-\negories, and for each keyword, it either occurs or does not occur in a given document; so \nthe above formulas can be further simplified. Nevertheless, we can cut down the number \nof features in the same way by choosing the keywords that have high information gain \nscores.\n\nChi square (CHI2)\n\nChi square measures the dependence between a feature and a class. A higher score \nimplies that the related class is more dependent on the given feature. Thus, a feature with \na low score is less informative and should be removed [3, 8, 10, 19]. Using the 2-by-2 \ncontingency table for feature f and class c, where A is the number of documents in class c \nthat contains feature f, B is the number of documents in the other class that contains f, C \nis the number of documents in c that does not contain f, D is the number of documents \nin the other class that does not contain f, and N is the total number of documents, then \nthe Chi square score can be defined in the following:\n\nThe Chi square statistics can also be computed between a feature and a class in the \ndataset, which are then combined across all classes to get the scores for each feature as \nfollows:\n\nOne problem with the CHI2 method is that it may produce high scores for rare features \nas long as they are mostly used for one specific class. This is a bit counter-intuitive, since \nrare features are not frequently used in text and thus do not have a big impact for text \n\n(1)H(C) = −\n\nm\n∑\n\ni=1\n\nP(ci) log2 P(ci)\n\n(2)H(C|A) =\n\nn\n∑\n\nj=1\n\n(\n\n−P(aj)\n\nm\n∑\n\ni=1\n\nP(ci|aj) log2P(ci|aj)\n\n)\n\n(3)IG(A) = H(C)−H(C|A)\n\n(4)χ2\n(\n\nf , c\n)\n\n=\nN (AD − CB)2\n\n(A+ C)(B+ D)(A+ B)(C + D)\n\n(5)χ2(f ) =\n\nm\n∑\n\ni=1\n\nP(ci)χ\n2(f , ci)\n\n\n\nPage 6 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nclassification. For SA, however, this is not a big issue since many sentiment-expressing \nfeatures are not frequently used within an individual review.\n\nDocument frequency difference\n\nInspired by the observation that sentiment-expressing words tends to be less frequent \nwithin a review, but more frequent across different reviews, Nicholls and Song [8] pro-\npose the DFD method that tries to differentiate the features for positive and negative \nclasses, respectively, across a document collection. More specifically, DFD is calculated \nas follows:\n\nwhere DFf\n+ is the number of documents in the positive class that contain feature f, DFf\n\n− \nis the number of documents in the negative class that contain f, and N is the total num-\nber of documents in the dataset. Note that all scores are normalized between 0 and 1; \nso they should be proportional for us to rank the features in a document collection. For \nexample, a non-sentiment word may have similar document frequencies in both posi-\ntive and negative classes, and will get a low score, but a sentiment word for the positive \nclass may have a bigger difference, resulting in a higher score. One limitation of the DFD \nmethod is that it requires an equal or nearly equal number of documents in both classes, \nwhich is more or less true for the datasets used in our experiments.\n\nOptimal orthogonal centroid (OCFS)\n\nOCFS method is an optimized form of the orthogonal centroid algorithm [26]. Docu-\nments are represented as high dimensional vectors where the weights of each dimension \ncorrespond to the importance of the related features, and a centroid is simply the aver-\nage vector for a set of document vectors. OCFS aims at finding a subset of features that \ncan make the sum of distances between all the class means maximized in the selected \nsubspace. The score of a feature f by OCFS is defined in the following [8]:\n\nwhere Nc is the number of documents in class c, N is the number of documents in the \ndataset, mc is the centroid for class c, m is the centroid for the dataset D, and mf, mc\n\nf are \nthe values of feature f in centroid m, mc respectively. The centroids of m and mc are cal-\nculated as follows:\n\nQuery expansion ranking\n\nQuery expansion ranking method is our proposed feature selection method inspired \nby the query expansion methods from the field of information retrieval (IR). Query \n\n(6)Scoref =\n|DF\n\nf\n+ − DF\n\nf\n−|\n\nN\n\n(7)Scoref =\n∑\n\nc\n\nNc\n\nN\n\n(\n\nm\nf\nc −mf\n\n)2\n\n(8)mc =\n\n∑\n\nxi∈c\nxi\n\nNc\n\n(9)m =\n\n∑\n\nxi∈D\nxi\n\nN\n\n\n\nPage 7 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nexpansion helps to find more relevant documents for a given query. It does so by adding \nnew terms to the query. The new terms are selected from documents that are relevant \nto the original query so that the expanded query can retrieve more relevant documents. \nMore specifically, terms from the relevant documents are extracted along with some \nscores, and those with the highest scores are included in the expanded query.\n\nWe propose a new feature selection method inspired by the query expansion technique \ndeveloped for probabilistic weighting model proposed by Harman [12]. Harman [12, 36] \nstudies how to assign scores to terms extracted from relevant documents for a given \nquery Q so that high scored terms are used to expand the original query and improve \nprecision of information retrieval strategy. In this method, first, query Q is sent to the \ninformation retrieval system, and then the system returns documents that are found as \nrelevant to the user. Then, user examines the returned documents and marks the ones \nthat are relevant with the query. After that, all the terms in the relevant documents are \nextracted and they are assigned scores by using a score formula as proposed by Har-\nman [12], and top scored k terms are chosen as the most valuable terms to expand the \nquery. Then, the expanded query Q’, which includes the terms in the original query plus \nthe k new terms that have the top-k scores, is sent to the information retrieval system to \nreturn more relevant documents to the original query Q. Equation 10 presents the score \nformula developed by Harman [12] to calculate ranking score of a term f extracted from \nthe set of relevant documents for a given query Q.\n\nwhere pf is the probability of term f in the set of relevant documents for query Q, and qf \nis the probability of term f in the set of non-relevant documents for query Q. These prob-\nability scores are computed according to Robertson and Sparck Jones [13].\n\nWe revise the above score computation method to develop an efficient feature selector \nfor SA. In our feature selection method, we propose a score formula given in Eq. 11 to \ncompute scores for features:\n\nwhere pf is the ratio of positive documents containing feature f and qf is the ratio of \nnegative documents containing feature f, which are computed according to Eqs. 12, 13, \nrespectively:\n\n(10)Scoref = log2\npf\n(\n\n1− qf\n)\n\n(\n\n1− pf\n)\n\nqf\n\n(11)Scoref =\npf + qf\n∣\n\n∣pf − qf\n∣\n\n∣\n\n(12)pf =\nDF\n\nf\n+ + 0.5\n\nN+ + 1.0\n\n(13)qf =\nDF\n\nf\n− + 0.5\n\nN− + 0.5\n\n\n\nPage 8 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nwhere DFf\n+ and DFf\n\n− are the raw counts of documents that contain f in the positive and \nnegative classes, respectively and N+ and N− are the numbers of documents in the \npositive and negative classes, respectively. In the probability calculations, we add small \nconstants to the numerators and denominators in Eqs. 12, 13 following Robertson and \nSparck Jones [13] who add similar constants to avoid having zero probabilities. Such a \nmethod is known as data smoothing in statistical language processing.\n\nIn QER feature selection method, scores of features are computed before the features \nhaving the lowest scores are selected and used in the classification process. When a fea-\nture has low score, the difference between the probabilities for the positive and negative \nclasses is high; therefore the feature is more class specific and more valuable for clas-\nsification process. Among the feature selection methods we considered, we notice that \nIG and OCFS are good at distinguishing multiple classes, while CHI2, DFD, and QER \nare restricted to two classes, although all of them are suitable for sentiment analysis. IG \nis considered as a greedy approach since it favors those that can maximize the informa-\ntion gain for separating the related classes. Although CHI2 tries to identify the features \nthat are dependent to a class, it can also give high values to rare features that only affect \nfew documents in a given collection. OCFS has been shown to be effective for tradi-\ntional topic-based text classification, but it depends on the distance/similarity measures \nbetween the vectors of the related documents. Since sentiment-expressing features do \nnot happen frequently within a review, as illustrated by the example in the introduction, \nthey may not be favored by the OCFS method. QER is similar to DFD in that they both \nrely on the differences of the document frequencies of a given feature between the two \nclasses. However, QER is different from DFD in that it normalizes the document fre-\nquencies of a feature in both classes into probabilities and uses the ratio of the sum over \nthe difference for these two probabilities.\n\nExperiments and results\nDatasets\n\nWe use Turkish and English review datasets in our experiments. The Turkish movie \nreviews are collected from a publicly available website (http://www.beyazperde.com) \n[30]. The dataset has 1057 positive and 978 negative reviews. The Turkish product review \ndataset is collected from an e-commerce website (http://www.hepsiburada.com) from \ndifferent domains [28]. It consists of four subsets of reviews about books, DVDs, elec-\ntronics, and kitchen appliances, each of which has 700 positive and 700 negative reviews. \nTo compare our results with existing work for sentiment analysis, we use similar datasets \nfor English reviews. The English movie review dataset is introduced by Pang and Lee [7], \nand consists of 1000 positive and 1000 negative reviews. English product review dataset \nis introduced by Blitzer et al. [37] and also has four subsets: books, DVDs, electronics, \nand kitchen appliances, with 1000 positive and 1000 negative reviews for each subset. In \norder to keep the same dataset sizes with Turkish product reviews, we randomly select \n700 positive and 700 negative reviews from each subset of the English product reviews.\n\nPerformance evaluation\n\nThe performance of a classification system is typically evaluated by F measure, which \nis a composite score of precision and recall. Precision (P) is the number of correctly \n\nhttp://www.beyazperde.com\nhttp://www.hepsiburada.com\n\n\nPage 9 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nclassified items over the total number of classified items with respect to a class. Recall \n(R) is the number of correctly classified items over the total number of items that belong \nto a given class. Together, the F measure gives the harmonic mean of precision and \nrecall, and is calculated as follows [33]:\n\nSince we are doing multi-fold cross validations in our experiments, we use the micro-\naverage of F measure for the final classification results. This is done by adding the clas-\nsification results for all documents across all five folds before computing the final P, R, \nand the F.\n\nExperimental settings\n\nWe conduct the experiments on a MacBook Pro with 2.5 GHz Intel Core i7 processor \nand 16 GB 1600 MHz DDR3. We use Python with NLTK [38] library in our experiments. \nAfter tokenizing text into words along with case normalization, we keep some punctua-\ntion marks and stop words, as they may express sentiments (e.g., punctuation marks like \nexclamation and question marks, and stop words like “too” in “too expensive”). In addi-\ntion, we do not apply stemming as Turkish is an agglutinative language and the polarity \nof a word is often included in the suffixes. Therefore, we can have a large feature space \nand it becomes important to apply feature selection methods to reduce this space. For \nsentiment classification, we use the Weka [33] data mining tool, which contains the four \nclassifiers we use in our experiments, i.e., NBM, SMO for SVM, J48 for C4.5, and LR for \nMEM. Since our datasets are relatively small with at most a couple of thousands of docu-\nments, we apply the fivefold cross validation, which divides a dataset into five portions: \nfour of them are used for training and the remaining one for testing, and then these por-\ntions are rotated to get a total of five F measures. Table 1 the average F measures for all \nthe classifiers where the whole feature spaces are used for each dataset, except the LR \nclassifier since it requires too much memory to handle the whole feature spaces for these \ndatasets. As can be seen in Table  1, the total number of features without any reduc-\ntion ranges from 9000 to 18,000 for the Turkish review datasets, and 8,000–38,000 for \nthe English review datasets. These results form the baselines of our study and any new \nresults obtained with feature selection methods by applying five folds cross validation \ncan be compared for possible improvements.\n\n(14)F = 2×\nP × R\n\nP + R\n\nTable 1 Baseline results in F measure for the Turkish and English review datasets\n\nTurkish review datasets English review datasets\n\nFeatures NBM SVM J48 LR Features NBM SVM J48 LR\n\nMovie 18,578 0.8248 0.8161 0.6954 – 38,869 0.8129 0.8480 0.6769 –\n\nDVDs 11,343 0.7957 0.7320 0.6886 – 17,674 0.7836 0.7649 0.6789 –\n\nElectronics 10,911 0.8155 0.7707 0.7371 – 9010 0.7629 0.7856 0.6750 –\n\nBook 10,511 0.8317 0.7955 0.7019 – 18,306 0.7619 0.7485 0.6407 –\n\nKitchen 9447 0.7762 0.7407 0.6647 – 8076 0.8099 0.8136 0.7093 –\n\n\n\nPage 10 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nPerformance of feature selection methods for Turkish reviews\n\nWe tested five feature selection methods: QER, CHI2, IG, DFD, and OCFS on both \nTurkish and English review datasets. For each feature selection method, we tried six fea-\nture sizes at 500, 1000, 1500, 2000, 2500, and 3000, since this is the range typically con-\nsidered for text classification, and in terms of total features, we have 9000–18,000 for the \nTurkish review datasets, and 8000–38,000 for English review datasets from our baseline \nsystems. In our previous study [10], we also observed that feature sizes up to 3000 tend \nto give good classification performance. For all feature selection methods, we pick the \ntop-ranked features of a desirable size n based on the scores of the related formulas for \nthese methods. All of these settings are run against four classifiers: NBM, SVM, LR, and \nJ48, resulting in a total of 120 experiments for each review dataset. Table 2 summarizes \nthe best results for all pairs of feature selection methods and Turkish review datasets. \nFor each pair, we show the best micro-average F measure along with the correspond-\ning classifier and feature size. Also, the best results for each review dataset are given in \nbold-face.\n\nAs observed in Table 2, our new method QER is the best performer for each review \ndataset. CHI2 and IG have almost the same performance for the Turkish reviews and \nhave better results than DFD and OCFS for the movie, book, DVDs, and kitchen review \ndatasets. DFD with NBM classifier has better results than CHI2, IG, and OCFS for the \nelectronics review dataset. Also, CHI2, IG, and QER tend to work well with smaller fea-\nture sizes, while DFD and OCFS tend to favour bigger feature sizes. Note that DFD does \nreasonably well across all review datasets, which confirms our intuition that sentiment-\nexpressing words usually have low frequencies within a document, but relatively high \nfrequencies across different documents. Although OCFS is quite robust for traditional \ntopical text classification as reported in Cai and Song [39], it is not doing well for senti-\nment analysis, perhaps for the same intuition as we just explained for DFD. Once again, \nNBM remains to be the best for most of our experiments except that SVM does the best \nfor the kitchen reviews when analysed with the CHI2 and IG methods. When analysed \nby univariate ANOVA and post hoc tests for the book, DVDs, electronics, and kitchen \nreview datasets, we found that there are significant differences between three groups \n(Baseline and OCFS), (DFD, CHI2, and IG) and (QER) at 95% confidence level. Within \neach group, however, there are no significant differences. For the movie review dataset, \nthere are significant differences between two groups (Baseline and OCFS), and (DFD, \nCHI2, IG, and QER) at the 95% confidence level. Overall, feature selection methods are \nshown to be effective for sentiment analysis, improving significantly over the baseline \nresults.\n\nTo examine the effects of text classifiers, we show the best classification results for \npairs of feature selection methods and text classifiers on the electronic review dataset in \nTable 3. Note that NBM does the best for all review datasets; J48 the worst; and SVM and \nLR in between, although LR is consistently better than SVM except for the QER method. \nOne reason that the decision-tree-based solution J48 does not do well for text classifi-\ncation in general [40] and sentiment analysis in specific is that it is a greedy approach, \nalways trying to find the features that separate the given classes the most. As a result, the \nclassifier may use a much smaller set of features, even though there are many more rel-\nevant features are available. SVM typically does well for the traditional topic-based text \n\n\n\nPage 11 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nTa\nb\n\nle\n 2\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nTu\nrk\n\nis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\nov\n\nie\n30\n\n00\nN\n\nBM\n:0\n\n.9\n11\n\n2\n30\n\n00\nN\n\nBM\n:0\n\n.8\n86\n\n4\n30\n\n00\nN\n\nBM\n:0\n\n.8\n44\n\n7\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n15\n\n00\nN\n\nBM\n:0\n\n.8\n88\n\n3\n\nD\nVD\n\ns\n15\n\n00\nN\n\nBM\n:0\n\n.9\n13\n\n6\n30\n\n00\nN\n\nBM\n:0\n\n.8\n65\n\n0\n30\n\n00\nN\n\nBM\n:0\n\n.8\n12\n\n9\n50\n\n0\nN\n\nBM\n:0\n\n.8\n67\n\n1\n50\n\n0\nN\n\nBM\n:0\n\n.8\n67\n\n1\n\nEl\nec\n\ntr\non\n\nic\ns\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n99\n6\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n56\n7\n\n20\n00\n\nN\nBM\n\n:0\n.8\n\n33\n7\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n56\n4\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n55\n1\n\nBo\nok\n\n15\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n77\n1\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n50\n6\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n86\n4\n\nKi\ntc\n\nhe\nn\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n79\n0\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n31\n4\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n01\n7\n\n50\n0\n\nSV\nM\n\n:0\n.8\n\n37\n8\n\n50\n0\n\nSV\nM\n\n:0\n.8\n\n37\n8\n\n\n\nPage 12 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nclassification by finding a hyperplane that clearly separates the two classes [40]. In order \nto do this, we need to represent documents as weighted vectors so that we can measure \nthe distances or similarities between the documents. For sentiment analysis, however, \nwe are favouring features that have low frequencies within a document, but relatively \nhigh frequencies across different documents (as illustrated by the example of “great” in \nthe introduction), making the distance/similarity measures less effective. Both NBM and \nLR are based on the probabilities of the features in the given dataset. In particular, LR \nis equivalent to the maximum entropy modelling and is capable of handling dependent \nfeatures, whereas NBM makes the naïve assumption that all features are independent \nof each other. In our experiments, NBM does better than LR, which could be due to the \nsame reason as we just explained for SVM above.\n\nTo see the impacts of feature sizes for different feature selection methods, we plot our \nresults for the Turkish electronic review dataset in Fig.  1. Clearly, OCFS lags behind \nother feature selection methods across all feature sizes. DFD tends to do better with big-\nger feature sizes, while CHI2 and IG tend to favour smaller feature sizes. In addition, \nthe results for CHI2 and IG are sufficiently close, although they are slightly different for \ncertain feature sizes. Our new method QER does reasonably well across all other meth-\nods. For Turkish electronics review dataset, QER is the best performer and the selected \nfeatures include 7.7% of the punctuation patterns and 25% of the stop words; the features \nselected by DFD method include 61.5% of the punctuation patterns and 59% of the stop \nwords; the features selected by CHI2 method include 15% of the punctuation patterns \nand 90% of the stop words; and the features selected by OCFS method include 69.2% of \n\nTable 3 Detailed results for the Turkish electronics review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 1500 0.8996 2000 0.8715 1000 0.7927 2000 0.6734\n\nCHI2 1000 0.8564 1000 0.8505 500 0.7969 1000 0.7435\n\nIG 1500 0.8551 1000 0.8505 500 0.8156 1500 0.7428\n\nDFD 1500 0.8567 1500 0.8128 2500 0.7829 500 0.7399\n\nOCFS 2000 0.8337 1000 0.7729 3000 0.7643 1500 0.7371\n\nFig. 1 Detailed results of feature sizes for the Turkish electronic review dataset\n\n\n\nPage 13 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nthe punctuation patterns and 49.6% of the stop words. Therefore, CHI2 method tends \nto favor stop words but not punctuation patterns, while DFD and OCFS tend to choose \nmore punctuation patterns and fewer stop words. In addition, when we compare the fea-\ntures selected by QER and CHI2 methods, we observe that 5.7% of selected features are \nthe same, and for QER and DFD methods, there are 6.9% of the features that are com-\nmon, and for QER and OCFS methods, there are 7% of the features that are common. \nHowever, for DFD and CHI2 methods, we observe that 49.8% of the selected features are \nthe same, and for DFD and OCFS methods, there are 76.7% of the features that are com-\nmon, and for CHI2 and OCFS methods, there are 34% of the features that are common. \nNote that although we only show the results on specific datasets in Table 3 and Fig. 1, \nsimilar trends are observed for other datasets as well, and to save space these results are \nnot included.\n\nPerformance of feature selection methods for English reviews\n\nUsing similar settings as described in “Performance of feature selection methods for \nTurkish reviews”, we also carried out experiments on the English review datasets. As \nshown in Table 4, QER achieved the best performance with LR classifier for the movie \nreview dataset and NBM classifier for other datasets. CHI2 and IG achieved better per-\nformance with NBM for all five datasets. Once again, the results are basically the same \nfor CHI2 and IG, indicating that the two methods are also strongly correlated for the \nEnglish review datasets. Compared with the Turkish movie reviews, the feature size for \nthe best performer of the English movie reviews is 3000, which is achieved with QER for \nthe LR classifier. This is likely due to the bigger vocabulary of the English movie reviews \nover that of the Turkish movie reviews as can be observed in Table 1. Also compared \nwith the Turkish review datasets, DFD is not as good as CHI2 and IG for the English \nreview datasets, even though the performance is close for the kitchen reviews and gener-\nally better than OCFS. Furthermore, the best results for DFD are achieved with differ-\nent classifiers for different datasets: SVM for the movie reviews and LR for the kitchen \nreviews. Statistical analysis with univariate ANOVA and post hoc tests show similar \nresults as those for the Turkish reviews: there are significant differences between three \ngroups (Baseline and OCFS), (DFD), and (CHI2, IG, and QER) at 95% confidence level \nfor the movie, DVDs, electronic, and kitchen review datasets, but for the book review \ndataset, there are significant differences between two groups (Baseline and OCFS) and \n(DFD, CHI2, IG, and QER) at the 95% confidence level.\n\nFor text classifiers, Table  4 shows that similar trends are observed for the English \nreviews as those for the Turkish reviews, although LR and SVM can over-perform NBM \nfor some feature selection methods. For different feature sizes, similar trends are also \nobserved, as illustrated in Fig.  2. Once again, in Table  5 and Fig.  2, we only show the \nresults for specific datasets, but the trends are similar to other datasets as well.\n\nIn summary, we see some similarities between Turkish and English reviews in that for \ndata pre-processing, we should keep punctual patterns and stop words, and not per-\nform stemming, leading us to use the same setting as the baselines for further study. \nIn addition, NBM seems to be the most suitable classifier for sentiment analysis since \nsentiment-expressing words tend to have low frequencies within a document, but rela-\ntively high frequencies across different documents. For feature selection methods, our \n\n\n\nPage 14 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nTa\nb\n\nle\n 4\n\n T\nh\n\ne \nb\n\nes\nt c\n\nla\nss\n\nifi\nca\n\nti\no\n\nn\n r\n\nes\nu\n\nlt\ns \n\nfo\nr \n\np\nai\n\nrs\n o\n\nf f\nea\n\ntu\nre\n\n s\nel\n\nec\nti\n\no\nn\n\n m\net\n\nh\no\n\nd\ns \n\nan\nd\n\n th\ne \n\nEn\ng\n\nlis\nh\n\n r\nev\n\nie\nw\n\n d\nat\n\nas\net\n\ns\n\nQ\nER\n\nD\nFD\n\nO\nC\n\nFS\nC\n\nH\nI2\n\nIG\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\nSi\n\nze\nF \n\nm\nea\n\nsu\nre\n\nSi\nze\n\nF \nm\n\nea\nsu\n\nre\n\nM\nov\n\nie\n30\n\n00\nLR\n\n:0\n.9\n\n55\n0\n\n25\n00\n\nSV\nM\n\n:0\n.8\n\n64\n0\n\n30\n00\n\nSV\nM\n\n: 0\n.8\n\n28\n5\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\n25\n00\n\nN\nBM\n\n:0\n.9\n\n15\n0\n\nD\nVD\n\ns\n25\n\n00\nN\n\nBM\n:0\n\n.9\n16\n\n9\n30\n\n00\nN\n\nBM\n:0\n\n.8\n50\n\n2\n10\n\n00\nN\n\nBM\n:0\n\n.7\n99\n\n6\n10\n\n00\nN\n\nBM\n:0\n\n.8\n96\n\n4\n10\n\n00\nN\n\nBM\n:0\n\n.8\n96\n\n4\n\nEl\nec\n\ntr\non\n\nic\ns\n\n20\n00\n\nN\nBM\n\n:0\n.8\n\n87\n8\n\n15\n00\n\nN\nBM\n\n:0\n.8\n\n22\n1\n\n20\n00\n\nSV\nM\n\n: 0\n.7\n\n82\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n62\n1\n\nBo\nok\n\n30\n00\n\nN\nBM\n\n:0\n.9\n\n16\n2\n\n30\n00\n\nN\nBM\n\n:0\n.8\n\n62\n8\n\n30\n00\n\nN\nBM\n\n:0\n.7\n\n89\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\n10\n00\n\nN\nBM\n\n:0\n.8\n\n87\n9\n\nKi\ntc\n\nhe\nn\n\n20\n00\n\nN\nBM\n\n:0\n.9\n\n10\n6\n\n30\n00\n\nLR\n:0\n\n.8\n89\n\n3\n15\n\n00\nSV\n\nM\n: 0\n\n.8\n15\n\n7\n50\n\n0\nN\n\nBM\n:0\n\n.8\n96\n\n4\n50\n\n0\nN\n\nBM\n:0\n\n.8\n96\n\n4\n\n\n\nPage 15 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nproposed QER achieves best performances with feature sizes between 2000 and 3000. \nCHI2 and IG are strongly correlated and tend to work well with smaller feature sizes, \nwhile DFD also works reasonably well, but with bigger feature sizes. For differences, \nthe English review datasets usually have bigger vocabulary, resulting in relatively big-\nger feature sizes for feature selection. Moreover, SVM and LR can also perform well for \nsome English review datasets, while NBM looks like a dominant classifier for the Turk-\nish reviews. Finally, the performance results for the English reviews are generally higher \nthan those for the Turkish reviews, possibly related to the differences between the two \nlanguages in terms of vocabularies, writing styles, and the agglutinative property of the \nTurkish language. The limitation of QER is that it is only suitable for classifying two \nclasses since it is especially developed for sentiment analysis with the observation that \nsentiment-expressing words are usually more frequent across different reviews. The con-\ntribution of QER is that, as it is shown in the experimental results, the method is both \nlanguage and classifier independent and can select better features than other methods \nfor sentiment analysis.\n\nComparison of our proposal with the previous studies\n\nIt is generally difficult to directly compare the results of different studies since there are \noften differences in partitioning and preprocessing the datasets for training and testing, \nas shown in the studies by Pang et al. [4]. That is why we tried different combinations of \nfeature selection methods and text classifiers on multiple datasets in our research so that \n\nFig. 2 Detailed results of feature sizes for the English DVDs review dataset\n\nTable 5 Detailed results for the English DVD review dataset\n\nNBM SVM LR J48\n\nSize F measure Size F measure Size F measure Size F measure\n\nQER 2500 0.9169 3000 0.8724 2000 0.8977 2000 0.5481\n\nCHI2 1000 0.8964 500 0.8650 3000 0.6976 3000 0.6799\n\nIG 1000 0.8964 1000 0.8614 2000 0.6970 500 0.6769\n\nDFD 3000 0.8502 1000 0.8293 3000 0.7600 500 0.6771\n\nOCFS 1000 0.7996 1000 0.7714 500 0.6800 2000 0.6829\n\n\n\nPage 16 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nwe can compare their performance collectively and accurately. However, we do agree \nthat it is helpful to describe the results from the related studies so that we can put our \nresults into a suitable context. Table 6 includes a summary for comparison of our results \nwith that of the previous studies which have used the same datasets with our study. For \nthe English movie review dataset, Nicholls and Song [8] obtained a baseline accuracy \nof 79.9% with the MEM classifier, and better classification accuracies of 86.9, 85.7, and \n80.9% when combined with DFD, CHI2, and OCFS feature selection methods, respec-\ntively. Dang et al. [23] examined their proposed semantic oriented method on the prod-\nuct dataset [37]. They achieved an accuracy of 84.2% for the kitchen dataset. Also, Xia \net al. [24] improved the classification performances from 84.8 to 87.7% using their pro-\nposed word relation based feature selection method. Bai [25] improved the accuracies \nfrom baseline 84.1–92.7% using their proposed Tabu search-enhanced Markov blanket \nmodel for the movie review dataset. Pang et al. [4] obtained accuracy around 78.7% with \nNB using the document frequency of 4 to eliminate the rare features. Agarwal et al. [9] \nimproved the accuracies from baseline 82.7–89.2% using IG feature selection method \nwith Boolean NBM. Our proposed QER method showed an improvement from the \nbaseline of 81.3–91.1% with NBM in terms of F measures.\n\nFor the Turkish movie review dataset, the best classification result of 82.58% is \nobtained with the SVM classifier [30]. As shown in the previous studies, classification \naccuracy is improved by applying feature selection, and NB based classifier performs the \nbest in the majority of the cases. The proposed feature selection method is also com-\nputationally efficient and easy to implement as it only computes scores for features by \ncounting document frequencies.\n\nConclusions\nIn this paper, we proposed a new feature selection method query expansion ranking \n(QER) for the sentiment analysis and compared it with the common feature selection \nmethods for sentiment classification, including DFD and OCFS, CHI2 and IG. All of \nthese methods are tested against five datasets of Turkish reviews, using four common \n\nTable 6 Summary of related work on the sentiment analysis for the same datasets\n\nPaper Dataset Baseline accuracy (%) Best accuracies observed (%) Classifier\n\n[4] Movie 78.7 NB, SVM\n\n[7] Movie 87.1 minimum cut SVM\n\n[8] Movie\nProduct\n\n79.9\n74.3\n\n85.7 CHI2; 86.9 DFD; 80.9 OCFS\n73.7 CHI2; 75 DFD; 73.8 OCFS\n\nMEM\n\n[9] Movie\nProduct\n\n84.2\n80.9 Book; 78.9 DVD; 80.8 El\n\n91.8\n92.5 Book; 91.5 DVD; 91.8 El\nmRMR with composite features\n\nBNBM, SVM\n\n[23] Product 70.1 84.2% Kitc. semantic orientation SVM\n\n[24] Movie\nProduct\n\n84.8\n74.7 Book; 77.2 DVD; 80.8 El.; \n\n83.3 Kitc\n\n87.7\n81.8 Book; 83.8 DVD; 85.9 El.; 88.7 Kitc \n\nword relation based method\n\nNB, SVM, MEM\n\n[25] Movie 84.1 92.7% Tabu search-enhanced Markov \nblanket model\n\nNB, SVM, MEM\n\nOur study Movie\nProduct\n\n84.8\n76.2 Book; 78.4 DVD; 78.6 \n\nElect; 81.4 Kitc\n\n91.5 CHI2-IG; 87.1 DFD; 82.9 OCFS; \n95.5 91.6 Book; 91.7 DVD; 88.8 Elect; \n91.1 Kitc proposed QER\n\nNBM, SVM, MEM, DT\n\n\n\nPage 17 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\ntext classifiers, including NBM, SVM, logistic regression (LR), and decision trees (J48). \nSimilar experiments are also conducted for English reviews so that we can compare \ntheir differences with the Turkish reviews. Our results show that for all Turkish review \ndatasets, the best results are all obtained with the NBM classifier, and for some Eng-\nlish review datasets, LR and SVM have the best performance. For feature selection, our \nproposed QER method helps to achieve the best performance compared with all other \nfeature selection methods for both Turkish and English reviews. For feature selection, \nour experiments show that our proposed QER method helps to achieve the best per-\nformance among all other feature selection methods. We found that CHI2 and IG have \nalmost the same performance for the Turkish reviews and they tend to work well with \nsmaller feature sizes compared with other feature selection methods. DFD does reason-\nably well across all review datasets, but it tends to favour bigger feature sizes. This con-\nfirms our intuition that sentiment-expressing words usually have low frequencies within \na document, but relatively high frequencies across different documents. Although OCFS \nis quite robust for traditional topical text classification, it does not do well for sentiment \nanalysis since it relies on word frequencies to measure the distances between docu-\nments. Once again, NBM remains the best performer for most of our experiments when \nanalysed with QER method. Overall, feature selection methods are shown to be effective \nfor sentiment analysis, improving significantly over the baseline results.\n\nFollowing a similar process, we also carried out experiments on English review data-\nsets and NBM seems to be the most suitable classifier for sentiment analysis. For fea-\nture selection methods, CHI2 and IG are strongly correlated and tend to work well with \nsmaller feature sizes, while DFD also works reasonably well, but with bigger feature \nsizes. Our proposed query expansion ranking method achieves the best performances \nfor the English datasets as well. As for differences, the English review datasets usually \nhave a bigger vocabulary, resulting in relatively bigger feature sizes for feature selection. \nMoreover, LR and SVM also perform well for some English review datasets, while NBM \nlooks like a dominant classifier for the Turkish reviews. The performance results for the \nEnglish reviews are generally higher than those for the Turkish reviews, possibly related \nto the differences between the two languages in terms of vocabularies, writing styles, \nand the agglutinative property of the Turkish language. Finally, the experimental results \nshow that our proposal QER method is language, domain and classifier independent \nand improve the classification performance better than other FS methods for sentiment \nanalysis.\nAuthors’ contributions\nTP drafted this manuscript, conducted experiments using the datasets and analyzed the results. SAO and FS suggested \nthe methods used in this study and provided guidelines in drafting the manuscript. FS edited and corrected the manu-\nscript. All authors read and approved the final manuscript.\n\nAuthors’ information\nTP received her Ph.D. degree in Computer Engineering from Çukurova University in 2016. She received a Bachelor of \nEngineering degree in Computer Engineering from Hacettepe University, and she holds a M.Sc. in Management Infor-\nmation Sciences and a M.Sc. in Mathematics. She studied for 4 months of 2015 as a visiting researcher in University of \nGuelph, Canada with a scholarship supporting by The Scientific and Technological Research Council of Turkey (TUBITAK). \nShe is currently working as a senior lecturer and head of the Computer Technologies Department, Antakya Vocational \nSchool, Mustafa Kemal University. Her research interest is in sentiment analysis, data mining, machine learning, and \napplying text processing techniques to medical data extraction and integration.\n\nSAO received her Ph.D. and Bachelor of Science degrees both in Computer Engineering from Bilkent University, \nTurkey, in 2004 and 1996, respectively. Currently she is a professor and head of the Department of Computer Engineer-\ning, Çukurova University, Turkey. Her research interests include text mining, information retrieval systems, and applying \nbiological and nature inspired computing to text mining.\n\n\n\nPage 18 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\nFS received his Ph.D. degree in Computer Science from the University of Waterloo in Canada. He is currently an \nassociate professor in the School of Computer Science, University of Guelph in Canada. His interests are mostly in Natural \nLanguage Processing, working on a wide range of topic areas, including information retrieval, text classification, topic \nmodeling, key phrase extraction, text segmentation, sentiment analysis, text summarization, and document clustering. \nMore recently, he is also interested in applying text processing techniques to privacy policy analysis and medical data \nextraction and integration.\n\nAuthor details\n1 Department of Mathematics, Mustafa Kemal University, Antakya, Hatay, Turkey. 2 Department of Computer Engineering, \nÇukurova University, Adana, Turkey. 3 School of Computer Science, University of Guelph, Guelph, Canada. \n\nAcknowledgements\nThis research is supported by TUBITAK-2214-A.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nNot applicable.\n\nEthics approval and consent to participate\nNot applicable.\n\nFunding\nThis research is supported by Çukurova University Fund of Scientific Research Projects under Grant No. FDK-2015-3833, \nand Mustafa Kemal University Fund of Scientific Research Projects under Grant No. 15426.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 10 February 2018 Accepted: 16 April 2018\n\nReferences\n 1. Pang B, Lee L (2008) Opinion mining and sentiment analysis. Found Trends Inf Retr 2:1–135. https://doi.\n\norg/10.1561/1500000011\n 2. Tripathy A, Anand A, Rath SK (2017) Document-level sentiment classification using hybrid machine learning \n\napproach. Knowl Inf Syst 53:805–831. https://doi.org/10.1007/s10115-017-1055-z\n 3. Yang D-H, Yu G (2013) A method of feature selection and sentiment similarity for Chinese micro-blogs. J Inf Sci \n\n39:429–441. https://doi.org/10.1177/0165551513480308\n 4. Pang B, Lee L, Vaithyanathan S (2002) Thumbs up? In: Proceedings of the ACL-02 conference on empirical methods \n\nin natural language processing—EMNLP’02. Association for computational linguistics, Morristown, pp 79–86\n 5. Mullen T, Collier N (2004) Sentiment analysis using support vector machines with diverse information sources. Conf \n\nEmpir Methods Nat Lang Process. https://doi.org/10.3115/1219044.1219069\n 6. Kaya M, Fidan G, Toroslu IH (2012) Sentiment analysis of Turkish political news. In: 2012 IEEE/WIC/ACM international \n\nconferences on web intelligence and intelligent agent technology. IEEE, Macau, pp 174–180\n 7. Pang B, Lee L (2004) A sentimental education. In: Proceedings of the 42nd annual meeting on association for com-\n\nputational linguistics—ACL’04. Association for Computational Linguistics, Morristown, p 271–es\n 8. Nicholls C, Song F (2010) Comparison of feature selection methods for sentiment analysis. In: Advances in artificial \n\nintelligence. Springer, Berlin, pp 286–289\n 9. Agarwal B, Mittal N (2016) Prominent feature extraction for review analysis: an empirical study. J Exp Theor Artif Intell \n\n28:485–498. https://doi.org/10.1080/0952813X.2014.977830\n 10. Parlar T, Ozel SA (2016) A new feature selection method for sentiment analysis of Turkish reviews. In: International \n\nSymposium on INnovations in Intelligent SysTems and Applications (INISTA). IEEE, Sinaia, pp 1–6\n 11. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst 13:1397–\n\n1409. https://doi.org/10.3745/JIPS.02.0076\n 12. Harman D (1992) Relevance feedback revisited. In: Proceedings of the 15th annual international ACM SIGIR confer-\n\nence on Research and development in information retrieval—SIGIR’92. ACM Press, New York, pp 1–10\n 13. Robertson SE, Jones KS (1976) Relevance weighting of search terms. J Am Soc Inf Sci 27:129–146. https://doi.\n\norg/10.1002/asi.4630270302\n 14. Aldoğan D, Yaslan Y (2017) A comparison study on active learning integrated ensemble approaches in sentiment \n\nanalysis. Comput Electr Eng 57:311–323. https://doi.org/10.1016/J.COMPELECENG.2016.11.015\n 15. Singh J, Singh G, Singh R (2017) Optimization of sentiment analysis using machine learning classifiers. Hum centric \n\nComput Inf Sci 7:32. https://doi.org/10.1186/s13673-017-0116-3\n 16. Mladenović M, Mitrović J, Krstev C, Vitas D (2016) Hybrid sentiment analysis framework for a morphologically rich \n\nlanguage. J Intell Inf Syst 46:599–620. https://doi.org/10.1007/s10844-015-0372-5\n 17. Asgarian E, Kahani M, Sharifi S (2018) The impact of sentiment features on the sentiment polarity classification in \n\nPersian reviews. Cognit Comput 10:117–135. https://doi.org/10.1007/s12559-017-9513-1\n\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1561/1500000011\nhttps://doi.org/10.1007/s10115-017-1055-z\nhttps://doi.org/10.1177/0165551513480308\nhttps://doi.org/10.3115/1219044.1219069\nhttps://doi.org/10.1080/0952813X.2014.977830\nhttps://doi.org/10.3745/JIPS.02.0076\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1002/asi.4630270302\nhttps://doi.org/10.1016/J.COMPELECENG.2016.11.015\nhttps://doi.org/10.1186/s13673-017-0116-3\nhttps://doi.org/10.1007/s10844-015-0372-5\nhttps://doi.org/10.1007/s12559-017-9513-1\n\n\nPage 19 of 19Parlar et al. Hum. Cent. Comput. Inf. Sci. (2018) 8:10 \n\n 18. Guyon I, Elisseeff A (2003) An introduction to variable and feature selection. J Mach Learn Res 3:1157–1182. https://\ndoi.org/10.1016/j.aca.2011.07.027\n\n 19. Akba F, Uçan A, Sezer E, Sever H (2014) Assessment of feature selection metrics for sentiment analyses: Turkish \nmovie reviews. In: 8th European conference on data mining. Lisbon, Portugal, pp 180–184\n\n 20. Liu Y, Bi JW, Fan ZP (2017) Multi-class sentiment classification: the experimental comparisons of feature selection \nand machine learning algorithms. Expert Syst Appl 80:323–339. https://doi.org/10.1016/j.eswa.2017.03.042\n\n 21. Sagar K, Saha A (2017) Qualitative usability feature selection with ranking: a novel approach for ranking the identi-\nfied usability problematic attributes for academic websites using data-mining techniques. Hum centric Comput Inf \nSci 7:29. https://doi.org/10.1186/s13673-017-0111-8\n\n 22. Abbasi A, Chen H, Salem A (2008) Sentiment analysis in multiple languages: Feature selection for opinion classifica-\ntion in Web forums. ACM Trans Inf Syst 26:1–34. https://doi.org/10.1145/1361684.1361685\n\n 23. Dang Y, Zhang Y, Chen H (2010) A Lexicon-enhanced method for sentiment classification: an experiment on online \nproduct reviews. IEEE Intell Syst 25:46–53. https://doi.org/10.1109/MIS.2009.105\n\n 24. Xia R, Zong C, Li S (2011) Ensemble of feature sets and classification algorithms for sentiment classification. Inf Sci \n(Ny) 181:1138–1152. https://doi.org/10.1016/j.ins.2010.11.023\n\n 25. Bai X (2011) Predicting consumer sentiments from online text. Decis Support Syst 50:732–742. https://doi.\norg/10.1016/j.dss.2010.08.024\n\n 26. Yan J, Liu N, Zhang B, et al (2005) OCFS: optimal orthogonal centroid feature selection for text categorization. In: \nProceedings of the 28th annual international ACM SIGIR conference on Research and development in information \nretrieval—SIGIR’05. ACM Press, New York, p 122\n\n 27. Zheng L, Wang H, Gao S (2018) Sentimental feature selection for sentiment analysis of Chinese online reviews. Int J \nMach Learn Cybern 9:75–84. https://doi.org/10.1007/s13042-015-0347-4\n\n 28. Demirtas E, Pechenizkiy M (2013) Cross-lingual polarity detection with machine translation. In: Second international \nworkshop on issues of sentiment discovery and opinion mining—WISDOM’13. ACM Press, New York, pp 1–8\n\n 29. Boynukalin Z (2012) Emotion analysis of Turkish texts by using machine learning methods. M.Sc. Thesis, Middle East \nTechnical University\n\n 30. Sevindi BI (2013) Türkçe Metinlerde Denetimli ve Sözlük Tabanlı Duygu Analizi Yaklaşımlarının Karşılaştırılması. M.Sc. \nThesis, Gazi University\n\n 31. Parlar T, Özel SA, Song F (2018) Interactions between term weighting and feature selection methods on the senti-\nment analysis of Turkish reviews. In: Computational linguistics and intelligent text processing. CICLing 2016. Lecture \nNotes in computer Science, vol 9624. Springer, Cham, pp 335–346\n\n 32. Çakici R (2009) Wide-coverage parsing for Turkish. Ph.D. Thesis, University of Edinburgh\n 33. Witten IH, Frank E, Hall MA (2011) Data mining: practical machine learning tools and techniques. Morgan Kaufmann, \n\nBurlington\n 34. McCallum A, Nigam K (1998) A comparison of event models for naive Bayes text classification. In: AAAI/ICML-98 \n\nworkshop on learning for text categorization. pp 41–48\n 35. Zhao X, Li D, Yang B et al (2015) A two-stage feature selection method with its application. Comput Electr Eng \n\n47:114–125. https://doi.org/10.1016/J.COMPELECENG.2015.08.011\n 36. Harman D (1988) Towards interactive query expansion. In: Proceedings of the 11th annual international ACM SIGIR \n\nconference on Research and development in information retrieval—SIGIR’88. ACM Press, New York, pp 321–331\n 37. Blitzer J, Dredze M, Pereira F (2007) Biographies, bollywood, boom-boxes and blenders: domain adaptation for senti-\n\nment classification. In: 45th annual meeting-association for computational linguistics. pp 440–447\n 38. Bird S, Klein E, Loper E (2009) Natural language processing with Python. O’Reilly, Newton\n 39. Cai J, Song F (2008) Maximum entropy modeling with feature selection for text categorization. In: Lecture notes in \n\ncomputer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics). pp \n549–554\n\n 40. Joachims T (1998) Text categorization with support vector machines: learning with many relevant features. Springer, \nBerlin, pp 137–142\n\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.aca.2011.07.027\nhttps://doi.org/10.1016/j.eswa.2017.03.042\nhttps://doi.org/10.1186/s13673-017-0111-8\nhttps://doi.org/10.1145/1361684.1361685\nhttps://doi.org/10.1109/MIS.2009.105\nhttps://doi.org/10.1016/j.ins.2010.11.023\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1016/j.dss.2010.08.024\nhttps://doi.org/10.1007/s13042-015-0347-4\nhttps://doi.org/10.1016/J.COMPELECENG.2015.08.011\n\n\tQER: a new feature selection method for sentiment analysis\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethods\n\tMachine learning algorithms\n\tFeature selection\n\tInformation gain\n\tChi square (CHI2)\n\tDocument frequency difference\n\tOptimal orthogonal centroid (OCFS)\n\tQuery expansion ranking\n\n\n\tExperiments and results\n\tDatasets\n\tPerformance evaluation\n\tExperimental settings\n\tPerformance of feature selection methods for Turkish reviews\n\tPerformance of feature selection methods for English reviews\n\tComparison of our proposal with the previous studies\n\n\tConclusions\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczEzNjczLTAxOC0wMTM1LTgucGRm0", "metadata_author": "Tuba Parlar ", "metadata_title": "QER: a new feature selection method for sentiment analysis", "metadata_creation_date": "2018-04-18T08:33:56Z", "keyphrases": [ "new feature selection method", "sentiment analysis", "QER" ] }, { "@search.score": 1, "content": "\nBig data stream analysis: a systematic \nliterature review\nTaiwo Kolajo1,2* , Olawande Daramola3 and Ayodele Adebiyi1,4 \n\nIntroduction\nAdvances in information technology have facilitated large volume, high-velocity of data, \nand the ability to store data continuously leading to several computational challenges. \nDue to the nature of big data in terms of volume, velocity, variety, variability, veracity, \nvolatility, and value [1] that are being generated recently, big data computing is a new \ntrend for future computing.\n\nBig data computing can be generally categorized into two types based on the process-\ning requirements, which are big data batch computing and big data stream computing \n\nAbstract \n\nRecently, big data streams have become ubiquitous due to the fact that a number of \napplications generate a huge amount of data at a great velocity. This made it difficult \nfor existing data mining tools, technologies, methods, and techniques to be applied \ndirectly on big data streams due to the inherent dynamic characteristics of big data. In \nthis paper, a systematic review of big data streams analysis which employed a rigorous \nand methodical approach to look at the trends of big data stream tools and technolo-\ngies as well as methods and techniques employed in analysing big data streams. It \nprovides a global view of big data stream tools and technologies and its comparisons. \nThree major databases, Scopus, ScienceDirect and EBSCO, which indexes journals and \nconferences that are promoted by entities such as IEEE, ACM, SpringerLink, and Elsevier \nwere explored as data sources. Out of the initial 2295 papers that resulted from the \nfirst search string, 47 papers were found to be relevant to our research questions after \nimplementing the inclusion and exclusion criteria. The study found that scalability, \nprivacy and load balancing issues as well as empirical analysis of big data streams and \ntechnologies are still open for further research efforts. We also found that although, sig-\nnificant research efforts have been directed to real-time analysis of big data stream not \nmuch attention has been given to the preprocessing stage of big data streams. Only a \nfew big data streaming tools and technologies can do all of the batch, streaming, and \niterative jobs; there seems to be no big data tool and technology that offers all the key \nfeatures required for now and standard benchmark dataset for big data streaming ana-\nlytics has not been widely adopted. In conclusion, it was recommended that research \nefforts should be geared towards developing scalable frameworks and algorithms that \nwill accommodate data stream computing mode, effective resource allocation strategy \nand parallelization issues to cope with the ever-growing size and complexity of data.\n\nKeywords: Big data stream analysis, Stream computing, Big data streaming tools and \ntechnologies\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nKolajo et al. J Big Data (2019) 6:47 \nhttps://doi.org/10.1186/s40537-019-0210-7\n\n*Correspondence: \ntaiwo.kolajo@stu.cu.edu.ng; \ntaiwo.kolajo@fulokoja.edu.ng \n1 Department of Computer \nand Information Sciences, \nCovenant University, Ota, \nNigeria\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-6780-2495\nhttp://orcid.org/0000-0001-6340-078X\nhttp://orcid.org/0000-0002-3114-6315\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0210-7&domain=pdf\n\n\nPage 2 of 30Kolajo et al. J Big Data (2019) 6:47 \n\n[2]. Big data batch processing is not sufficient when it comes to analysing real-time \napplication scenarios. Most of the data generated in a real-time data stream need real-\ntime data analysis. In addition, the output must be generated with low-latency and any \nincoming data must be reflected in the newly generated output within seconds. This \nnecessitates big data stream analysis [3].\n\nThe demand for stream processing is increasing. The reason being not only that huge \nvolume of data need to be processed but that data must be speedily processed so that \norganisations or businesses can react to changing conditions in real-time.\n\nThis paper presents a systematic review of big data stream analysis. The purpose is to \npresent an overview of research works, findings, as well as implications for research and \npractice. This is necessary to (1) provide an update about the state of research, (2) iden-\ntify areas that are well researched, (3) showcase areas that are lacking and need further \nresearch, and (4) build a common understanding of the challenges that exist for the ben-\nefit of the scientific community.\n\nThe rest of the paper is organized as follows: “Background and related work” section \nprovides information on stream computing and big data stream analysis and the key \nissues involved in it and presents a review on big data streaming analytics. In “Research \nmethod” section, the adopted research methodology is discussed, while “Result” section \npresents the findings of the study. “Discussion” section presents a detailed evaluation \nperformed on big data stream analysis, “Limitation of the review” section highlights the \nlimitations of the study, while “Conclusion and further work” concludes the paper.\n\nBackground and related work\nStream computing\n\nStream computing refers to the processing of massive amount of data generated at high-\nvelocity from multiple sources with low latency in real-time. It is a new paradigm neces-\nsitated because of new sources of data generating scenarios which include ubiquity of \nlocation services, mobile devices, and sensor pervasiveness [4]. It can be applied to the \nhigh-velocity flow of data from real-time sources such as the Internet of Things, Sensors, \nmarket data, mobile, and clickstream.\n\nThe fundamental assumption of this paradigm is that the potential value of data lies in \nits freshness. As a result, data are analysed as soon as they arrive in a stream to produce \nresult as opposed to what obtains in batch computing where data are first stored before \nthey are analysed. There is a crucial need for parallel architectures and scalable com-\nputing platforms [5]. With stream computing, organisations can analyse and respond in \nreal-time to rapidly changing data. Streaming processing frameworks include Storm, S4, \nKafka, and Spark [6–8]. The real contrasts between the batch processing and the stream \nprocessing paradigms are outlined in Table 1.\n\nIncorporating streaming data into decision-making process necessitates a program-\nming paradigm called stream computing. With stream computing, fairly static questions \ncan be evaluated on data in motion (i.e. real-time data) continuously [9].\n\nBig data stream analysis\n\nThe essence of big data streaming analytics is the need to analyse and respond to real-\ntime streaming data using continuous queries so that it is possible to continuously \n\n\n\nPage 3 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperform analysis on the fly within the stream. Stream processing solutions must be \nable to handle a real-time, high volume of data from diverse sources putting into con-\nsideration availability, scalability and fault tolerance. Big data stream analysis involves \nassimilation of data as an infinite tuple, analysis and production of actionable results \nusually in a form of stream [10].\n\nIn a stream processor, applications are represented as data flow graph made up of \noperations and interconnected streams as depicted in Fig. 1. In a streaming analytics \nsystem, application comes in a form of continuous queries, data are ingested continu-\nously, analysed and correlated, and stream of results are generated. Streaming analytic \napplications is usually a set of operators connected by streams. Streaming analytics \nsystems must be able to identify new information, incrementally build models and \naccess whether the new incoming data deviate from model predictions [9].\n\nThe idea of streaming analytics is that each of the received data tuples is processed \nin the data processing node. Such processing includes removing duplicates, filling \nmissing data, data normalization, parsing, feature extraction, which are typically done \nin a single pass due to the high data rates of external feeds. When a new tuple arrives, \nthis node is triggered, and it expels tuples older than the time specified in the sliding \nwindow (sliding window is a typical example of windows used in stream computing \nwhich keeps only the latest tuples up to the time specified in the windows). A window \n\nTable 1 Comparison between batch processing and streaming processing [82]\n\nDimension Batch processing Streaming processing\n\nInput Data chunks Stream of new data or updates\n\nData size Known and finite Infinite or unknown in advance\n\nHardware Multiple CPUs Typical single limited amount of memory\n\nStorage Store Not store or store non-trivial portion in memory\n\nProcessing Processed in multiple rounds A single or few passes over data\n\nTime Much longer A few seconds or even milliseconds\n\nApplications Widely adopted in almost every domain Web mining, traffic monitoring, sensor networks\n\nFig. 1 Data flow graph of a stream processor. The figure shows how applications (made up of operations and \ninterconnected streams) are represented as data flow graph in a stream processor [10]\n\n\n\nPage 4 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nis referred to as a logical container for data tuples received. It defines how frequently \ndata is refreshed in the container as well as when data processing is triggered [4].\n\nKey issues in big data stream analysis\n\nBig data stream analysis is relevant when there is a need to obtain useful knowledge \nfrom current happenings in an efficient and speedy manner in order to enable organisa-\ntions to quickly react to problems, or detect new trends which can help improve their \nperformance. However, there are some challenges such as scalability, integration, fault-\ntolerance, timeliness, consistency, heterogeneity and incompleteness, load balancing, \nprivacy issues, and accuracy [3, 11–18] which arises from the nature of big data streams \nthat must be dealt with.\n\nScalability\n\nOne of the main challenges in big data streaming analysis is the issue of scalability. The \nbig data stream is experiencing exponential growth in a way much faster than computer \nresources. The processors follow Moore’s law, but the size of data is exploding. There-\nfore, research efforts should be geared towards developing scalable frameworks and \nalgorithms that will accommodate data stream computing mode, effective resource allo-\ncation strategy and parallelization issues to cope with the ever-growing size and com-\nplexity of data.\n\nIntegration\n\nBuilding a distributed system where each node has a view of the data flow, that is, every \nnode performing analysis with a small number of sources, then aggregating these views \nto build a global view is non-trivial. An integration technique should be designed to ena-\nble efficient operations across different datasets.\n\nFault‑tolerance\n\nHigh fault-tolerance is required in life-critical systems. As data is real-time and infinite \nin big data stream computing environments, a good scalable high fault-tolerance strat-\negy is required that allows an application to continue working despite component failure \nwithout interruption.\n\nTimeliness\n\nTime is of the essence for time-sensitive processes such as mitigating security threats, \nthwarting fraud, or responding to a natural disaster. There is a need for scalable architec-\ntures or platforms that will enable continuous processing of data streams which can be \nused to maximize the timeliness of data. The main challenge is implementing a distrib-\nuted architecture that will aggregate local views of data into global view with minimal \nlatency between communicating nodes.\n\nConsistency\n\nAchieving high consistency (i.e. stability) in big data stream computing environments is \nnon-trivial as it is difficult to determine which data are needed and which nodes should \nbe consistent. Hence a good system structure is required.\n\n\n\nPage 5 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nHeterogeneity and incompleteness\n\nBig data streams are heterogeneous in structure, organisations, semantics, accessi-\nbility and granularity. The challenge here is how to handle an always ever-increas-\ning data, extract meaningful content out of it, aggregate and correlate streaming \ndata from multiple sources in real-time. A competent data presentation should be \ndesigned to reflect the structure, diversity and hierarchy of the streaming data.\n\nLoad balancing\n\nA big data stream computing system is expected to be self-adaptive to data streams \nchanges and avoid load shedding. This is challenging as dedicating resources to cover \npeak loads 24/7 is impossible and load shedding is not feasible when the variance \nbetween the average load and the peak load is high. As a result, a distributing envi-\nronment that automatically streams partial data streams to a global centre when local \nresources become insufficient is required.\n\nHigh throughput\n\nDecision with respect to identifying the sub-graph that needs replication, how many \nreplicas are needed and the portion of the data stream to assign to each replica is an \nissue in big data stream computing environment. There is a need for good multiple \ninstances replication if high throughput is to be achieved.\n\nPrivacy\n\nBig data stream analytics created opportunities for analyzing a huge amount of data \nin real-time but also created a big threat to individual privacy. According to the Inter-\nnational Data Cooperation (IDC), not more than half of the entire information that \nneeds protection is effectively protected. The main challenge is proposing techniques \nfor protecting a big data stream dataset before its analysis.\n\nAccuracy\n\nOne of the main objectives of big data stream analysis is to develop effective tech-\nniques that can accurately predict future observations. However, as a result of inher-\nent characteristics of big data such as volume, velocity, variety, variability, veracity, \nvolatility, and value, big data analysis strongly constrain processing algorithms spatio-\ntemporally and hence stream-specific requirements must be taken into consideration \nto ensure high accuracy.\n\nRelated work\n\nThis section discusses some of the previous research efforts that relate to big data \nstreaming analytics.\n\nThe work of [13] presented a review of various tools, technologies and methods \nfor big data analytics by categorizing big data analytics literature according to their \nresearch focus. This paper is different in that it presents a systematic literature review \nthat focused on big data “streaming” analytics.\n\n\n\nPage 6 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAuthors in [19] presented a systematic review of big data analytics in e-commerce. The \nstudy explored characteristics, definitions, business values, types and challenges of big \ndata analytics in the e-commerce landscape. Likewise, [20] conducted a study that is cen-\ntred on big data analytics in technology and organisational resource management specifi-\ncally focusing on reviews that present big data challenges and big data analytics methods. \nAlthough they are systematic reviews, the focus is not, particularly on big data streaming.\n\nAuthors in [21] presented the status of empirical research and application areas in big \ndata by employing a systematic mapping method. In the same vein, authors in [22] also \nconducted a survey on big data technologies and machine learning algorithms with a \nparticular focus on anomaly detection. A systematic review of literature which aims to \ndetermine the scope, application, and challenges of big data analytics in healthcare was \npresented by [23]. The work of [2] presented a review of four big data streaming tools \nand technologies. While the study conducted in this paper provided a comprehensive \nreview of not only big data streaming tools and technologies but also methods and tech-\nniques employed in analyzing big data streams. In addition, authors [2] did not provide a \nclear explanation of the methodical approach for selecting the reviewed papers.\n\nResearch method\nThe study was grounded in a systematic literature review of tools and technologies \nwith methods and techniques used in analysing big data streams by adopting [24, 25] as \nmodels.\n\nResearch question\n\nThe study tries to answer the following research questions:\n\nResearch Question 1: What are the tools and technologies employed for big data \nstream analysis?\nResearch Question 2: What methods and techniques are used in analysing big data \nstreams?\nResearch Question 3: What do these tools and technologies have in common and \ntheir differences in terms of concept, purpose and capabilities?\nResearch Question 4: What are the limitations and strengths of these tools and tech-\nnologies?\nResearch Question 5: What are the evaluation techniques or benchmarks used for \nevaluating big data streaming tools and technology?\n\nSearch string\n\nCreating a good search string requires structuring in terms of population, compari-\nson, intervention and outcome [24]. Relevant publications were identified by forming \na search string that combined keywords driven by the research questions earlier stated. \nThe searches were conducted by employing three standard database indexes, which are \nScopus, Science Direct and EBSCOhost. The search string is “big data stream analysis” \nOR “big data stream technologies” OR “big data stream framework” OR “big data stream \nalgorithms” OR “big data stream analysis tools” OR “big data stream processing” OR “big \n\n\n\nPage 7 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndata stream analysis reviews” OR “big data stream literature review” OR “big data stream \nanalytics”.\n\nData sources\n\nAs research becomes increasingly interdisciplinary, global and collaborative, it is expedi-\nent to select from rich and standard databases. The databases consulted are as follows:\n\n i. Scopus1: Scopus is a bibliographic database containing abstracts and citations for \nacademic journal articles launched in 2004. It covers nearly 36,377 titles from over \n11,678 publishers of which 34,346 are peer-reviewed journals, delivering a compre-\nhensive overview of the world’s research output in the scientific, technical, medi-\ncal, and social sciences (including arts and humanities). It is the largest abstract \nand citation database of peer-reviewed literature.\n\n ii. ScienceDirect2: ScienceDirect is Elsevier’s leading information solution for \nresearchers, students, teachers, information professionals and healthcare profes-\nsionals. It provides both subscription-based and open access-based to a large data-\nbase combining authoritative, full-text scientific, technical and health publications \nwith smart intuitive functionality. It covers over 14 million publications from over \n3800 journals and more than 35,000 books. The journals are grouped into four \ncategories: Life Sciences, Physical Sciences and Engineering, Health Sciences, and \nSocial Sciences and Humanities.\n\n iii. EBSCOhost3: EBSCOhost covers a wide range of bibliographic and full-text data-\nbases for researchers, providing electronic journal service available to both cor-\nporate and academic researchers. It has a total of 16,711 journals and magazine \nindexed and abstracted of which 14,914 are peer-reviewed; more than 900,000 \nhigh-quality e-books and titles and over 60,000 audiobooks from more than 1500 \nmajor academic publishers.\n\n iv. ResearchGate4: A free online professional network for scientists and researchers to \nask and answer questions, share papers and find collaborators. It covers over 100 \nmillion publications from over 11 million researchers. ResearchGate was used as \na secondary source where the authors could not access some papers due to lack of \nsubscription.\n\nData retrieval\n\nThe search was conducted in Scopus, ScienceDirect and EBSCOhost since most of \nthe high impact journals and conferences are indexed in these set of rich databases. \nBoolean ‘OR’ was used in combining the nine (9) search strings. A total of 2295 arti-\ncles from the three databases were retrieved as shown in Table 2.\n\n1 http://www.scopu s.com.\n2 http://www.scien cedir ect.com.\n3 https ://www.ebsco host.com.\n4 https ://www.resea archg ate.net.\n\nhttp://www.scopus.com\nhttp://www.sciencedirect.com\nhttps://www.ebscohost.com\nhttps://www.reseaarchgate.net\n\n\nPage 8 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFurther refinement was performed by (i) limiting the search to journals and confer-\nence papers; (ii) selecting computer science and IT related as the subject domain; (iii) \nselecting ACM, IEEE, SpringerLink, Elsevier as sources; and year of publication to \nbetween 2004 and 2018. The year range was selected due to the fact that interest in \nbig data stream analysis actually started in 2004. At this stage, a total of 1989 papers \nwere excluded leaving a total of 315 papers (see Table  3). The result of the search \nstring was exported to PDF.\n\nBy going through the title of the papers, 111 seemingly relevant papers were extracted \nexcluding a total number of 213 that were not relevant at this stage (see Table 4).\n\nThe abstracts of 111 papers and introduction (for papers that the abstracts were not \nclear enough) were then read to have a quick overview of the paper and to ascertain \nwhether they are suitable or at variance with the research questions. The citations of \nthe papers were exported to Microsoft Excel for easy analysis. The papers were grouped \ninto three categories; “relevant”, “may be relevant” and “irrelevant”. The “relevant” papers \nwere marked with black colour, “may be relevant” and “irrelevant” with green and red \ncolours respectively. At the end of this stage, 45 papers were classified as “relevant”, 9 \npapers as “may be relevant” and 11 as “irrelevant”. Looking critically at the abstract again, \n18 papers were excluded by using the exclusion criteria leaving a total of 47 papers (see \nTable 5) which were manually reviewed in line with the research questions.\n\nInclusion criteria\n\nPapers published in journals, peer-reviewed conferences, workshops, technical and \nsymposium from 2004 and 2018 were included. In addition, the most recent papers \nwere selected in case of papers with similar investigations and results.\n\nTable 2 First search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 2097 65 133 2295\n\nTable 3 Second search string result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 196 27 92 315\n\nTable 4 Third Search string refinement result\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 64 23 24 111\n\nTable 5 Final Selection\n\nScopus ScienceDirect EBSCOhost Total\n\nNumber of papers 25 10 12 47\n\n\n\nPage 9 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nExclusion criteria\n\nPapers that belong to the following categories were excluded from selection as part of \nthe primary study: (i) papers written in source language other than English; (ii) papers \nwith an abstract and or introduction that does not clearly define the contributions of the \nwork; (iii) papers whose abstract do not relate to big data stream analysis.\n\nResult\nThe findings of the study are now presented with respect to the research questions that \nguided the execution of the systematic literature review.\n\nResearch Question 1: What are the tools and technologies employed for big data stream \n\nanalysis?\n\nBig data stream platforms provide functionalities and features that enable big data \nstream applications to develop, operate, deploy, and manage big data streams. Such \nplatforms must be able to pull in streams of data, process the data and stream it back \nas a single flow. Several tools and technologies have been employed to analyse big data \nstreams. In response to the growing demand for big data streaming analytics, a large \nnumber of alternative big data streaming solutions have been developed both by the \nopen source community and enterprise technology vendors. According to [26], there are \nsome factors to consider when selecting big data streaming tools and technologies in \norder to make effective data management decisions. These are briefly described below.\n\nShape of the data\n\nStreaming data sources require serialization technologies for capturing, storing and rep-\nresenting such high-velocity data. For instance, some tools and technologies allow pro-\njection of different structures across data stores, giving room for flexibility for storage \nand access of data in different ways. However, the performance of such platforms may \nnot be suitable for high-velocity data.\n\nData access\n\nThere is a need to put into consideration how the data will be accessed by users and \napplications. For instance, many NoSQL databases require specific application interfaces \nfor data access. Hence there is a need to consider the integration of some other neces-\nsary tools for data access.\n\nAvailability and consistency requirement\n\nIf a distributed system is needed, then CAP theorem states that consistency and avail-\nability cannot be both guaranteed in the presence of network partition (i.e. when there is \na break in the network). In such a scenario, consistency is often traded off for availability \nto ensure that requests can always be processed.\n\nWorkload profile required\n\nPlatform as a service deployment may be appropriate for a spike load profile platform. \nIf platform distribution can be deployed on Infrastructure as a service cloud, then this \noption may be preferred as users will need to pay only when processing. On-premise \n\n\n\nPage 10 of 30Kolajo et al. J Big Data (2019) 6:47 \n\ndeployment may be considered for predictable or consistent loads. But if workloads are \nmixed (i.e. consistent flows or spikes), a combination of cloud and on-premise approach \nmay be considered so as to give room for easy integration of web-based services or soft-\nware and access to critical functions on the go.\n\nLatency requirement\n\nIf a minimal delay or low latency is required, key-value stores may be considered or bet-\nter still, an in-memory solution which allows the process of large datasets in real-time is \nrequired in order to optimize the data loading procedure.\n\nThe tools and technologies for big data stream analysis can be broadly categorized into \ntwo, which are open source and proprietary solutions. These are listed in Tables 6 and 7.\n\nThe selection of big data streaming tools and technologies should be based on the impor-\ntance of each factor earlier mentioned in this section. Proprietary solutions may not be eas-\nily available because of pricing and licensing issues. While open source supports innovation \nand development at a large scale, careful selection must be made especially when choosing \na recent technology still in production due to limited maturity and lack of support from \nacademic researchers or developer communities. In addition, open source solutions may \nlead to outdating and modification challenges [27]. Moreover, the selection of whether pro-\nprietary or open source or combination of both should depend on the problem to address, \nthe understanding of the true costs, and benefits of both open and proprietary solutions.\n\nTable 6 Open source tools and technologies for big data stream analysis\n\nTools and technology Article\n\nBlockMon [83]\n\nNoSQL [4, 84–86]\n\nSpark streaming [67, 87–91]\n\nApache storm [68, 85, 86, 92–97]\n\nKafka [85, 91, 95, 96, 98]\n\nYahoo! S4 [6, 45, 87, 99]\n\nApache Samza [46, 67, 100]\n\nPhoton [67, 101]\n\nApache Aurora [67, 102]\n\nMavEStream [103]\n\nEsperTech [104, 105]\n\nRedis [106]\n\nC-SPARQL [107, 108]\n\nSAMOA [56, 78, 109]\n\nCQELS [108, 110, 111]\n\nETALIS [112]\n\nXSEQ [73]\n\nApache Kylin [113]\n\nSplunk stream [114]\n\n\n\nPage 11 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nResearch Question 2: What methods and techniques are used in analysing big data \n\nstreams?\n\nGiven the real-time nature, velocity and volume of social media streams, the clus-\ntering algorithms that are applied on streaming data must be highly scalable and \nefficient. Also, the dynamic nature of data makes it difficult to know the required or \ndesirable number of clusters in advance. This renders partitioning clustering tech-\nniques (such as k-median, k-means and k-medoid) or expectation-maximization \n(EM) algorithms-based approaches unsuitable for analysing real-time social media \ndata because they require prior knowledge of clusters in advance. In addition, due \nto concept drift inherent in social media streams, scalable graph partitioning algo-\nrithms are not also suitable because of their tendency towards balanced partitioning. \nSocial media streams must be analysed dynamically in order to provide decisions at \nany given time within a limited space and time window [28–30].\n\nDensity-based clustering algorithm (such as DenStream, OpticStream, Flock-\nStream, Exclusive and Complete Clustering) unlike partitioning algorithms does not \nrequire apriori number of clusters in advance and can detect outliers [31]. However, \nthe issue with density-based clustering algorithms is that most of them except for few \nlike HDDStream, PreDeCon-Stream and PKS-Stream (which are memory intensive) \nperform less efficiently in the face of high dimensional data and as a result are not \nsuitable for analyzing social media streams [32].\n\nThreshold-based techniques, hierarchical clustering, and incremental clustering \nor online clustering are more relevant to social media analysis. Several online thresh-\nold-based stream clustering approaches or incremental clustering approaches such as \nMarkov Random Field [33, 34], Online Spherical K-means [35], and Condensed Clusters \n[36] have been adopted. Incremental approaches are suitable for continuously generated \ndata grouping by setting a maximum similarity threshold between the incoming stream \n\nTable 7 Proprietary tools and technologies for big data stream analysis\n\nTools and technology Article\n\nCodeBlue [115]\n\nAnodot [116]\n\nCloudet [117]\n\nSentiment brand monitoring [118]\n\nNumenta [119]\n\nElastic streaming processing engine [120]\n\nMicrosoft azure stream analytics [121]\n\nIBM InfoSphere streams [8, 122]\n\nGoogle MillWheel [123]\n\nArtemis [124]\n\nWSO2 analytics [125]\n\nMicrosoft StreamInsight [126]\n\nTIBCO StreamBase [127]\n\nStriim [128]\n\nKyvos insights [129]\n\nAtScale [130, 131]\n\nLambda architecture [57]\n\n\n\nPage 12 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nand the existing clusters. Much work has been done in improving the efficiency of online \nclustering algorithms, however, little research efforts have been directed to threshold \nand fragmentation issues. Incremental algorithm threshold setting should employ adap-\ntive approach instead of relying on static values [37, 38]. Some of the methods and tech-\nniques that have been employed in analysing big data streams are outlined in Table 8.\n\nTable 8 Methods and techniques for big data stream analysis\n\nMethods and techniques Article\n\nSPADE [132]\n\nLocally supervised metric learning (LSML) [133]\n\nKTS [106]\n\nMultinomial latent dirichlet allocation [106]\n\nVoltage clustering algorithm [106]\n\nLocality sensitive hashing (LSH) [134]\n\nUser profile vector update algorithm [134]\n\nTag assignment stream clustering (TASC) [134]\n\nStreamMap [117]\n\nDensity cognition [117]\n\nQRS detection algorithm [87]\n\nForward chaining rule [110]\n\nStream [135]\n\nCluStream [136, 137]\n\nHPClustering [138]\n\nDenStream [139]\n\nD-Stream [140]\n\nACluStream [141]\n\nDCStream [142]\n\nP-Stream [143]\n\nADStream [144]\n\nContinuous query processing (CQR) [145]\n\nFPSPAN-growth [146]\n\nOutlier method for cloud computing algorithm (OMCA) [147]\n\nMulti-query optimization strategy (MQOS) [148]\n\nParallel K-means clustering [72]\n\nVisibly push down automata (VPA) [73]\n\nIncremental MI outlier detection algorithm (Inc I-MLOF) [149]\n\nAdaptive windowing based online ensemble (AWOE) [74]\n\nDynamic prime-number based security verification [84]\n\nK-anonymity, I-diversity, t-closeness [90]\n\nSingular spectrum matrix completion (SS-MC) [76]\n\nTemporal fuzzy concept analysis [96]\n\nECM-sketch [77]\n\nNearest neighbour [91]\n\nMarkov chains [91]\n\nBlock-QuickSort-AdjacentJobMatch [86]\n\nBlock-QuickSort-OverlapReplicate [86]\n\nFuzzy-CSar-AFP [150]\n\nWeighted online sequential extreme learning machine with kernels (WOS-ELMK) [22]\n\nConcept-adapting very fast decision tree (CVFDT) [151]\n\n\n\nPage 13 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nMany researchers have looked at the aspect of the real-time analysis of big data \nstreams but not much attention has been directed towards social media stream pre-\nprocessing. For instance, the social media stream is characterized by incomplete, noisy, \nslang, abbreviated words. Also, contextual meaning of social media post is essential for \nimproved event detection, sentiment analysis or any other social media analytics algo-\nrithms in terms of quality and accuracy [36, 39]. There is the need to give more atten-\ntion to the preprocessing stage of social media stream analysis in the face of incomplete, \nnoisy, slang, and abbreviated words that are pertinent to social media streams. These \nchallenges create opportunities application of new semantic technology approaches, \nwhich are more suited to social media streams [40, 41].\n\nResearch Question 3: What do big data streaming tools and technologies have in common \n\nand their differences in terms of concept, purpose, and capabilities?\n\nThe features of various tools and technologies for big data stream were compared in \norder to answer this question. An overview analysis based on 10 dimensions, which are \ndatabase support, execution model, workload, fault-tolerance, latency, throughput, reli-\nability, operating system, implementation languages and application domain or areas is \npresented in Table 9.\n\nFor organisations with existing applications that have support for SQL, MySQL, SQL \nServer, Oracle Database, for instance, may consider choosing big data streaming tools \nand technologies that have support for their existing databases. There are few big data \nstreaming tools and technology that support virtually any data format. An example of \nsuch is Infochimps Cloud.\n\nThe major big data streaming tools and technologies considered are all suitable for \nstreaming execution model, however out of 19 big data tools and technology compared \nand contrasted in this section, only 10.5% is suitable for streaming, batch, and iterative \nprocessing while 47.4% can handle jobs requiring both batch and streaming processing. \nIt is safer for a job to be executed on a single platform which can accommodate all the \ndependencies required in order to avoid interoperability constraints than combining \ntwo or more platforms or frameworks. The best fit with respect to the choice of big data \nstreaming tools and technologies will depend on the state of data to process, infrastruc-\nture preference, business use case, and kind of results interested in.\n\nVirtually all the big data streaming tools and technologies are memory intensive. This \nimplies that the main performance bottleneck at higher load conditions will be due to \nlack of memory [42]. However, research has shown that the benefit of high intensive \nmemory applications outweighs the performance loss due to long memory latency [43].\n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once” “at-least-once” \nand “exactly-once” message delivery mechanisms while others support one or two of the \nthree delivery mechanisms. “At-most-once” is the cheapest with least implementation \noverhead and highest performance because it can be done in a fire-and-forget fashion \nwithout keeping the state in the transport mechanism or at the sending end. “At-least-\nonce” delivery requires multiple attempts in order to counter transport losses which \nmeans keeping the state at the sending end and having an acknowledgement mechanism \nat the receiving end. “Exactly-once” is the most expensive and has consequently worst \n\n\n\nPage 14 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\nCo\nm\n\npa\nri\n\nso\nn \n\nof\n b\n\nig\n d\n\nat\na \n\nst\nre\n\nam\nin\n\ng \nto\n\nol\ns \n\nan\nd \n\nte\nch\n\nno\nlo\n\ngi\nes\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nBl\noc\n\nkM\non\n\nCa\nss\n\nan\ndr\n\na,\n M\n\non\n-\n\ngo\nD\n\nB,\n X\n\nM\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nul\nti-\n\nsl\nic\n\ne \nm\n\nem\n-\n\nor\ny \n\nal\nlo\n\nca\ntio\n\nn \nan\n\nd \nba\n\ntc\nh \n\nal\nlo\n\nca\ntio\n\nns\n\nC\nhe\n\nck\npo\n\nin\nt, \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nC\n\n +\n+\n\n11\n, P\n\nyt\nho\n\nn\nA\n\nno\nm\n\nal\ny \n\nde\nte\n\nct\nio\n\nn,\n \n\nne\ntw\n\nor\nk \n\nop\ntim\n\niz\na-\n\ntio\nn,\n\n m\nul\n\ntim\ned\n\nia\n \n\nco\nnt\n\nen\nt d\n\nel\niv\n\ner\ny,\n\n \nfin\n\nan\nci\n\nal\n m\n\nar\nke\n\nt \nan\n\nal\nys\n\nis\n, w\n\neb\n \n\nan\nal\n\nyt\nic\n\ns\n\nSp\nar\n\nk \nSt\n\nre\nam\n\nin\ng\n\nKa\nfk\n\na,\n H\n\nBa\nse\n\n, \nH\n\niv\ne \n\nFl\num\n\ne,\n \n\nH\nD\n\nF/\nS3\n\n, \nKi\n\nne\nsi\n\ns, \nTC\n\nP \nso\n\nck\net\n\ns, \nTw\n\nit-\nte\n\nr, \nSQ\n\nL\n\nBa\ntc\n\nh,\n It\n\ner\nat\n\niv\ne,\n\n \nSt\n\nre\nam\n\nin\ng\n\nC\nPU\n\n/m\nem\n\nor\ny \n\nin\nte\n\nns\niv\n\ne\nRD\n\nD\n b\n\nas\ned\n\n \nC\n\nhe\nck\n\n-p\noi\n\nnt\n-\n\nin\ng,\n\n p\nar\n\nal\nle\n\nl \nre\n\nco\nve\n\nry\n, \n\nre\npl\n\nic\nat\n\nio\nn\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nSc\n\nal\na,\n\n P\nyt\n\nho\nn,\n\n \nJa\n\nva\n, R\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nst\nre\n\nam\nin\n\ng \nm\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\n, f\nog\n\n c\nom\n\n-\npu\n\ntin\ng,\n\n in\nte\n\nra\nct\n\niv\ne \n\nan\nal\n\nys\nis\n\n, m\nul\n\ntim\ne-\n\ndi\na \n\nan\nal\n\nys\nis\n\n, c\nlu\n\nst\ner\n\n \nan\n\nal\nys\n\nis\n, fi\n\nlte\nrin\n\ng,\n \n\nre\n-p\n\nro\nce\n\nss\nin\n\ng,\n \n\nca\nch\n\ne \nin\n\nva\nlid\n\nat\nio\n\nn\n\nA\npa\n\nch\ne \n\nSt\nor\n\nm\nSp\n\nou\nt, \n\nH\nBa\n\nse\n, \n\nH\niv\n\ne,\n S\n\nQ\nL,\n\n \nCa\n\nss\nan\n\ndr\na,\n\n \nM\n\nem\nca\n\nch\ned\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n, \n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\n, \n\nre\nco\n\nrd\n-le\n\nve\nl \n\nac\nkn\n\now\nle\n\ndg\ne-\n\nm\nen\n\nt, \nst\n\nat\nel\n\nes\ns \n\nm\nan\n\nag\nem\n\nen\nt\n\nVe\nry\n\n lo\nw\n\nLo\nw\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nm\nac\n\nO\nS,\n\n L\nin\n\nux\nC\n\nlo\nju\n\nre\n, J\n\nav\na,\n\n S\nca\n\nla\n, \n\nC\nlo\n\nju\nre\n\n, n\non\n\n-J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nIn\nte\n\nrn\net\n\n o\nf t\n\nhi\nng\n\ns, \nst\n\nre\nam\n\nin\ng \n\nm\nac\n\nhi\nne\n\n \nle\n\nar\nni\n\nng\n, m\n\nul\ntim\n\ne-\ndi\n\na \nan\n\nal\nys\n\nis\n\nYa\nho\n\no!\n S\n\n4\nM\n\nyS\nQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nRi\nch\n\n D\nat\n\na \nFo\n\nrm\nat\n\nSt\nre\n\nam\nin\n\ng\nC\n\nPU\n/m\n\nem\nor\n\ny \nin\n\nte\nns\n\niv\ne\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nLo\nw\n\nLo\nw\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\n, P\n\nyt\nho\n\nn,\n C\n+\n+\n\n, \nPe\n\nrl\nO\n\nnl\nin\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nm\non\n\nito\nrin\n\ng,\n fr\n\nau\nd \n\nde\nte\n\nct\nio\n\nn,\n fi\n\nna\nnc\n\nia\nl \n\nda\nta\n\n p\nro\n\nce\nss\n\nin\ng,\n\n \nw\n\neb\n p\n\ner\nso\n\nna\nliz\n\na-\ntio\n\nn \nan\n\nd \nse\n\nss\nio\n\nn \nm\n\nod\nel\n\nlin\ng\n\n\n\nPage 15 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nSa\nm\n\nza\nKa\n\nfk\na,\n\n H\nD\n\nFS\n, \n\nKi\nne\n\nsi\ns, \n\nSt\nre\n\nam\n \n\nco\nns\n\num\ner\n\n, K\ney\n\n-\nva\n\nlu\ne \n\nst\nor\n\nes\n\nSt\nre\n\nam\nin\n\ng,\n b\n\nat\nch\n\n \npr\n\noc\nes\n\nsi\nng\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nC\n\nhe\nck\n\npo\nin\n\nt\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nA\nt l\n\nea\nst\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n W\nin\n\ndo\nw\n\ns\nJa\n\nva\n, S\n\nca\nla\n\n, J\nVM\n\n \nla\n\nng\nua\n\nge\ns\n\nFi\nlte\n\nrin\ng,\n\n re\n-p\n\nro\n-\n\nce\nss\n\nin\ng,\n\n c\nac\n\nhe\n \n\nin\nva\n\nlid\nat\n\nio\nn\n\nA\npa\n\nch\ne \n\nFl\nin\n\nk\nKa\n\nfk\na,\n\n F\nlu\n\nm\ne,\n\n \nH\n\nD\nF/\n\nS3\n, \n\nKi\nne\n\nsi\ns, \n\nTC\nP \n\nso\nck\n\net\ns, \n\nTw\nit-\n\nte\nr, \n\nCa\nss\n\nan\ndr\n\na,\n \n\nRe\ndi\n\ns, \nM\n\non\n-\n\ngo\nD\n\nB,\n H\n\nBa\nse\n\n, \nSQ\n\nL\n\nSt\nre\n\nam\nin\n\ng,\n \n\nba\ntc\n\nh,\n it\n\ner\nat\n\niv\ne,\n\n \nin\n\nte\nra\n\nct\niv\n\ne\n\nM\nem\n\nor\ny \n\nin\nte\n\nn-\nsi\n\nve\nSt\n\nre\nam\n\n re\npl\n\nay\n \n\nan\nd \n\nm\nar\n\nke\nr-\n\nch\nec\n\nkp\noi\n\nnt\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nW\n\nin\ndo\n\nw\ns\n\nJa\nva\n\n, S\nca\n\nla\n, P\n\nyt\nho\n\nn\nO\n\npt\nim\n\niz\nat\n\nio\nn \n\nof\n \n\ne-\nco\n\nm\nm\n\ner\nce\n\n \nse\n\nar\nch\n\n re\nsu\n\nlt,\n \n\nne\ntw\n\nor\nk/\n\nse\nns\n\nor\n \n\nm\non\n\nito\nrin\n\ng \nan\n\nd \ner\n\nro\nr d\n\net\nec\n\ntio\nn,\n\n \nET\n\nL \nfo\n\nr b\nus\n\nin\nes\n\ns \nin\n\nte\nlli\n\nge\nnc\n\ne \nin\n\nfra\n-\n\nst\nru\n\nct\nur\n\ne,\n m\n\nac\nhi\n\nne\n \n\nle\nar\n\nni\nng\n\nA\npa\n\nch\ne \n\nA\nur\n\nor\na\n\nH\n2,\n\n Ja\nva\n\n m\nap\n\ns, \nM\n\nyB\nat\n\nis\n, \n\nM\nyS\n\nQ\nL,\n\n P\nos\n\nt-\ngr\n\neS\nQ\n\nL\n\nSt\nre\n\nam\nin\n\ng\nM\n\nem\nor\n\ny \nan\n\nd \ndi\n\nsk\n s\n\npa\nce\n\nPe\nrio\n\ndi\nc \n\nre\nco\n\nv-\ner\n\ny \nch\n\nec\nkp\n\noi\nnt\n\n \nan\n\nd \nro\n\nllb\nac\n\nk\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nLi\nnu\n\nx\nPy\n\nth\non\n\nM\non\n\nito\nrin\n\ng \nap\n\npl\nic\n\na-\ntio\n\nns\n s\n\nuc\nh \n\nas\n \n\nfin\nan\n\nci\nal\n\n a\nna\n\nly\nsi\n\ns \nan\n\nd \nm\n\nili\nta\n\nry\n a\n\npp\nli-\n\nca\ntio\n\nns\n\nRe\ndi\n\ns\nKe\n\ny-\nva\n\nlu\ne \n\nst\nor\n\nes\n, \n\nra\nbi\n\ntm\nq,\n\n M\non\n\n-\ngo\n\nD\nB\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nbu\nt \n\npe\nrs\n\nis\nte\n\nnt\n o\n\nn-\ndi\n\nsk\n d\n\nat\nab\n\nas\ne\n\nRe\npl\n\nic\na \n\nm\nig\n\nra\n-\n\ntio\nn,\n\n S\nen\n\ntin\nel\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nU\nbu\n\nnt\nu,\n\n L\nin\n\nux\n, \n\nO\nSX\n\nC\n, C\n\n#,\n Ja\n\nva\n, P\n\nH\nP, \n\nPy\nth\n\non\nW\n\neb\n a\n\nna\nly\n\nsi\ns, \n\nca\nch\n\ne,\n \n\nm\nes\n\nsa\nge\n\n q\nue\n\nue\ns\n\nC\n-S\n\nPA\nRQ\n\nL\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nLo\nw\n\n m\nem\n\nor\ny \n\nus\nag\n\ne\nA\n\nda\npt\n\nat\nio\n\nn\nVe\n\nry\n lo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nJe\nna\n\n \nlib\n\nra\nrie\n\ns\nRe\n\nal\n-t\n\nim\ne \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n s\n\nen\nso\n\nr d\nat\n\na,\n \n\nso\nci\n\nal\n s\n\nem\nan\n\ntic\n \n\nda\nta\n\n, u\nrb\n\nan\n c\n\nom\n-\n\npu\ntin\n\ng\n\nSA\nM\n\nO\nA\n\nH\nBa\n\nse\n, H\n\niv\ne,\n\n C\nas\n\n-\nsa\n\nnd\nra\n\nSt\nre\n\nam\nin\n\ng\nLo\n\nw\n m\n\nem\nor\n\ny \nus\n\nag\ne\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nC\n\nla\nss\n\nifi\nca\n\ntio\nn,\n\n c\nlu\n\nst\ner\n\n-\nin\n\ng,\n s\n\npa\nm\n\n d\net\n\nec\n-\n\ntio\nn,\n\n re\ngr\n\nes\nsi\n\non\n, \n\nfre\nqu\n\nen\nt p\n\nat\nte\n\nrn\n \n\nm\nin\n\nin\ng\n\n\n\nPage 16 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nCQ\nEL\n\nS\nRD\n\nF, \nSQ\n\nLJ\n, \n\nN\noS\n\nQ\nL,\n\n H\nD\n\nF\nBa\n\ntc\nh,\n\n s\ntr\n\nea\nm\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nH\n\nig\nh\n\nCu\nm\n\nul\nat\n\niv\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx,\n\n M\nac\n\nO\nS,\n\n \nA\n\nnd\nro\n\nid\n\nJa\nva\n\nRe\nal\n\n-t\nim\n\ne \nre\n\nas\non\n\nin\ng \n\nov\ner\n\n s\nen\n\nso\nr d\n\nat\na,\n\n \nso\n\nci\nal\n\n s\nem\n\nan\ntic\n\n \nda\n\nta\n, u\n\nrb\nan\n\n c\nom\n\n-\npu\n\ntin\ng\n\nET\nA\n\nLI\nS\n\nRD\nF\n\nSt\nre\n\nam\nin\n\ng\nBi\n\nna\nriz\n\nat\nio\n\nn\nA\n\nda\npt\n\nat\nio\n\nn\nLo\n\nw\nLo\n\nw\nCu\n\nm\nul\n\nat\niv\n\ne\nW\n\nin\ndo\n\nw\ns, \n\nLi\nnu\n\nx,\n M\n\nac\nO\n\nS,\n \n\nA\nnd\n\nro\nid\n\nPr\nol\n\nog\n, J\n\nav\na,\n\n C\n, \n\nSP\nA\n\nRQ\nL,\n\n C\n#,\n\n \nET\n\nA\nLI\n\nS \nLa\n\nng\nua\n\nge\n \n\nfo\nr E\n\nve\nnt\n\ns \n(E\n\nLE\n)\n\nEv\nen\n\nt d\net\n\nec\ntio\n\nn,\n \n\nre\nas\n\non\nin\n\ng \nov\n\ner\n \n\nst\nre\n\nam\nin\n\ng \nev\n\nen\nts\n\nXS\nEQ\n\nXM\nL\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny \n\nw\nith\n\n \nbu\n\nffe\nrin\n\ng\nch\n\nec\nkp\n\noi\nnt\n\nLo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne\n\nW\nin\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, A\npa\n\nch\ne \n\nXe\nrc\n\nes\nBi\n\nol\nog\n\nic\nal\n\n d\nat\n\na,\n s\n\noc\nia\n\nl \nne\n\ntw\nor\n\nks\n, u\n\nse\nr \n\nbe\nha\n\nvi\nou\n\nr, \nfin\n\nan\nci\n\nal\n \n\nda\nta\n\n a\nna\n\nly\nsi\n\ns, \nfil\n\nte\nrin\n\ng\n\nIB\nM\n\n In\nfo\n\nSp\nhe\n\nre\n \n\nst\nre\n\nam\ns\n\nPi\ng,\n\n H\niv\n\ne,\n Ja\n\nql\n, \n\nH\nBa\n\nse\n F\n\nlu\nm\n\ne,\n \n\nLu\nce\n\nne\n, A\n\nvr\no,\n\n \nZo\n\noK\nee\n\npe\nr, \n\nO\noz\n\nie\n, O\n\nra\ncl\n\ne \nD\n\nat\nab\n\nas\ne,\n\n \nD\n\nB2\n, N\n\net\nez\n\nza\n, \n\nM\nyS\n\nQ\nL,\n\n A\nst\n\ner\n, \n\nIn\nfo\n\nrm\nix\n\n.\n\nSt\nre\n\nam\nin\n\ng\nCa\n\npt\nur\n\ne \nda\n\nta\nba\n\nse\n \n\nw\nor\n\nkl\noa\n\nds\n a\n\nnd\n \n\nre\npl\n\nay\n th\n\nem\n in\n\n \na \n\nte\nst\n\n d\nat\n\nab\nas\n\ne \nen\n\nvi\nro\n\nnm\nen\n\nt\n\nA\nut\n\nom\nat\n\nic\n \n\nre\nco\n\nve\nry\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne,\n \n\nA\nt l\n\nea\nst\n\n \non\n\nce\n, A\n\nt \nm\n\nos\nt o\n\nnc\ne\n\nLi\nnu\n\nx,\n C\n\nen\ntO\n\nS\nC\n\n +\n+\n\nJa\nva\n\nSP\nL\n\nSp\nac\n\ne \nw\n\nea\nth\n\ner\n p\n\nre\n-\n\ndi\nct\n\nio\nn,\n\n p\nhy\n\nsi\nol\n\nog\ni-\n\nca\nl d\n\nat\na \n\nst\nre\n\nam\ns \n\nan\nal\n\nys\nis\n\n, t\nra\n\nffi\nc \n\nm\nan\n\nag\nem\n\nen\nt, \n\nre\nal\n\n-\ntim\n\ne \npr\n\ned\nic\n\ntio\nns\n\n, \nev\n\nen\nt d\n\net\nec\n\ntio\nn,\n\n \nvi\n\nsu\nal\n\nis\nat\n\nio\nn\n\nG\noo\n\ngl\ne \n\nM\nill\n\n-\nW\n\nhe\nel\n\nBi\ngT\n\nab\nle\n\n, S\npa\n\nn-\nne\n\nr\nSt\n\nre\nam\n\nin\ng\n\nIn\n-m\n\nem\nor\n\ny \nan\n\nd \nbl\n\noo\nm\n\n fi\nlte\n\nrin\ng\n\nU\nnc\n\noo\nrd\n\nin\nat\n\ned\n \n\npe\nrio\n\ndi\nc,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nup\n\nst\nre\n\nam\n \n\nba\nck\n\nup\n\nLo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nLi\n\nnu\nx\n\nVi\nrt\n\nua\nlly\n\n a\nny\n\n \npr\n\nog\nra\n\nm\nm\n\nin\ng \n\nla\nng\n\nua\nge\n\nA\nno\n\nm\nal\n\ny \nde\n\nte\nct\n\nio\nn,\n\n \nhe\n\nal\nth\n\n m\non\n\nito\nrin\n\ng,\n \n\nim\nag\n\ne \npr\n\noc\nes\n\nsi\nng\n\n, \nne\n\ntw\nor\n\nk \nsw\n\nitc\nh \n\nm\nan\n\nag\nem\n\nen\nt\n\n\n\nPage 17 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n9 \n\n(c\non\n\nti\nnu\n\ned\n)\n\nTo\nol\n\ns \nan\n\nd \nte\n\nch\nno\n\nlo\ngy\n\nD\nat\n\nab\nas\n\ne \nsu\n\npp\nor\n\nt\nEx\n\nec\nut\n\nio\nn \n\nm\nod\n\nel\nW\n\nor\nkl\n\noa\nd\n\nFa\nul\n\nt t\nol\n\ner\nan\n\nce\nLa\n\nte\nnc\n\ny\nTh\n\nro\nug\n\nhp\nut\n\nRe\nlia\n\nbi\nlit\n\ny\nO\n\npe\nra\n\ntin\ng \n\nsy\nst\n\nem\nIm\n\npl\nem\n\nen\nta\n\ntio\nn/\n\nsu\npp\n\nor\nte\n\nd \nla\n\nng\nua\n\nge\ns\n\nA\npp\n\nlic\nat\n\nio\nn\n\nIn\nfo\n\nch\nim\n\nps\n \n\ncl\nou\n\nd\nSQ\n\nL,\n N\n\noS\nQ\n\nL,\n \n\nH\niv\n\ne,\n P\n\nig\n \n\nW\nuk\n\non\ng,\n\n \nH\n\nad\noo\n\np,\n \n\nRD\nBM\n\nS,\n V\n\nirt\nu-\n\nal\nly\n\n a\nny\n\n d\nat\n\na \nfo\n\nrm\nat\n\nBa\ntc\n\nh,\n s\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nU\nps\n\ntr\nea\n\nm\n \n\nba\nck\n\nup\nLo\n\nw\nH\n\nig\nh\n\nEx\nac\n\ntly\n o\n\nnc\ne\n\nLi\nnu\n\nx\nJa\n\nva\nD\n\nis\nas\n\nte\nr d\n\nis\nco\n\nve\nry\n\n, \nte\n\nxt\n a\n\nna\nly\n\nsi\ns, \n\nco\nm\n\n-\npl\n\nex\n e\n\nve\nnt\n\n p\nro\n\nce\nss\n\n-\nin\n\ng,\n v\n\nis\nua\n\nlis\nat\n\nio\nn\n\nM\nic\n\nro\nso\n\nft\n \n\nSt\nre\n\nam\nIn\n\nsi\ngh\n\nt\nSQ\n\nL \nSe\n\nrv\ner\n\nSt\nre\n\nam\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nRe\npl\n\nic\nat\n\nio\nn,\n\n \nch\n\nec\nkp\n\noi\nnt\n\n, \nda\n\nta\n re\n\nco\nve\n\nry\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nEx\n\nac\ntly\n\n o\nnc\n\ne\nW\n\nin\ndo\n\nw\ns\n\n.N\nET\n\n, C\n#,\n\n L\nIN\n\nQ\n, R\n\nx\nM\n\nan\nuf\n\nac\ntu\n\nrin\ng \n\npr\noc\n\nes\ns \n\nm\non\n\nito\nr-\n\nin\ng \n\nan\nd \n\nco\nnt\n\nro\nl, \n\nfin\nan\n\nci\nal\n\n d\nat\n\na \nan\n\nal\nys\n\nis\n, o\n\npe\nra\n\n-\ntio\n\nn \nan\n\nal\nyt\n\nic\ns, \n\nw\neb\n\n \nan\n\nal\nyt\n\nic\ns, \n\nev\nen\n\nt \npa\n\ntt\ner\n\nn \nde\n\nte\nct\n\nio\nn\n\nTI\nBC\n\nO\n S\n\ntr\nea\n\nm\n-\n\nBa\nse\n\nO\nra\n\ncl\ne \n\nda\nta\n\nba\nse\n\n, \nSQ\n\nL \nSe\n\nrv\ner\n\n, \nIm\n\npa\nla\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny\n\nSy\nnc\n\nhr\non\n\niz\nat\n\nio\nn,\n\n \nre\n\npl\nic\n\nat\nio\n\nn,\n \n\nro\nllb\n\nac\nk\n\nVe\nry\n\n lo\nw\n\nH\nig\n\nh\nA\n\nt l\nea\n\nst\n o\n\nnc\ne/\n\nat\n m\n\nos\nt \n\non\nce\n\n/\nex\n\nac\ntly\n\n o\nnc\n\ne\n\nW\nin\n\ndo\nw\n\ns, \nM\n\nac\nO\n\nS,\n L\n\nin\nux\n\nR,\n Ja\n\nva\nM\n\nis\nsi\n\non\n c\n\nrit\nic\n\nal\n \n\nan\nal\n\nys\nis\n\n, I\noT\n\n a\nna\n\nly\n-\n\nsi\ns, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n \nan\n\nal\nys\n\nis\n, p\n\nre\ndi\n\nct\niv\n\ne \nan\n\nal\nyt\n\nic\ns, \n\nw\nor\n\nkfl\now\n\n \nop\n\ntim\niz\n\nat\nio\n\nn,\n ri\n\nsk\n \n\nav\noi\n\nda\nnc\n\ne\n\nLa\nm\n\nbd\na \n\nA\nrc\n\nhi\n-\n\nte\nct\n\nur\ne\n\nRD\nBM\n\nS,\n C\n\nas\nsa\n\nn-\ndr\n\na,\n K\n\naf\nka\n\n, D\nat\n\na \nW\n\nar\neh\n\nou\nse\n\ns, \nKi\n\nne\nsi\n\ns \nD\n\nat\na \n\nSt\nre\n\nam\n, H\n\nD\nFS\n\n, \nH\n\nBa\nse\n\nBa\ntc\n\nh,\n S\n\ntr\nea\n\nm\nin\n\ng\nIn\n\n-m\nem\n\nor\ny/\n\ndi\nsk\n\n \nda\n\nta\nba\n\nse\nRe\n\npl\nic\n\nat\nio\n\nn,\n \n\nch\nec\n\nkp\noi\n\nnt\nLo\n\nw\nLo\n\nw\nEx\n\nac\ntly\n\n o\nnc\n\ne\nU\n\nbu\nnt\n\nu,\n W\n\nin\n-\n\ndo\nw\n\ns, \nLi\n\nnu\nx\n\nJa\nva\n\n, C\n#,\n\n P\nyt\n\nho\nn,\n\n \nPi\n\ng \nLa\n\ntin\nIo\n\nT \nan\n\nal\nys\n\nis\n, t\n\nra\nck\n\nin\ng \n\nre\nal\n\n-t\nim\n\ne \nup\n\nda\nte\n\ns, \nfin\n\nan\nci\n\nal\n ri\n\nsk\n m\n\nan\n-\n\nag\nem\n\nen\nt, \n\ncl\nic\n\nk-\nst\n\nre\nam\n\n a\nna\n\nly\nsi\n\ns\n\n\n\nPage 18 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nperformance because, in addition to “at-least-once” delivery mechanism, it requires the \nstate to be kept at the receiving end in order to filter duplicate deliveries. In other words, \n“at-most-once” delivery mechanism implies that the message may be lost while “at-\nleast-once” delivery ensures that messages are not lost and “exactly-once” implies that \nmessage can neither be lost nor duplicated. “Exactly-once” is suitable for many critical \nsystems where duplicate messages are unacceptable.\n\nResearch Question 4: What are the limitations and strengths of big data streaming tools \n\nand technologies?\n\nObservations from the literature reveal that specific big data streaming technology may \nnot provide the full set of features that are required. It is rare to find specific big data \ntechnology that combines key features such as scalability, integration, fault-tolerance, \ntimeliness, consistency, heterogeneity and incompleteness management, and load bal-\nancing. For instance, Spark streaming [16] and Sonora [44] are excellent and efficient \nfor checkpointing but the operator space available to user codes are limited. S4 does not \nguarantee 100% fault-tolerant persistent state [45]. Storm does not guarantee the order-\ning of messages due to its “at-least-once” mechanism for record delivery [46, 47]. Strict \ntransaction ordering is required by Trident to operate [48]. While streaming SQL pro-\nvide simple and succinct solutions to many streaming problems, the complex application \nlogic (such as matrix multiplication) and intuitive state abstractions are expressed with \nthe operational flow of an imperative language rather than a declarative language such as \nSQL [49–51].\n\nMoreover, BlockMon uses batches and cache locality optimization techniques for \nmemory allocation efficiency and data speed up access. However, deadlock may occur \nif data streams are enqueued with a higher rate than that of the block consumption [52]. \nApache Samza solves batch latency processing problems but requires an added layer for \nflow control [53]. Flink is suitable for heavy stream processing and batch-oriented tasks \nalthough it has scaling limitations [46]. Redis’ in-memory data store makes it extremely \nfast although this implies that available memory size determines the size of the Redis \ndata store [54]. While C-SPARQL and CQELS are excellent for combining static and \nstreaming data, they are not suitable when scalability is required [55]. SAMOA is suit-\nable for machine learning paradigm as it focuses on speed/real-time analytics, scales \nhorizontally and is loosely coupled with its underlying distributed computation platform \n[56]. With Lambda architecture, a real-time layer can complement the batch processing \none thereby reducing maintenance overhead and risk for errors as a result of duplicate \ncode bases. In addition, Lambda architecture handles reprocessing, which is one of the \nkey challenges in stream processing. Two main problems with Lambda architecture are \ncode maintenance in two complex distributed systems that need to produce the same \nresult and high operational complexity [57, 58].\n\nSummarily, there exists various tools and technologies for implementing big data \nstreams and there seems to be no big data streaming tool and technology that offers all \nthe key features required for now. While each tool and technology may have its strengths \nand weaknesses, the choice depends on the objective of the research and data availa-\nbility. A decision in favour of the wrong technology may result in increased overhead \ncost and time. The decision should take into consideration empirical analysis along with \n\n\n\nPage 19 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nsystem requirements. In addition, research efforts should also be directed to how to \nimprove on existing big data streaming tools and technologies to provide key features \nsuch as scalability, integration, fault-tolerance, timeliness, consistency, heterogeneity \nand incompleteness management, and load balancing.\n\nResearch Question 5: What are the evaluation techniques or benchmarks that are used \n\nfor evaluating big data streaming tools and technologies?\n\nThe diversity of big data poses a challenge when it comes to developing big data bench-\nmarks that will be suitable for all workload cases. One cannot stick to one big data \nbenchmark because it has been observed that using only one benchmark on differ-\nent data sets do not give the same result. This implies that benchmark testing should \nbe application specific. Subsequently, in evaluating big data system, the identification \nof workload for an application domain is a prerequisite [59]. Most of the existing big \ndata benchmarks are designed to evaluate a specific type of systems or architectures. For \ninstance, HiBench [60] is suitable for benchmarking Hadoop, Spark and streaming work-\nloads, GridMix [61] and PigMix [62] are for MapReduce Hadoop systems. BigBench [63, \n64] is suitable for benchmarking Teradata Aster DBMS, MapReduce systems, Redshift \ndatabase, Hive, Spark and Impala. Presently, BigDataBench [65, 66] seems to be the only \nbig data benchmark that can evaluate a hybrid of different big data systems.\n\nSo far, many researchers have evaluated their work by making use of synthetic and \nreal-life data. Standard benchmark dataset for big data streaming analytics has not been \nwidely adopted. However, few of the researchers that used standardized benchmarking \nare briefly discussed below. The work of [67] was tested with two benchmarks; Word \nCount and Grep. The result showed that the proposed algorithm can effectively handle \nunstable input and the delay of the total event can be limited to an expected range.\n\nThe tool developed by [68] was tested on both car dataset and Wikinews5 dataset in \ncomparison with sequential processing. It was discovered that their tool (pipeline imple-\nmentation) performed better and faster.\n\nKrawczyk and Wozniak used several benchmark datasets which include Breast-Wis-\nconsin, Pima, Yeast3, Voting records, CYP2C19 isoform, RBF for estimating weights for \nthe new incoming data stream with their proposed method against other standard meth-\nods. They also analysed time and memory requirements. Experimental investigation \nresult proved that the proposed method can achieve better [69].\n\nA benchmark evaluation using an English movie review dataset collected from Rotten \nTomatoes website (a de facto benchmark for analysing sentiment applications) was con-\nducted by [70], the result showed that sentiment analysis engine (SAE) proposed by the \nauthors outperformed the bag of words approach.\n\nAuthors’ suite of ideas in [71] outperformed state-of-the-art searching technique \ncalled EBSM. The work of [72] used various datasets such as KDD-Cup 99, Forest Cover \ntype, Household power consumption, etc. They compared their algorithm—parallel \nK-means clustering with k-means and k-means++, the result showed that their algo-\nrithm performed better in terms of speed.\n\n5 http://en.wikin ews.org.\n\nhttp://en.wikinews.org\n\n\nPage 20 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nMozafari et al. in [73] benchmarked their system, XSeq against other general-purpose \nXML engines. The system outperformed other complex event processing engines by two \norders of magnitude improvement.\n\nAuthors in [74] evaluated their work in terms of time, accuracy and memory using \nForest cover type, Poker hand, and electricity datasets. They compared their method, \nadaptive windowing based online ensemble (AWOE) with other standard methods such \nas accuracy updated ensemble (AUE), online accuracy updated ensemble (OAUE), accu-\nracy weighted ensemble (AWE), dynamic weighted majority (DWM) and Lev Bagging \n(Lev). Their proposed approach outperformed other methods in three perspectives \nwhich include suitability in terms of different type of drifts, better resolved appropriate \nsize of block, and efficiency.\n\nThe evaluation performed by [75] using FACup and Super Tuesday datasets showed \nthat their method, which is a hybrid of topic extraction methods (i.e. a combination of \nfeature pivot and document pivot) has high efficiency and accuracy with respect to recall \nand precision.\n\nEvaluating the performance of low-rank reconstruction and prediction scheme, spe-\ncifically, singular spectrum matrix completion (SS-MC) proposed by [76], SensorScope \nGrand St-Bernard dataset6 and Intel Berkeley Research Lab dataset7 were used. The \nauthors compared their proposed method with three state-of-the-art methods; KNN-\nimputation, RegEM and ADMM version of MC and discovered that their method \noutperformed the other methods in terms of pure reconstruction as well as in the \ndemanding case of simultaneous recovery and prediction.\n\nThe authors in [77] evaluated their work using World Cup 1998 and CAIDA \nAnonymized Internet Traces 2011 datasets. When their method, ECM-Sketch (a sketch \nsynopsis that allows effective summarization of streaming data over both time-based \nand count-based sliding windows) was compared with three state-of-the-art algorithms \n(Sketch variants); ECM-RW, ECM-DW, and ECM-EH, variants using randomized waves, \ndeterministic waves and exponential histograms respectively, their method reduce \nmemory and computational requirements by at least one order of magnitude with a very \nsmall loss in accuracy.\n\nThe work of [78] centred on benchmarking real-time vehicle data streaming models \nfor a smart city using a simulator that emulates the data produced by a given amount of \nsimultaneous drivers. Experiment with the simulator shows that streaming processing \nengine such as Apache Kafka could serve as a replacement to custom-made streaming \nservers to achieve low latency and higher scalability together with cost reduction.\n\nA benchmark among Kyvos Insight, Impala and Spark conducted by [79] shows that \nKyvos Insight performed analytical queries with much lower latencies when there is a \nlarge number of concurrent users due to pre-aggregation and incremental code building \n[80].\n\nAuthors in [81] proposed that in addition to execution time and resource utilization, \nmicroarchitecture-level and energy consumption are key to fully understanding the \nbehaviour of big data frameworks.\n\n6 http://lcav.epfl.ch.page-86035 -en.html.\n7 http://db.csail .mit.edu/labda ta/labda ta.html.\n\nhttp://lcav.epfl.ch.page-86035-en.html\nhttp://db.csail.mit.edu/labdata/labdata.html\n\n\nPage 21 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nIn addition, to strengthen the confidence of big data research evaluation or result, \napplication of empirical methods (i.e. tested or evaluated concept or technology for \nevidence-based result) should be highly encouraged. The current status of empirical \nresearch in big data stream analysis is still at an infant stage. The maturity of a research \nfield is directly proportional to the number of publications with empirical result [20, 21]. \nAccording to [21] that conducted a systematic literature mapping to verify the current \nstatus of empirical research in big data, it was found out that only 151 out of 1778 stud-\nies contained empirical result. As a result, more research efforts should be directed to \nempirical research in order to raise the level of confidence of big data research outputs \nthan it is at present.\n\nMoreover, only a few big data benchmarks are suitable for different workloads at pre-\nsent. Research efforts should be geared towards advancing benchmarks that are suitable \nfor evaluating different big data systems. This would go a long way to reduce cost and \ninteroperability issue.\n\nDiscussion\nFrom the analysis, it was observed that there has been a wave of interest in big data \nstream analysis since 2013. The number of papers produced in 2012 was doubled in \n2013. In the same vein, more than double of the papers in 2013 were produced in 2014. \nThere was a relative surge in 2017 having a total of 98 paper while the year 2018 received \n156 papers (see Tables 9, 10 and Fig. 2). The percentage of papers analyzed from journals \nwas 50%; that of conferences was 41% while that of workshop/technical/symposium was \n9% as depicted in Fig. 3. Figure 4 presented the frequency of research efforts from differ-\nent geographical locations with researchers from China taking the lead. \n\nThe selection of big data streaming tools and technologies should be based on the \nimportance of each of the factors such as the shape of the data, data access, availabil-\nity and consistent requirements, workload profile required, and latency requirement. \nCareful selection with respect to open source technology must be made especially when \nchoosing a recent technology still in production. Moreover, the problem to address, the \nunderstanding of the true costs, and benefits of both open and proprietary solutions are \nalso vital when making a selection.\n\nA lot of research efforts have been directed to big data stream analysis but social media \nstream preprocessing is still an open issue. Due to inherent characteristics of social \nmedia stream which include incomplete, noisy, slang, abbreviated words, social media \nstreams present a challenge to big data streams analytics algorithms. There is the need \nto give more attention to the preprocessing stage of social media stream analysis in the \nface of incomplete, noisy, slang, and abbreviated words that are pertinent to social media \nstreams in order to improve big data streams analytics result.\n\nOut of 19 big data streaming tools and technologies compared, 100% support stream-\ning, 47.4% can do both batch and streaming processing while only 10.5% support stream-\ning, batch and iterative processing. Depending on the state of the data to be processed, \ninfrastructure preference, business use case, and kind of results that is of interest, choos-\ning a single big data streaming technology platform that supports all the system require-\nments minimizes the effect of interoperability constraints.\n\n\n\nPage 22 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nFrom all the big data streaming tools and technologies reviewed, only IBMInfoS-\nphere and TIBCO StreamBase support all of the three “at-most-once”, “at-least-once”, \nand “exactly-once” message delivery mechanisms while others support one or two of \nthe three delivery mechanisms. Having all the three delivery mechanisms give room for \nflexibility.\n\nIt is rare to find a specific big data technology that combines key features such as scal-\nability, integration, fault-tolerance, timeliness, consistency, heterogeneity and incom-\npleteness management, and load balancing. There seems to be no big data streaming \ntool and technology that offers all the key features required for now. This calls for more \nresearch efforts that are directed to building more robust big data streaming tools and \ntechnologies.\n\nFew big data benchmarks are suitable for a hybrid of big data systems at present and \nstandard benchmark datasets for big data streaming analytics have not been widely \nadopted. Hence, research efforts should be geared towards advancing benchmarks that \nare suitable for evaluating different big data systems.\n\nLimitation of the review\nWhile authors explored Scopus, ScienceDirect and EBSCO databases which index high \nimpact journals and conference papers from IEEE, ACM, SpringerLink, and Elsevier to \nidentify all possible relevant articles, it is possible that some other relevant articles from \nother databases such as Web of Science could have been missed.\n\nThe analysis and synthesis are based on interpretation of selected articles by the \nresearch team. The authors attempted to avoid this by cross-checking papers to deal \nwith bias though that cannot completely rule out the possibility of errors. In addition, \nthe authors implemented the inclusion and exclusion criteria in the selection of articles \nand only relevant articles written in the English Language were selected. Building on the \nunderpinning of the findings of the research, while a lot of research has been done with \nrespect to tools and technologies as well as methods and techniques employed in big \ndata streaming analytics, method of evaluation or benchmarks of the technologies of \nvarious workloads for big data streaming analytics have not received much attention. As \nit could be gathered from the literature reviewed that most of the researchers evaluated \ntheir work using either synthetic or real-life datasets.\n\nConclusion and further work\nAs a result of challenges and opportunities presented by the Information Technology \nrevolution, big data streaming analytics has emerged as the new frontier of competition \nand innovation. Organisations who seize the opportunity of big data streaming analytics \nare provided with insights for robust decision making in real-time thereby making them \nto have an edge over their competitors.\n\nIn this paper, the authors have tried to present a holistic view of big data streaming \nanalytics by conducting a comprehensive literature review to understand and identify \nthe tools and technologies, methods and techniques, benchmarks or methods of evalu-\nation employed, and key issues in big data stream analysis to showcase the signpost of \nfuture research directions.\n\n\n\nPage 23 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nTa\nbl\n\ne \n10\n\n D\nis\n\ntr\nib\n\nut\nio\n\nn \nof\n\n p\nap\n\ner\ns \n\nov\ner\n\n th\ne \n\nst\nud\n\nie\nd \n\nye\nar\n\ns\n\nYe\nar\n\n20\n04\n\n20\n05\n\n20\n06\n\n20\n07\n\n20\n08\n\n20\n09\n\n20\n10\n\n20\n11\n\n20\n12\n\n20\n13\n\n20\n14\n\n20\n15\n\n20\n16\n\n20\n17\n\n20\n18\n\nTo\nta\n\nl\n\nPa\npe\n\nr\n2\n\n1\n2\n\n3\n5\n\n2\n5\n\n4\n5\n\n10\n22\n\n28\n38\n\n98\n15\n\n6\n38\n\n1\n\n\n\nPage 24 of 30Kolajo et al. J Big Data (2019) 6:47 \n\nAlthough a lot of research efforts have been directed towards big data at rest (i.e. \nbig data batch processing), there has been increased interest in analysing big data \nin mo", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNTM3LTAxOS0wMjEwLTcucGRm0", "metadata_author": "Taiwo Kolajo ", "metadata_title": "Big data stream analysis: a systematic literature review", "metadata_creation_date": "2019-06-04T14:40:29Z", "keyphrases": [ "Big data stream analysis", "systematic literature review" ] }, { "@search.score": 1, "content": "\nRESEARCH Open Access\n\nA classification method for social\ninformation of sellers on social network\nHaoliang Cui1, Shuai Shao2* , Shaozhang Niu1, Chengjie Shi3 and Lingyu Zhou1\n\n* Correspondence: shaoshuaib@163.\ncom\n2China Information Technology\nSecurity Evaluation Center, Beijing\n100085, China\nFull list of author information is\navailable at the end of the article\n\nAbstract\n\nSocial e-commerce has been a hot topic in recent years, with the number of users\nincreasing year by year and the transaction money exploding. Unlike traditional e-\ncommerce, the main activities of social e-commerce are on social network apps. To\nclassify sellers by the merchandise, this article designs and implements a social\nnetwork seller classification scheme. We develop an app, which runs on the mobile\nphones of the sellers and provides the operating environment and automated\nassistance capabilities of social network applications. The app can collect social\ninformation published by the sellers during the assistance process, uploads to the\nserver to perform model training on the data. We collect 38,970 sellers’ information,\nextract the text information in the picture with the help of OCR, and establish a\ndeep learning model based on BERT to classify the merchandise of sellers. In the\nfinal experiment, we achieve an accuracy of more than 90%, which shows that the\nmodel can accurately classify sellers on a social network.\n\nKeywords: User model, Machine learning, Social e-commerce\n\n1 Introduction\nWith the continuous improvement of social network and mobile payment technology,\n\none kind of commodity trading based on social relations called social e-commerce is in\n\nrapid development. According to the 2019 China social e-commerce industry develop-\n\nment report released by the Internet society of China, the number of employees of so-\n\ncial e-commerce in China is expected to reach 48.01 million in 2019, up by 58.3\n\npercent year on year, and the market size is expected to reach 2060.58 billion yuan, up\n\nby 63.2% year on year. Social e-commerce has become a large scale, and the high\n\ngrowth cannot be ignored. Different from e-commerce platforms such as Taobao, so-\n\ncial e-commerce is at the end of online retail. It carries out trading activities through\n\nsocial software and uses social interaction, user generated content and other means to\n\nassist the purchase and sale of goods. At the same time, sellers on social network use\n\ndifferent social software without uniform registration, have no systematic classification\n\nof products for sale, and there are no standardized terms for product description.\n\nThese bring great difficulty to the accurate classification of user portrait. This paper\n\nproposes a method based on the NLP classification model, which can realize accurate\n\n© The Author(s). 2021 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n\nEURASIP Journal on Image\nand Video Processing\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 \nhttps://doi.org/10.1186/s13640-020-00545-z\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-020-00545-z&domain=pdf\nhttp://orcid.org/0000-0001-9638-0201\nmailto:shaoshuaib@163.com\nmailto:shaoshuaib@163.com\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nbusiness classification of social e-commerce based on social information of social e-\n\ncommerce. This method analyzes 38,970 sellers on social networks and establishes a\n\ndeep learning model based on BERT to accurately classify the merchandise of sellers.\n\nIn addition, we introduced the OCR algorithm to extract the text information in the\n\npicture and superimposed it on the social content data, which effectively improved the\n\nclassification accuracy. The final experiment shows that the measured accuracy is more\n\nthan 90%.\n\n2 Related work\n2.1 Natural language processing\n\nIn order to analyze e-commerce business classification based on social data of sellers on a\n\nsocial network, the text needs to be analyzed based on the NLP correlation algorithm.\n\nThe rapid development of NLP at the present stage is due to the neural network language\n\nmodel (NNLM) Bengio et al. [1] proposed in 2003. Researchers have been trying to realize\n\nthe end-to-end classification recognition by using a neural network as a classifier in the\n\ntext classification research based on word embedding. Kim first introduces the convolu-\n\ntional neural network (CNN) into the study of text classification. The network structure is\n\na dropout full connection layer and a softmax layer connected after one convolution layer\n\n[2]. Although this algorithm achieves good results in various benchmark tests, it cannot\n\nobtain long-distance text dependency due to the limitation of network structure. There-\n\nfore, Tencent AI Lab proposed DPCNN, which further enhanced the extraction capacity\n\nof long-distance text dependency by deepening CNN [3].\n\nSocial content data includes multimedia text data and picture data. With the help of\n\nOCR, we extract the text in the picture and convert the picture data into text data. Text\n\nis a kind of sequential data, and the classification of it by recurrent neural network\n\n(RNN) has been the focus of long-term research in academia [4]. As a variation of\n\nRNN, long short-term memory (LSTM) adds control units such as forgetting gate, in-\n\nput gate, and output gate on the original basis, which solves the problem of gradient\n\nexplosion and gradient disappearance in the long sequence training of RNN and pro-\n\nmotes the use of RNN [5]. By introducing the sharing information mechanism, Liu\n\net al. further improved the accuracy of the RNN algorithm in the text multi-\n\nclassification task and achieved good results in four benchmark text classifications [6].\n\nHowever, Word vectors cannot be constructed in Word embedding to solve the\n\nproblem of polysemy. Even though different semantic environments are considered\n\nduring training, the result of training is still one word corresponding to one row vector.\n\nConsidering the widespread phenomenon of polysemy, Peters et al. propose embed-\n\ndings from language model (ELMO) to address the impact of polysemy on natural lan-\n\nguage modeling [7]. ELMO uses a feature-based form of pre-training. First, two-way\n\nLSTM is used to pre-train the corpus, and then word embedding resulting from train-\n\ning is adjusted by double-layer two-way LSTM when processing downstream tasks to\n\nadd more grammatical and semantic information according to the context words.\n\nThe ability of ELMO to extract features is limited for choosing LSTM as the feature\n\nextractor instead of Transformer [8], and ELMO’s bidirectional splicing method is also\n\nweak in feature fusion. Therefore, Devlin et al. propose the BERT model, taking Trans-\n\nformer as a feature extractor to pre-train large-scale text corpus [9].\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 2 of 12\n\n\n\n2.2 User analysis of social networks\n\nUser analysis is an important part of social network analysis. Most existing studies use\n\nuser-generated content or social links between users to simulate users. Wu et al. mod-\n\neled users on the content curation social network (CCSN) in the unified framework by\n\nmining user-generated content and social links [10]. They proposed a potential Bayes-\n\nian model, multilevel LDA (MLLDA), that could represent users of potential interest\n\nfound in social links formed by text descriptions contributed by users and information\n\nsharing. In 2017, Wu et al. proposed a latent model [11], trying to explain how the so-\n\ncial network structure and users’ historical preferences change over time affect each\n\nuser’s future behavior and predict each user’s consumption preferences and social con-\n\nnections in the near future. Malli et al. proposed a new online social network user pro-\n\nfile rating model [12], which solved the problem of large and complicated user data. In\n\nterms of data analysis platform, Chen et al. [13] developed a big data platform for the\n\nstudy of the garlic industry chain. Garlic planting management, price control, and pre-\n\ndiction were realized through data collection, storage, and pretreatment. Ning et al.\n\n[14] designed a ga-bp hybrid algorithm based on the fuzzy theory and constructed an\n\nair quality evaluation model by combining the knowledge of BP neural network, genetic\n\nalgorithm, and fuzzy theory. Yin et al. [15] studied two methods of extracting supervis-\n\nory relations and applied them to the field of English news. One is the combination of\n\nsupport vector machine and principal component analysis, and the other is the combin-\n\nation of support vector machine and CNN, which can extract high-quality feature vec-\n\ntors from sentences of support vector machine. In the social apps, the data we obtain is\n\nmostly image data, so we introduced the OCR technology to identify text information\n\nin images.\n\n3 Data collection\nIn order to analyze the behavior patterns of social e-commerce, we developed an auxil-\n\niary tool for social e-commerce. In this tool, sellers on a social network are provided\n\nwith the independent running environment of social software and the automatic auxil-\n\niary ability, and the information acquisition module of the auxiliary process is used to\n\ncollect the social information published by sellers on a social network, which is\n\nuploaded to the background server for model training. We provided this tool to nearly\n\n10,000 sellers on a social network who participated in the experiment to obtain their\n\nsocial information in their e-commerce activities.\n\n3.1 Overall structure\n\nThe whole data collection scheme is mainly composed of two parts: intelligent space\n\napp and background server. The overall architecture is shown in Fig. 1. Intelligent\n\nspace app is deployed in the mobile phones of sellers on a social network and imple-\n\nmented based on the application layer of the Android platform, providing sellers on a\n\nsocial network with a secure container for the independent operation of social software.\n\nThe app contains the automatic assistant module, which provides the automatic assist-\n\nant capability of various business processes for seller, and collects the social informa-\n\ntion in the auxiliary process through the information grasping module. The collected\n\ninformation is cached and uploaded locally through the information collection service.\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 3 of 12\n\n\n\nThe background server is responsible for receiving the collected data uploaded by the\n\nintelligent space, preprocessing the data first, and then classifying the social e-\n\ncommerce through the data based on the machine learning classification model, and fi-\n\nnally storing the classification results.\n\n3.1.1 Security container\n\nThe security container is designed to allow social software to run independently with-\n\nout modifying the OS or gaining root privileges. The basic principle of its realization is\n\nto create an independent container process; load APK file of social software dynamic-\n\nally; monitor and intercept process communication interface such as Binder IPC\n\nthrough Libc hook, Java reflection, dynamic proxy, and other technical means; and col-\n\nlect social information through an automatic assistant module. The main part of the\n\ncontainer is composed of an application layer module and a service layer module.\n\nThe application layer module is responsible for the process startup and execution of\n\nsocial software, and its main functions include three parts.\n\n3.1.1.1 Interactive interception The application layer module intercepts the inter-\n\naction between the application process and the underlying system in the container and\n\nmodifies the calling logic. By hook or dynamic proxy of system library API and Binder\n\ncommunication interface, the application layer module blocks all interfaces that interact\n\nwith the system during the execution of social software and controls the process\n\nboundary of interaction between social applications and system services.\n\n3.1.1.2 Social information collection The loading of the automatic auxiliary module\n\nby social software is realized when initializing the process of social application.\n\nThe application layer module injects the corresponding plugins in the automatic\n\nassistant module into the social application process. The automatic assistance mod-\n\nule provides a number of e-commerce auxiliary functions for sellers on a social\n\nnetwork, including customer acquisition, social customer relationship management\n\nLinux Kernel\n\nBinder Mode\n\nIntelligent Space\n\nService Layer Mode\nAMS Proxy PMS Proxy\n\nApplication Layer Mode\nSocial App\n\nInteractive \ninterception\n\nautomatic \nassistance \nmodule\n\nInformation\nCollection \n\nBinder IPC\n\nBinder IPC\n\nBinder \nIPC Backgroud\n\n Server\n\nInternet\n\nFig. 1 The overall architecture diagram of the data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 4 of 12\n\n\n\n(SCRM), group management, sales assistance, and daily affairs. Sellers on social\n\nnetworks publish social information with commercial attributes through auxiliary\n\nfunctions, then the automatic auxiliary module will automatically collect the social\n\ninformation and send it to the information collection service for processing.\n\n3.1.1.3 Local processing of social information When the information collection ser-\n\nvice receives the social information collected by the automatic auxiliary module, the\n\ndata will be compressed and encrypted in the local cache. The service then uploads the\n\ncollected data to the background server periodically through the timer, and HTTPS is\n\nused to ensure data transmission security.\n\nThe main function of the service layer module is to take over the call logic modified\n\nby the application layer module by simulating the system service modify the parameters\n\nin the communication process and finally call the real system service. The service layer\n\nmodule exists in the container as an independent process. It focuses on the simulation\n\nof activity manager service (AMS) and package manager service (PMS) and realizes the\n\nsupport of system services in the process of launching and running social software.\n\n3.1.2 Background server\n\nThe background server mainly realizes the machine learning model processing of the\n\ncollected social data, including the functions of data preprocessing, data training, classi-\n\nfication, and result storage. The core processing logic will be described in chapter 5.\n\n3.2 Key processes\n\nThere are four key processes in the process of social information collection and pro-\n\ncessing. They are social software process initialization, social software process\n\nIntelligent \nSpace App \nlaunched\n\nProcess Boundaries\n\nUser Process\n\nSocial software process \ninitialization\n\nlaunching social \nsoftware\n\ninject automatic \nauxiliary\n\nSocial software process \nexecution\n\nRun the plug-in\n\nCollect social \ninformation\n\nProcess Boundaries\n\nUser Process\n\nLocal processing of \nsocial information\n\nBatch upload \nprocessed social \n\ninformation\n\nEncrypt, compress \nand store social \n\ninformation\n\nInternet\n\nInformation collecting \nservice process\n\nBackground processing \nof social information\n\nReceiving social \ninformation\n\nPreprocessing social \ninformation\n\nThe Server\n\nMachine learning \ncategorizes social \n\ninformation\n\nStore the \nclassification results\n\nFig. 2 Key flow chart of data acquisition scheme\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 5 of 12\n\n\n\nexecution, local processing of social information, and background processing of social\n\ninformation. The complete process is shown in Fig. 2.\n\n3.2.1 Social software process initialization\n\nWhen launching social software, the intelligent space will first intercept the callback\n\nfunction of the life cycle of all its components, then realize the process loading of the\n\nautomatic auxiliary module during the process initialization.\n\n3.2.2 Social software process execution\n\nThe process execution is completed by the application layer module and service layer\n\nmodule together. Sellers on a social network use automatic auxiliary modules to\n\ncomplete business activities, trigger information capture module to collect social infor-\n\nmation, and send it to the information collection service for subsequent processing.\n\n3.2.3 Local processing of social information\n\nThe local processing of social information is mainly completed by the information col-\n\nlection service. In order to ensure the safe storage and transmission of the collected so-\n\ncial information, the information collection service first adopts the encryption and\n\ncompression method to realize the local security cache and then adopts the HTTPS se-\n\ncure communication and transmission protocol to upload the data.\n\n3.2.4 Background processing of social information\n\nThe background processing of social information is completed by the background ser-\n\nver. The server first receives the social information uploaded by the intelligent space,\n\nnext decrypts and decompresses the social information, cleans the plaintext data, uses\n\nthird-party OCR technology to identify text information in images, and adds it to the\n\nuser’s social information after simple data processing. Then, the classification of sellers\n\non a social network is realized through the data based on machine learning modeling.\n\nFinally, the classification results are stored in the target database.\n\n4 Methods\nTo classify the business attributes of social e-commerce based on the information of\n\nsellers on a social network, traditional feature matching scheme and classification clus-\n\ntering scheme based on machine learning can be used to establish the model. In this\n\nchapter, we introduce the scheme based on term frequency-inverse document fre-\n\nquency (TF-IDF) clustering and the classification scheme based on BERT.\n\n4.1 Feature classification and TF-IDF clustering\n\n4.1.1 Feature classification\n\nWe randomly select 5000 sellers on a social network from the data collected by the\n\nbackground server and extracted the text data of their social information for analysis.\n\nEach social e-commerce user contains an average of 50 social text data. Based on the\n\ncontent, we manually classify social e-commerce into 11 categories. With the help of e-\n\ncommerce platforms like JD.COM, 50–100 keywords are sorted out for each category,\n\nand these keywords are screened and expanded according to the language habits of\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 6 of 12\n\n\n\nsellers on a social network. On this basis, we collect all the social information of each\n\nsocial network seller, cut and remove word segmentation, and match the results with\n\nthe keywords of the selected 11 categories. The number of keywords that are matched\n\nis counted as the matching degree. According to the situation of different classification,\n\nthe threshold of matching degree is determined by manual screening of some results,\n\nand then all social e-commerce is classified according to the threshold. After\n\noptimization and verification, the accuracy of the classical feature matching scheme fi-\n\nnally reached 40%. However, due to the simplicity of the rules of the feature matching\n\nscheme, the small optimization space, the high misjudgment rate of the scheme, and\n\nthe large human intervention in the basic word segmentation process, it is difficult to\n\ncover various situations of social e-commerce due to the limitation of these basic key-\n\nwords, thus making it insensitive to the dynamic changes of new hot words of social e-\n\ncommerce.\n\n4.1.2 TF-IDF clustering\n\nTo achieve the goal of accurate classification of social e-commerce, we designed a\n\nscheme based on TF-IDF clustering. Term frequency-inverse document frequency (TF-\n\nIDF) is a commonly used weighted technique for information retrieval and text mining\n\nto evaluate the importance of a single word to a document in a set of documents or a\n\ncorpus. In this scheme, the social information of each social e-commerce user is\n\nmapped as one file set of TF-IDF, and all texts of all sellers on a social network are\n\nmapped as the whole corpus. The words with the highest frequency used by each social\n\ne-commerce user are the most representative words in this document and become key-\n\nwords. Category labels can be generated to calculate the probability that a document\n\nbelongs to a certain category using the naive Bayes algorithm formula. The advantages\n\nof TF-IDF clustering to achieve the classification of sellers on the social network in-\n\nclude the following: (1) clear mapping; (2) emphasize the weight of keywords and lower\n\nthe weight of non-keywords; (3) compared with other machine learning algorithms, the\n\ncharacteristic dimension of the model is greatly reduced to avoid the dimension disas-\n\nter; and (4) while improving the efficiency of classification calculation, ensure that the\n\nclassification effect has a good accuracy and recall rate. The architecture of the entire\n\nsolution is shown in Fig. 3.\n\nIn the text preprocessing stage, the first thing to do is to format the social informa-\n\ntion, mainly including deleting the space, deleting the newline character, merging the\n\nsocial e-commerce text, and so on, and finally getting the text to be processed for word\n\nsegmentation. In this scheme, we choose Jieba’s simplified mode for word segmenta-\n\ntion, then filter out the noise by filtering the stop words (e.g., yes, ah, etc.).\n\nIn the stage of establishing the vector space model, the first step is to load the train-\n\ning set and take the pre-processed social information of each social e-commerce user\n\nas a document. The next step is to generate a dictionary, by adding every word that ap-\n\npears in the training set to it, using the complete dictionary to calculate the TF-IDF\n\nvalue of each document. In this scheme, CountVectorizer and TfidfTransformer in Py-\n\nthon’s Scikit-Learn library are used. CountVectorizer is used to convert words in the\n\ntext into word frequency matrix, TfidfTransformer is used to count the TF-IDF value\n\nof each word in each document, and the top20 words in each document are taken as\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 7 of 12\n\n\n\nkeywords of sellers on a social network. After this step, the keywords with a large TF-\n\nIDF value in each document are the most representative words in the document, which\n\nbecome the keyword set of the social e-commerce user. Finally, the naive Bayes method\n\nis used to generate the category label, and the document vectors belonging to the same\n\ncategory in the TF-IDF matrix are added to form a matrix of m*n, where m represents\n\nthe number of categories and n represents the number of documents. The weight of\n\neach word is divided by the total weight of all words of the class, to calculate the prob-\n\nability that a document belongs to a certain class.\n\nIn the model optimization stage, we optimize the whole scheme model by adjusting\n\nthe stop word set, adjusting parameters (including CountVectorizer, TfidfTransformer\n\nclass construction parameters), and adjusting the category label generation method.\n\nThe main idea of TFIDF is if a word or phrase appears in an article with a high fre-\n\nquency of TF, and rarely appears in other articles, it is considered that the word or\n\nphrase has a good classification ability and is suitable for classification. TFIDF is actu-\n\nally: TF * IDF, TF is term frequency and IDF is inverse document frequency.\n\nIn a given document, word frequency refers to the frequency of a given word in the\n\ndocument. This number is a normalization of the number of words to prevent it from\n\nbeing biased towards long documents. For the word ti in a particular document, its im-\n\nportance can be expressed as:\n\ntf i; j ¼\nj D j\n\nj j : ti∈d j\n� � j\n\namong them:\n\n|D|: The total number of files in the corpus\n\n∣{j : ti ∈ dj}∣: The number of documents containing the term ti (i.e., the number of\n\ndocuments in ni, j ≠ 0). If the term is not in the corpus, it will cause the dividend to be\n\nzero, so it is generally used 1 + ∣ {j : ti ∈ dj}∣.\n\nand then:\n\n Social e-\ncommerce data\n\nData preparation\n\nFormat processing\n\nFilter stop words\n\nText preprocessing\n\nGenerate directory\n\nBuild the vector space and \nTF-IDF\n\nGenerate category tags\n\nBayesian classifier\n\nText articiple\n\nLoad training set \n\nBuild tf matrix \n\nbuild vector\n\nbuild matrix \n\nConditional probability \nmatrix\n\nModel optimization\n\nFig. 3 TF-IDF scheme framework\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 8 of 12\n\n\n\ntfidf i; j ¼ tf i; j � idf i\n\nA high word frequency in a particular document and a low document frequency of\n\nthe word in the entire document collection can produce a high-weight TF-IDF. There-\n\nfore, TF-IDF tends to filter out common words and keep important words.\n\n4.2 Classification scheme based on BERT\n\n4.2.1 Data label\n\nWe manually classify and mark the data of sellers on a social network according to the\n\ncharacteristics of the products. Classified labels include 38,970 items and 17 categories\n\nof data, including 3c, dress, food, car, house, beauty, makeup, training, jewelry, promo-\n\ntion, medicine and health, phone charge recharge, finance, card category, cigarettes, es-\n\nsays, and others. The pre-processing phase removes emojis, numbers, and spaces from\n\nthe text through Unicode encoding.\n\n4.2.2 Classification scheme\n\nIn the BERT model, Transformer, as an encoder-decoder model based on attention\n\nmechanism, solves the problem that RNN cannot deal with long-distance dependence\n\nand the model cannot be parallel, improving the performance of the model without re-\n\nducing the accuracy. At the same time, BERT introduced the shading language model\n\n(MLM, masked language model) and context prediction method, further enhance the\n\ntwo-way training of the ability of feature extraction and text. MLM uses Transformer\n\nencoders and bilateral contexts to predict random masked tokens to pre-train two-way\n\ntransformers. This makes BERT different from the GPT model, which can only conduct\n\none-way training and can better extract context information through feature fusion.\n\nAnaphase prediction is more embodied in QA and NLI. Therefore, we choose the\n\nBERT model based on the bidirectional coding technology of pre-training and attention\n\nmechanism to classify sellers on a social network.\n\nWe chose the official Chinese pre-training model of Google as the pre-training model\n\nof the experiment: BERT-Base which is Chinese simplified and traditional, 12-layer,\n\n768-hidden, 12-head, 110M parameters [16]. This pre-training model is obtained by\n\nGoogle’s unsupervised pre-training on a large-scale Chinese corpus. On this basis, we\n\nwill carry out fine-tuning to realize the classification model of sellers on a social net-\n\nwork. When dividing the data set, we divided 38,970 pieces of data into training set\n\nand verification set according to the ratio of 6:4, that is, 23,382 pieces of training set\n\nand 15,588 pieces of verification set.\n\n5 Results and discussion\n5.1 TF-IDF clustering scheme\n\nThe computer used in the experiment is configured with AMD Ryzen R5-4600H CPU,\n\n16G memory, and windows10 64bit operating system. First, the default construction\n\nparameters are used, and the average accuracy of each classification is 45.7%. Next, the\n\nparameters are adjusted through a genetic algorithm, and 100 rounds of genetic algo-\n\nrithm optimization are performed, then the average accuracy reached the highest value\n\nof 52.5%. In the process of genetic algorithm, statistical estimation of algorithm time is\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 9 of 12\n\n\n\nalso carried out. On average, on this data set, the running time of each round of the\n\nTF-IDF model is about 28 s.\n\nExperiments show that the accuracy of the TF-IDF clustering scheme has been\n\nimproved after optimization, and it has a certain reference value for the classifica-\n\ntion of sellers on a social network, but there is still a big gap from the accurate\n\nclassification. We found three reasons after analyzing the experimental results. (1)\n\nCompared to the feature matching scheme, the TF-IDF-based model is improved\n\nto some extent. However, the input of the model is still the result of direct word\n\nsegmentation, and more information is lost in the word segmentation process, such\n\nas the semantic information of previous and later texts and the repetition fre-\n\nquency of corpus, which are relatively important in the process of natural language\n\nprocessing. (2) The classification problem of sellers on a social network is compli-\n\ncated. This model does not analyze the correlation between words and is essen-\n\ntially an upgraded version of word frequency statistics, which makes it difficult to\n\nimprove the accuracy after reaching a certain value. (3) For the optimization of the\n\nmodel, only the parameters of the intermediate function are adjusted, and the\n\nmethod is not upgraded. Therefore, the machine learning scheme based on TF-IDF\n\nclustering cannot solve the problem of accurate classification of sellers on a social\n\nnetwork. In the next chapter, we will introduce a scheme based on deep learning\n\nto achieve the goal of classifying sellers on a social network.\n\n5.2 Classification scheme based on BERT\n\nText classification fine-tuning is to serialize the preprocessed text information\n\ntoken and input BERT, and select the final hidden state of the first token [CLS] as\n\na sentence vector to output to the full connection layer, and then output the prob-\n\nability of obtaining various labels corresponding to the text through the softmax\n\nlayer. The experimental schematic diagram is shown in Figs. 4 and 5. The max-\n\nimum length of the sequence (ma_seq_length) is set to 256 according to the actual\n\ntext length of the social information data set of the sellers on a social network and\n\nFig. 4 Text message token serialization\n\nFig. 5 Text classification BERT fine-tuning model structure diagram\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 10 of 12\n\n\n\nthe batch_size and learning rate adopt the official recommended values of 32 and\n\n2e−5. In addition, we also adjust the super parameter num_train_epochs and in-\n\ncrease the number of training epochs (num_train_epochs) from 3 to 9 to improve\n\nthe recognition rate of the model (Table 1). The results are shown in Table 2.\n\nWe select an additional 9500 text data of sellers on social networks and test the\n\nmodel after the same preprocessing. The accuracy rate is 90.5%, which is lower than\n\nthat of the verification set (96.2%). The reason may be that the data of the test set con-\n\ntains a large number of commodity terms not included in the corpus and training set,\n\nand the text description of these commodities is too colloquial. Sellers on a social net-\n\nwork often use colloquial words in the industry to replace the standard product names\n\nwhen releasing product information, such as “Bobo” instead of “Botox,” which to some\n\nextent limits the accuracy of text-based classification in the social e-commerce market\n\nscene.\n\n6 Conclusion\nThe classification model proposes in this paper achieves an accuracy of 90.5% in the\n\ntest data. However, there are still some problems such as non-standard description text.\n\nA corpus with a high correlation with a social e-commerce environment will be estab-\n\nlished in order to further improve the accuracy of social e-commerce classification. At\n\nthe same time, we will use the knowledge distillation technology to compress and refine\n\nthe existing model, so as to improve the model recognition rate while simplifying the\n\nmodel and improving the operational performance [16]. In addition, in view of the high\n\nlabor cost and time cost of large-scale data marking, the next step will be trying to\n\nmake full use of semi-supervised learning to train unlabeled data and labeled completed\n\ndata [17]. The full use of large-scale unlabeled data is conducive to further improving\n\nthe accuracy and generalization ability of the model, as well as the analysis and process-\n\ning of emerging products, providing strong data support for the model landing. Since\n\nthe image data have also been studied to profiling the users in a social network [18]\n\nand perceptual image hashing schemes are proposed [19], we will improve our model\n\nso that the image and text data are combined for analysis.\n\nTable 1 Corresponding table of epoch and accuracy\n\nEpoch eval_accuracy (%)\n\n3 95.84\n\n6 96.05\n\n9 96.2\n\nThe training results are shown in Table 2, and the recognition rate is 96.2%\n\nTable 2 Text information classification results of sellers on social network\n\nResults Value\n\neval_accuracy 96.2%\n\neval_loss 0.25033528\n\nglobal_step 6024\n\nLoss 0.25023073\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 11 of 12\n\n\n\nAbbreviations\nBERT: Bidirectional Encoder Representations from Transformers; DPCNN: Deep pyramid convolutional neural networks;\nOCR: Optical character recognition\n\nAcknowledgements\nNot applicable\n\nAuthors’ contributions\nHaoliang Cui designed the scheme and carried out the experiments. Shuai Shao gave suggestions on the structure of\nthe manuscript and participated in modifying the manuscript. All authors read and approved the final manuscript.\n\nFunding\nNational Natural Science Foundation of China (Award Number 61370195, U1536121)\n\nAvailability of data and materials\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1Beijing Key Laboratory of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and\nTelecommunications, Beijing 100876, China. 2China Information Technology Security Evaluation Center, Beijing 100085,\nChina. 3Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100088, China.\n\nReceived: 16 March 2020 Accepted: 25 December 2020\n\nReferences\n1. Y. Bengio, R. Ducharme, P. Vincent, et al., A neural probabilistic language model. J. Mach. Learn. Res. 3, 1137–1155\n\n(2003)\n2. Kim Y. Convolutional neural networks for sentence classification arXiv preprint arXiv:1408.5882, 2014.\n3. R. Johnson, T. Zhang, Deep pyramid convolutional neural networks for text categorization [C]//Proceedings of the 55th\n\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (2017), pp. 562–570\n4. Otter D W, Medina J R, Kalita J K. A survey of the usages of deep learning in natural language processing arXiv preprint\n\narXiv:1807.10854, 2018.\n5. R. Jozefowicz, W. Zaremba, I. Sutskever, An empirical exploration of recurrent network architectures [C]//International\n\nconference on machine learning (2015), pp. 2342–2350\n6. Liu P, Qiu X, Huang X. Recurrent neural network for text classification with multi-task learning arXiv preprint arXiv:1605.\n\n05101, 2016.\n7. Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n8. A. Vaswani, N. Shazeer, N. Parmar, et al., Attention is all you need [C]//Advances in neural information processing systems\n\n(2017), pp. 5998–6008\n9. Devlin J, Chang M W, Lee K, et al. BERT: pre-training of deep bidirectional transformers for language understanding\n\narXiv preprint arXiv:1810.04805, 2018.\n10. L. Wu et al., MLLDA: multi-level LDA for modelling users on content curation social networks. Neurocomputing 236, 73–\n\n81 (2017)\n11. L. Wu et al., Modeling the evolution of users’ preferences and social links in social networking services. IEEE Transact.\n\nKnowledge. Data. Eng. 29.6, 1240–1253 (2017)\n12. M. Malli, N. Said, A. Fadlallah, A new model for rating users’ profiles in online social networks. Comput. Information. Sci.\n\n10.2, 39–51 (2017)\n13. W. Chen et al., Development and application of big data platform for garlic industry chain. Comput. Mater. Continua 58.\n\n1, 229 (2019)\n14. M. Ning et al., GA-BP air quality evaluation method based on fuzzy theory. Comput. Mater. Continua 58.1, 215–227 (2019)\n15. Yin, Libo, et al. Relation extraction for massive news texts. Tech Science Press, CMC,60, no.1(2019), pp.275-285.\n16. Sun S, Cheng Y, Gan Z, et al. Patient knowledge distillation for BERT model compression arXiv preprint arXiv:1908.09355, 2019.\n17. Yalniz I Z, Jégou H, Chen K, et al. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.\n\n00546, 2019.\n18. Yaqiong Qiao, Xiangyang Luo, Chenliang Li, et al. Heterogeneous graph-based joint representation learning for users\n\nand POIs in location-based social network, Inf. Process. Manag., 2020, 57, 102151-1~102151-17\n19. Jinwei Wang, Hao Wang, Jian Li, Xiangyang Luo, Yun-Qing Shi, Sunil Kr. Jha, Detecting double JPEG compressed color\n\nimages with the same quantization matrix in spherical coordinates, IEEE Trans. on CSVT, doi: 10.1109/TCSVT.2019.\n2922309.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nCui et al. EURASIP Journal on Image and Video Processing (2021) 2021:4 Page 12 of 12\n\nhttps://github.com/cuihaoliang/User-portraits-of-social-e-commerce\n\n\tAbstract\n\tIntroduction\n\tRelated work\n\tNatural language processing\n\tUser analysis of social networks\n\n\tData collection\n\tOverall structure\n\tSecurity container\n\tBackground server\n\n\tKey processes\n\tSocial software process initialization\n\tSocial software process execution\n\tLocal processing of social information\n\tBackground processing of social information\n\n\n\tMethods\n\tFeature classification and TF-IDF clustering\n\tFeature classification\n\tTF-IDF clustering\n\n\tClassification scheme based on BERT\n\tData label\n\tClassification scheme\n\n\n\tResults and discussion\n\tTF-IDF clustering scheme\n\tClassification scheme based on BERT\n\n\tConclusion\n\tAbbreviations\n\tAcknowledgements\n\tAuthors’ contributions\n\tFunding\n\tAvailability of data and materials\n\tCompeting interests\n\tAuthor details\n\tReferences\n\tPublisher’s Note\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczEzNjQwLTAyMC0wMDU0NS16LnBkZg2", "metadata_author": "Haoliang Cui", "metadata_title": "A classification method for social information of sellers on social network", "metadata_creation_date": "2021-01-12T23:22:39Z", "keyphrases": [ "classification method", "social information", "social network", "sellers" ] }, { "@search.score": 1, "content": "\nMining aspects of customer’s review \non the social network\nTu Nguyen Thi Ngoc1*, Ha Nguyen Thi Thu1 and Viet Anh Nguyen2\n\nIntroduction\nIn recent years, a lot of people often express their opinions about things such as products \nand services on social networks and e-commerce web sites. These opinions or reviews \noften play significant role in improving the quality of products and services. However, \nthe huge amount of reviews poses a challenge of how to efficiently mine useful informa-\ntion about a product or a service. To deal with this problem, much work has been intro-\nduced including summarizing users’ opinions [1], extracting information from reviews \n[2–5], analyzing user sentiments [6–9], and so on. In this paper, we focus on the problem \nof extracting information from reviews. More specifically, this study aims at developing \nefficient methods for dealing with the three tasks: extracting aspects mentioned in the \nreviews of a product, inferring the user’s rating for each identified aspect, and estimating \nthe weight posed on each aspect by the users.\n\nA user review often mentions different aspects, which are attributes or components of \na product. An aspect is usually a concept in which the user’s opinion is expressed in dif-\nferent level of positivity or negativity. For example, in the review given in Fig. 1, the user \nlikes the coffee, manifested by a 5-star overall rating. However, positive opinions about \n\nAbstract \n\nThis study represents an efficient method for extracting product aspects from cus-\ntomer reviews and give solutions for inferring aspect ratings and aspect weights. \nAspect ratings often reflect the user’s satisfaction on aspects of a product and aspect \nweights reflect the degree of importance of the aspects posed by the user. These \ntasks therefore play a very important role for manufacturers to better understand their \ncustomers’ opinion on their products and services. The study addresses the problem \nof aspect extraction by using aspect words based on conditional probability com-\nbined with the bootstrap technique. To infer the user’s rating for aspects, a supervised \napproach called the Naïve Bayes classification method is proposed to learn the aspect \nratings in which sentiment words are considered as features. The weight of an aspect \nis estimated by leveraging the frequencies of aspect words within each review and \nthe aspect consistency across all reviews. Experimental results show that the proposed \nmethod obtains very good performance on real world datasets in comparison with \nother state-of-the-art methods.\n\nKeywords: Aspect extraction, Aspect rating, Aspect weight, Conditional probability, \nCore term, Naive Bayes\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nMETHODOLOGY\n\nNguyen Thi Ngoc et al. J Big Data (2019) 6:22 \nhttps://doi.org/10.1186/s40537-019-0184-5\n\n*Correspondence: \ntunn.dhdl@gmail.com \n1 Department \nof E-Commerce, Vietnam \nElectric Power University, \n235 Hoang Quoc Viet, Hanoi, \nVietnam\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-019-0184-5&domain=pdf\n\n\nPage 2 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nbody, taste, aroma and acidity aspects of the coffee are also given. The task of aspect \nextraction is to identify all such aspects from the review. A challenge here is that some \naspects are explicitly mentioned and some are not. For instance, in the review given in \nFig. 1, taste and acidity of the coffee are explicitly mentioned, but body and aroma are \nnot explicitly specified. Some previous work dealt with identifying explicit aspects only, \nfor example [10]. In our paper, both explicit and implicit aspects are identified. Another \ndifficulty of the aspect extraction task is that it may generate a lot of noise in terms of \nnon-aspect concepts. How to minimize noise while still be able to identify rare and \nimportant aspects is also one of our concerns in this paper.\n\nMost of the earliest work to identify aspects are unsupervised model-based [11], in \nwhich statistics of relevant words are used. These methods do not require the labeled \ntraining data and have low cost. For example, frequency-based methods [10, 12, 13] \nconsider high-frequent nouns or noun phrases as aspect candidates. However, fre-\nquency-based approaches may miss low-frequent aspects. Several complex filter-based \napproaches are applied to solve this problem; however, the results are not as good as \nexpected because some aspects are still missed [14, 15]. Moreover, these methods face \ndifficulty in identifying implicit aspects. To overcome these problems, some supervised \nlearning techniques, such as the Hidden Markov Model (HMM) and Conditional Ran-\ndom Field (CRF) have been proposed. These techniques, however, require a set of manu-\nally labeled data for training the model and thus could be costly.\n\nThe problem of aspect extraction is solved by using aspect words based on conditional \nprobability combined with the bootstrap technique. It is assumed that the universal set \nof all possible aspects for each product are readily available together with aspect words \ncalled core terms (terms that describe aspects). This assumption is practical because \nthe number of important aspects is often small and can be easily obtained by domain \nexperts. The aspect extraction task then becomes how to correctly assign existing \naspects to sentences in the review. The main challenge here is that in many reviews, sen-\ntences do not contains enough core terms or even do not have any core term at all, and \nthus may be assigned with wrong aspects. This problem is solved by repeatedly updating \n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish-style cardamon coffee, brewed in a flared \ncopper stove-top pot like you see in Istanbul! But wow! This stuff is \namazing. \n\nDark without being bitter. Never acid at all, no matter how strong \nyou make it. So soft, so lovely. There’s a chocolate-like note, all warm \nand clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened condensed \nmilk as they suggest but it seems superfluous. Just drink it hot and strait\nand you will be very happy! \n\nFig. 1 Comment of Trung Nguyen coffee\n\n\n\nPage 3 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nand enlarging the set of core terms to the set of aspect words by using the conditional \nprobability technique combined with the bootstrap technique. This method leads to bet-\nter results of aspect extraction as shown in “Results and discussion” section.\n\nAfter the aspects are identified, inferring the user’s rating for them may bring more \nthorough understanding of the user’s satisfaction. A user usually gives an overall rat-\ning which express a general impression about a product. The overall rating is not always \ninformative enough. However, it can be assumed that the overall rating on a product \nis weighted sum of the user’s specific rating on multiple aspects of the product, where \n\nThree tasks\n\nExtracting \nAspects\n\nInferring \nAspect Rate\n\nEstimating Aspect \nWeight\n\nDark without \nbeing bitter.\n\nNever acid at \nall, no matter \nhow strong you \nmake it..\n\nSo soft, so \nlovely.\n\nThere’s a \nchocolate-like \nnote, all warm \nand clean, but \nnothing \nchocolate about \ntaste.\n\n“This is my new go-to \n\n“By MYOB on January 2, \n\nI am a big fan of Turkish -style cardamon coffee, brewed \nin a flared copper stove -top pot like you see in Istanbul! But \nwow! This stuff is amazing.\n\nDark without being bitter. Never acid at all, no matter how \nstrong you make it. So soft, so lovely. There’s a chocolate-like\nnote, all warm and clean, but nothing chocolate about taste.\n\nI drink it black, no cream or sugar. I tried it with sweetened \ncondensed milk as they suggest but it seems superfluous. \nJust drink it hot and strait and you will be very happy!\n\nBody: 5\n\nAroma: -\n\nTaste: 5\n\nAcidity: 4\n\nBody: 0.2\n\nAroma: 0\n\nTaste: 0.6\n\nAcidity: \n0.2\n\nDark , \nbitter\n\nAcid, \nstrong\n\nSoft, \nlovely\n\nChocol-\nate-like, \nnote, \nwarm, \nclean, \ntaste\n\nFig. 2 An example of aspect extracting, aspect inferring, and aspect weighting tasks\n\n\n\nPage 4 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nthe weights basically measure the degree of importance of the aspects. Some previous \nwork [16, 17] infer the user’s rating for aspects and estimate the weight of aspects at \nthe simultaneously based on regression methods and using only the review content and \nthe associated overall rating. Different approach is applied to infer rating and weight of \naspects. More specifically, the weight of an aspect is calculated by leveraging the aspect \nwords frequency within the review and the aspect consistency across all reviews. Then, \na supervised approach called the Naïve Bayes classification method is used to infer the \nuser’s rating for aspects. Despite the fact that the solution is relatively simple, its tested \naccuracy on different real-life datasets are comparable to much more sophisticated state \nof the art approaches as shown in “Results and discussion” section.\n\nThe Fig. 2 summaries the three tasks mentioned above. The methods for solving these \ntasks are discussed in details in “Method” section of this paper.\n\nThe rest of this paper is structured as follows. “Related work” section introduces \nrelated works. “Problem definition” and “Method” sections represent the proposed \nmethodology. “Results and discussion” section show experimental and evaluation of the \nproposed method. Finally, “Conclusion” section concludes the paper and gives some \nfuture research directions.\n\nRelated work\nDuring the last decade, many researches work has been proposed in the opinion mining \narea. Researchers are paying increasing attention to methods of extracting information \nfrom reviews that indicates users’ opinions of aspects about products. A survey on opin-\nion mining and sentiment analysis [18] shows that two important tasks of aspect-based \nopinion mining are aspect identification and aspect-based rating inference. The survey \nalso mentions some interesting methods for these tasks including frequency-based, lexi-\ncon-based, machine learning and topic modeling.\n\nMost of the earliest researches to identify aspects are frequency-based ones [11]. In \nthese approaches, nouns and noun phrases are considered as aspect candidates [10, \n12–15]. Hu and Liu [10] uses a data mining algorithm for nouns and noun phrases iden-\ntification and label assignment by the part-of-speech/POS [19]. Their occurrence fre-\nquencies are counted, and only the frequent ones are kept. A frequency threshold is used \nand can be decided via experimental. In spite of its simplicity, this method is actually \nquite effective. Some commercial companies are using this method with some improve-\nments to increase in their business [11]. However, producing “non-aspect” is the limita-\ntion of these methods because some nouns or noun phrases that have high-frequency \nare not really aspects.\n\nTo solve these problems, some improved methods of this filtering approach have been \nproposed. [15] augments the frequency-based approach with an additional pattern-\nbased filters to remove some non-aspect terms. A similar solution, [14] extracts aspects \n(nouns) based on frequency and information distance. Firstly, they find seed words for \neach aspect by using the frequency-based method. Secondly, they use the information \ndistance in [20] to find other related words to aspects, e.g., for aspect price, it may find \n“$” and “dollars”. However, the frequency-based and rule-based approaches require the \nmanual effort of tuning various parameters, which limits their generalization in practice.\n\n\n\nPage 5 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nTo deal with the limitations of frequency-based methods, in recent years, topic mod-\neling has emerged as a principled method for discovering topics from a large collection \nof texts. These researches are primarily based on two main basic models, pLSA (Prob-\nabilistic Latent Semantic Analysis) [21] and LDA (Latent Dirichlet allocation) [22]. In \n[4, 15, 23–25], the authors apply topic modeling to learn latent topics that correlate \ndirectly with aspects. [23] proposes a topic modeling for mining aspects. Firstly, they \nidentify aspects using topic modeling and then identify aspect-specific sentiment words \nby considering adjectives only. Lin et al. [4] proposes Joint Sentiment-Topic (JST) and \nReverse-JST. Both models were based on the modified Latent Dirichlet allocation (LDA). \nThese models can extract sentiment as well as positive and negative topic from the text. \nBoth JST and RJST yield an accuracy of 76.6% on Pang and Lee [7] dataset. While topic-\nmodeling approaches learn distributions of words used to describe each aspect, in [24], \nthey separate words that describe an aspect and words that describe sentiment about an \naspect. To perform, this study use two parameter vectors to encode these two proper-\nties, respectively. Then, a weighted bipartite graph is constructed for each review, which \nmatches sentences in review to aspects. Learning aspect labels and parameters are per-\nformed with no supervision (i.e., using only aspect ratings), weak supervision (using a \nsmall number of manually-labeled sentences in addition to unlabeled data), or with full \nsupervision (using only manually-labeled data). Moghaddam and Ester [15] devised fac-\ntorized LDA (FLDA) to extract aspects and estimate aspect rating. The FLDA method \nassumes that each user (and item) has a set of distributions over aspects and aspect-\nbased ratings. Their work on multi-domain reviews reaches to 74% for review rating on \nTripAdvisor data set. In [26], the authors propose a new method called Aspect Identi-\nfication and Rating model (AIR) for mining textual reviews and overall ratings. Within \nAIR model, they allow an aspect rating to influence the sampling of word distribution \nof the aspect for each review. This approach is based on the LDA model. However, dif-\nferent from traditional topic models, the extraction of aspects (topics) and the sampling \nof words for each aspect are affected by the sampled latent aspect ratings which are \ndependent on the overall ratings given by reviewers. Then, they further enhance AIR \nmodel to handle quite unbalance of aspects mentioned in short reviews.\n\nAlthough topic modeling is an approach based on probabilistic inference and it can be \nexpanded to many types of information models, it has some limitations that restrict their \nuse in real-life sentiment analysis applications. For example, it requires a huge amount of \ndata and a significant amount of tuning in order to achieve reasonable results. It is very \neasy to find those general and frequent topics or aspects from a large document collec-\ntion, but it is hard to find those locally frequent but globally that is not frequent aspects. \nSuch locally frequent aspects are often the most useful ones for applications because \nthey are likely to be most relevant to the specific entities that the user is interested in. In \nshort, the results from current topic modeling methods are usually not relevant or spe-\ncific enough for many practical sentiment analysis applications [11].\n\nBesides, some lexicon-based methods, which are also unsupervised approach, are pro-\nposed. Opinions are extracted with respect to each feature using the dictionary-based \napproach, which also yields polarity and strength. These methods use a dictionary \nof sentiment words and phrases with their associated orientations and strength. They \nare combined with intensification and negation to compute a sentiment score for each \n\n\n\nPage 6 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\ndocument [8]. Xiaowen Ding, Minqing Hu use sentence and aspect-level sentiment clas-\nsification [10, 27, 28]. Yan et al. [29] propose a method called EXPRS (An Extended Pag-\neRank algorithm enhanced by a Synonym lexicon) to extract product features. To do so, \nthey extract nouns/noun phrases first and then extract dependency relations between \nnouns/noun phrases and associated sentiment words. Dependency relations included \nsubject-predicate relations, adjectival modifying relations, relative clause modifying rela-\ntions, and verb-object relations. The list of product features was extended by using its \nsynonyms. Non-features nouns are removed on the basis of proper nouns, brand names, \nverbal nouns and personal nouns. Peñalver-Martinez et al. [30] developed a methodol-\nogy to perform aspect-based sentiment analysis of movie reviews. To extract the movie \nfeatures from the reviews, they make a domain ontology (Movie Ontology). SentiWord-\nNet is utilized to calculate the sentiment score. However, the critical issue here is how \nto construct such a sentiment lexicon, due to the cost of time and money to build such \ndictionaries.\n\nSentiment classification can be performed using machine learning approaches which \noften yield higher accuracy. Machine learning methods can be further divided into \nsupervised and unsupervised ones. For supervised methods, two sets of annotated data, \none for training and the other for testing are needed. Some of the commonly applied \nclassifiers for supervised learning are Decision Tree (DT), SVM, Neural Network (NN), \nNaïve Bayes, and Maximum Entropy (ME). In paper Asha et  al. [31], propose a Gini \nIndex based feature selection method with Support Vector Machine (SVM) classifier \nfor sentiment classification for large movie review data set. The Gini Index method for \nfeature selection in sentiment analysis has improved the accuracy. Another research, \nDuc-Hong Pham and Anh-Cuong Le [32] design a multiple layer architecture of knowl-\nedge representation for representing the different sentiment levels for an input text. This \nrepresentation is then integrated into a neural network to form a model for prediction \nof product overall ratings. These techniques, however, require a set of manually labeled \ndata for training the model and thus could be costly.\n\nProblem definition\nA user review i on some product is assumed containing two parts: the review’s text \ndenoted by di, and the review’s overall rating denoted by yi. Each review’s text di can \ncontain multiple sentences. Furthermore, each sentence contains multiple words coming \nfrom the universal set of all possible worlds V = {wk| k = 1, P} , called a word dictionary.\n\nIt is assumed further that for a product, the set of all possible K aspects is already \nknown together with topic words, called core terms that describe each aspect of the \nproduct.\n\nDefinition 1. Aspect An aspect is a feature (an attribute or a component) of a product. \nFor example, taste, aroma, and body are some possible aspects of the product “coffee”. We \nassume that there are K aspects mentioned in all reviews, denoted by A = {aj|j = 1, K} . \nAn aspect is represented by a set of words and denoted by aj = {w|w ∈ V ,A(w) = j} , \nwhere aj is the name of the aspect, w is a word from the set V , and A(.) is a operator that \nmaps a word to the aspect. For example, words such as “taste”, “aftertaste”, and “mouth \nfeel” can characterize the taste aspect of the product coffee.\n\n\n\nPage 7 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nDefinition 2. Aspect rating Given a review i, a K-dimensional vector ri ∈ R\nK is used to \n\nrepresent the rating of K aspects in the review’s text di, denoted by ri = (ri1 , ri2 , . . . , riK ) , \nwhere rij is a number indicating the user’s opinion assessment on aspect aj, and \nrij ∈ [rmin, rmax] (e.g., the range of rij can be from 1 to 5).\n\nDefinition 3. Aspect weight Given a review i, a K-dimensional vector αi ∈ R\nK is used. \n\nThe vector is denoted as αi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\n ) where αij is a number measuring the \ndegree of importance of aspect aj posed by the user, αij ∊ [0, 1], and \n\n∑K\nj=1 αij = 1 . A \n\nhigher weight means more emphasis is put on the corresponding aspect.\n\nDefinition 4. Aspect core terms Given an aspect aj, the set of associated core terms \nfor aj is denoted by Cj =\n\n{\n\nwj1, wj2, . . . ,wjN\n\n}\n\n where wjk is a word that describes the \naspect aj. The core terms can be provided by the user or by some field experts.\n\nMajor notations used throughout the paper are given in Table 1.\n\nExtracting aspect\n\nThe goal of this task is to extract aspects mentioned in a review. It is assumed that each \naspect is a probability distribution over words. It is also assumed that each sentence in \na review’s text can mention more than one aspect. Therefore, our method to extract \naspects is based on conditional probability of words such that each sentence can be \nassigned with multiple labels.\n\nInferring aspect rate\n\nThis task is to infer the vector ri of aspect ratings (defined in Definition 2) given a \nreview di. Rating of an aspect reflects the user’s sentiment on the aspect which is often \nexpressed in positive or negative words. The more positive words the user use, the higher \nrating he/she want to pose on the aspect. This research adopts a supervised learning \nmethod, the Naive Bayes method, to learn the aspect ratings in which sentiment words \nare considered as features.\n\nTable 1 Notations used in this paper\n\nNotation Description\n\nD =\n{\n\ndi |i = 1,Q\n}\n\nThe set of reviews’ text, where Q is the number of reviews\n\nY =\n{\n\nyi |i = 1,Q\n}\n\nThe set of overall rating, yi is overall rating corresponded with di\nA = {a1, a2, . . . , aK } The set of aspect, where K is the number of aspects\n\nCj =\n{\n\nwj1,wj2, . . . ,wjN\n\n}\n\nThe set of associated core terms for aspect aj, where N is the number of words\n\nV =\n{\n\nwk| k = 1, P\n}\n\nThe corpus of words, where P is the number of words\n\nSj =\n{\n\nsj1, sj2, . . . , sjM\n}\n\nThe set of sentences are assigned aspect aj, where M is the number of sentences\n\nTj =\n{\n\nwj1,wj2, . . . ,wjT\n\n}\n\nThe set of aspect words are aspect expressions, where Tj is the expression for aspect \naj, and T is the number of words\n\nij ∈ R\nK The aspect rating inferred from review di over K aspect, ri = (ri1 , ri2 , . . . riK)\n\nαi ∈ R\nK The aspect weights user places on K aspect within reviews’ text di, \n\nαi =\n(\n\nαi1 ,αi2 , . . . ,αiK\n)\n\nyi ∈ R\n+ The overall rating of review di\n\nrij The aspect rating on j-th aspect of review i, rij ∈ [1,5]\n\nαij The aspect weight of j-th aspect of review i, αij ∈ [0,1]\n\n\n\nPage 8 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nEstimating aspect weight\n\nThis task is to estimate non-negative weights αi that a user places on aspect aij of \nreview i. Weight of an aspect essentially measures the degree of importance posed \nby the user on the aspect. It is observed that people often talk more on aspects that \nthey are interested in a same review. Besides, the idea that an aspect is important is \noften shared by many other people. Based on these observations, a formula is devised \nto calculate aspect weight. The formula takes into account the occurrences of words \ndiscussing the aspect within a review and the frequency of text sentences discussing \nthe same aspect across all reviews.\n\nMethod\nExtracting aspect\n\nThe goal of this task is to assign a subset of aspect labels from the universal set of all \naspect labels of a product to every sentence in a review. Aspect label is determined \nbased on the set of relevant words called aspect words or terms. Each aspect in the \nuniversal label set is provided with some initial core terms. The main challenge here \nis that many reviews contain very few core terms or even do not contain any term at \nall. This results in incorrect labels being assigned to sentences. Therefore, it is required \nto expand the core terms to a richer set of aspect words based on the given data (the \nreviews). In some existing methods, the set of aspect words is built based on Bayes or \nHidden Markov Model. Our method use conditional probabilistic model [33] combined \nwith the Bootstrap technique to generate aspect words. Figure 3 illustrates four aspects \nof a coffee product represented by their corresponding aspect words, in which the sym-\nbol O represents core terms, the symbol X represents words appearing in the corpus. \nFor this coffee product four aspects body, taste, aroma, and acidity are already known. \n\naroma\n\nsmell\n\nflavor\n\ntaste\n\naftertaste\n\nmouthfeel \nfinishing\n\nbody\n\nacidity\n\nacid\n\nFig. 3 Core terms with aspects\n\n\n\nPage 9 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nThe sets of core terms corresponding to these aspects are {body}, {taste, aftertaste, fin-\nishing, mouthfeel}, {aroma, smell, flavor} and {acid, acidity}, respectively. Core terms are \nthen enlarged by inserting words that have high probability to appear in the same sen-\ntences that they occur. Sets of aspect words are represented by the four circles. These \ncircles may overlap, indicating that some aspect words may belong to different aspects.\n\nSuppose that A = {a1, a2, . . . , aK } is the set of K aspects. For each aj , a set of words \nthat appear in sentences labeled with aspect aj such that their occurrences exceed a \ngiven threshold is obtained. The set of words of two aspects can overlap, such that \nsome terms may belong to multiple aspects. First, sentences that contain at least one \nword in the original core terms of the aspect are located. Then, all words including \nnouns, noun phrases, adjectives, and adverbs that appeared in these sentences are \nfound. Words that occur more than a given threshold θ are inserted to the set of \naspect words. Words with maximum number of occurrences in the set of new-found \naspect words are added to the set of core terms. The new set of aspect words with \ncore terms excluded is used to find new sentences. The above-mentioned process is \nrepeated until no more new words are found.\n\nThe procedure for updating aspect words for an aspect aj is given below.\n\n\n\nPage 10 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nA bootstrapping algorithm to assign labels to sentences in the reviews is given below.\n\n\n\nPage 11 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nThe proposed Aspect Extraction Algorithm works as follows. First all reviews’ texts are \nsplit into sentences (step 2). Then, aspect labels from the set A of all labels are assigned \nto every sentence of the set D of reviews’ text based on the initial aspect core terms \n(step 3). Based on this initial aspect labeling, the set of aspect core terms and the set \nof aspect words for every aspect are updated (step 4). The labels for all sentences are \nupdated using the new core terms and the aspect words sets (step 5). Step 4 and step 5 \nare repeated until no more new aspect word set are found or the number of iterations \nexceeds a given threshold.\n\nInferring aspect rating and estimating aspect weight\n\nAspect ratings often reflect the user’s satisfaction on aspects of a product. Meanwhile, \naspect weights measure the degree of importance of the aspects posed by the user. Given \nthe overall rating on a product, it is assumed that the overall rating is the weighted sum \nof rating on multiple aspects of the product. Following this assumption, some regres-\nsion-based methods [16, 17, 34] have been proposed to estimate the two parameters by \nsolving the following equation:\n\nwhere rij and αij are the rating and the weight of k-th aspect of the review i, respectively.\nThere are linear regression methods [35] which estimate only the aspect weight and \n\nrequire that the aspect ratings are available. Some other methods [17, 34] estimate both \naspect’s rating and weight at the same time. The key point of these methods is to use sen-\ntiment words, more specifically the polarity of sentiment words, to calculate ratings and \nweights. Even though sentiment words can usually correctly reflect the user’s rating for \neach aspect, they do not always reflect the user’s opinion about an aspect’s weight.\n\nAspect rating and aspect weight of an aspect are estimated separately. An important \npoint in our method is that aspect rating and aspect weight are calculated based on the \nreview content only, without the requirement of knowing the user’s overall rating. How-\never, in “Results and discussion” section, Eq.  (1) is still used to test our method. It is \nshown experimentally that our results conform well to the assumption that the overall \nrating is the weighted sum of rating on multiple aspects.\n\nThe aspect rating problem is treated as the problem of multi-label classification, in which \nratings (from 1 to 5) as considered as labels, and sentiment words are used as features. \nIn most sentiment analysis work, adjectives and adverbs are used as candidate sentiment \nwords. Adjectives and adverbs are detected based on the well-known Part of Speech tech-\nnique (POS). It is recognized that some phrases can also be used to express sentiments \ndepending on different contexts. For example, in the following two sentences “we have big \nproblem with staff”, and “we have a big room”, the two noun phrases “big problem” and “big \nroom” convey opposite sentiments, negative vs. positive, while both phrases contain the \nsame adjective “big”. Some fixed syntactic patterns in [9] as phrases of sentiment word fea-\ntures are used. Only fixed patterns of two consecutive words in which one word is an adjec-\ntive or an adverb and the other provides a context are considered.\n\n(1)yi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n\n\nPage 12 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nTwo consecutive words are extracted if their POS tags conform to any of the rules in \nTable 2 in which JJ tags are adjectives, NN tags are nouns, RB tags are adverbs, and VB \ntags are verbs. For example, rule 2 in this table means that two consecutive words are \nextracted if the first word is an adverb, the second word is an adjective, and the third \nword (which is not extracted) is not a noun. As an example, in the sentence “Quite dry, \nwith a good grassy note”, two patterns “quite dry” and “good grassy” are extracted as they \nsatisfy the second and the third rules, respectively. Then, conditional probability of word \nfeatures in the corpus is determined. Label (scoring) for each aspect is predicted based \non Naïve Bayes method.\n\nGiven a review’s text di, the rating of an aspect aj with q extracted features is inferred \nbased on the probability rij that the rating label belongs to class c ∈ C = {1, 2, 3, 4, 5}. The \nprobability is as:\n\nIt is assumed that the features are independent, then (2) is transformed into:\n\nin which: P\n(\n\nfk |rij ∈ c\n)\n\n= naj\n(\n\nfk , c\n)\n\n/naj(c) is the probability that feature fk belongs to the \n\nclass c, naj(fk, c) is the number of sentences labeled as c of the aspect aj which contains \nthe feature fk, and naj(c) is the number of all sentences containing the aspect aj and has \nclass label c,\nP(rij ∈ c)= naj(c)/naj is the probability that the rating rij belongs to the class c, naj(c) is \n\nthe number of sentences labeled as c of aspect aj, and naj is the number of all sentences \ncontaining the aspect aj,\n\nP(fk) is the probability of feature fk.\nFor smoothing (3), Laplace transformation is used. We get:\n\nin which, |V| is number of word features regarding the aspect aj.\nThe rating rij is the label c that maximize P(rij ∈ c|f1, . . . , fq).\n\n(2)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\nP\n(\n\nf1, . . . , fq|rij ∈ c\n)\n\nP\n(\n\nrij ∈ c\n)\n\nP\n(\n\nF1, . . . , Fq\n)\n\n(3)P\n(\n\nrij ∈ c|f1, . . . , fq\n\n)\n\n=\n\n∏q\nk=1 P(fk |rij ∈ c)P\n\n(\n\nrij ∈ c\n)\n\n∑q\nk=1 P\n\n(\n\nfk\n)\n\n(4)P\n(\n\nfk |rij ∈ c\n)\n\n=\nnaj\n\n(\n\nfj , c\n)\n\n+ 1\n\nnaj(c)+ |V | + 1\n\nTable 2 POS labeled rules [9]\n\nThe first word The second word The third word \n(non extracted)\n\n1. JJ NN or NNS Any word\n\n2. RB, RBR, or RBS JJ Not NN nor NNS\n\n3. JJ JJ Not NN nor NNS\n\n4. NN or NNS JJ Not NN nor NNS\n\n5. RB, RBR, or RBS VB, VBD, VBN, or VBG Any word\n\n\n\nPage 13 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nNow the method to estimate aspect weight is given. By doing research carefully through-\nout the reviews, it can be seen that if a user care more about an aspect (showing that the \naspect is important to the user), he/she will mention more about it in the review. Moreover, \nthe idea that an aspect is important is often shared by many other users. Following this \nobservation, we estimate aspect weights by calculating two components: the weight meas-\nure of aspect aj within the reviews’ text di, denoted by EDij, and the weight measure of the \naspect across all reviews, denoted by ECj. Note that in this way, the polarity measures of \nsentiment words are not used as in some other approaches. Instead, probability measures of \nwords and sentences regarding an aspect in the review and the corpus are considered. This \nidea is similar to the idea of using tf/idf for measuring word importance to some extent.\n\nGiven a review i, the weight component of the aspect aj, EDij, is calculated as:\n\nIn which: wijk is the k-th word in the aspect words of aspect aj , and Ni is the number of \naspect words that occur in the review’s text di for all aspects.\n\nThe weight component ECj is calculated as:\n\nIn which: sjk is the k-th sentence in the corpus labeled by the aspect aj , and M is the \nnumber of all sentences in the corpus.\n\nFinally, the weight αij for an aspect aj of review i is calculated as:\n\nThe denominator \n∑K\n\nj=1 EDijECj is to normalize the value of αij to the range [0,1].\n\nResults and discussion\nIn this section, experiments to evaluate the proposed methods are conducted.\n\nData set\n\nThe experiments are carried out using three different data sets including a data set for \nhotel review collected from Tripadvisor.com [17], one data set for beer review used in \n[24], and a data set for Trung Nguyen coffee review collected by our self from the Ama-\nzon web site.\n\nThe Hotel data set contains seven different aspects that are room, location, cleanliness, \ncheck-in/front desk, service and business services. The beer data set has five distinct \naspects that are aroma (or smell), palate (or feel), taste, appearance (or look), and over-\nall. This data set is quite big with millions of reviews. A subset of 50,000 beer reviews is \n\nĉ = argmaxc∈C\n\nq\n∏\n\nk=1\n\nP(fk |rij ∈ c)P\n(\n\nrij ∈ c\n)\n\n.\n\n(5)EDij =\n\n∑Ni\n\nk=1 wijk\n\nNi\n.\n\n(6)ECj =\n\n∑M\nk=1 sjk\n\nM\n\n(7)αij =\nEDij ∗ ECj\n\n∑K\nj=1 EDij ∗ ECj\n\n\n\nPage 14 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nused in the experiment. The coffee data set contains 1200 reviews belongs to 17 different \nkinds of coffee. Table 3 gives some statistics of the three data sets.\n\nInferring aspect rating task\n\nNote that each review may be assigned with different labels. This means that sentence \nlevel, not review level is considered. Testing sets of 2500, 2000, and 500 sentences are \nselected randomly from the hotel data set, beer data set, and coffee data set, respectively. \nThe rest of sentences are used as the training sets.\n\nTable 4 gives initial core terms for the three data sets.\nThe precision measure is used to evaluate the experimental results:\n\nTable  5 shows the performance of our method on three data sets for the aspect \nextraction task. Our method yields up to average precision of 0.786, 0.803 and 0.653 \nfor hotel data set, beer data set and coffee data set, respectively. Our method obtains \ngood performance on the hotel and beer data set. However, for the coffee data set, \nthe result is not as good as expected. This is because in the coffee data set, users often \ngive only general view about a product, and moreover, the data set contains mostly \n\n(8)P =\n\n∣\n\n∣extrating Aspect ∩ True Aspect\n∣\n\n∣\n\n∣\n\n∣extracting Aspect\n∣\n\n∣\n\nTable 3 Summary of the Data Set\n\nHotel dataset Beer dataset Coffee dataset\n\n#Reviews 193,661 50,000 1200\n\n#Sentences 1,790,880 509,320 5289\n\n#Avg. sentences per review 9.25 10.19 4.41\n\nTable 4 Seed word for main aspects\n\nCategory Aspect Seed words\n\nHotel Value Value, price, worth\n\nRoom Room, rooms\n\nLocation Location\n\nCleanliness Dirty, smelled, clean\n\nCheck in/front desk Staff\n\nService Service, breakfast, food\n\nBusiness service Internet, wifi\n\nBeer Appearance Appearance, color, colors, coloring, head, foam\n\nAroma Aroma, aromas, smell, smelling\n\nPalate Palate, mouth, feel, mouth feel\n\nTaste Taste, tastes, aftertaste, in the end, finish, finishing\n\nOverall Overall\n\nCoffee Aroma Aroma, aromas, smell, smelling, flavor, flavors\n\nTaste Taste, tastes, aftertaste, finish, finishing, mouth feel\n\nAcidity Acid, acidity\n\nBody Body, aged, vintage\n\n\n\nPage 15 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nvery short reviews, with average number of sentences of 4.5, compared to 10 and 9 of \nthe hotel data set and the beer data set.\n\nOur method is compared with other works. First, our method is compared with the \nfrequency-based method in [14] on the hotel dataset. Figure 4 shows that our method \noutperforms Long’s in room (R), service (S), and cleanliness (C) aspects. But Long’s \nmethod outperforms us in detecting the value (V) aspect.\n\nOur method is compared with two topic modeling-based methods in [22] and in \n[24] on the beer data set. The method in [22] is a semi-supervised method, called \n\nTable 5 Aspect Identification results\n\nCategory Aspect Precision\n\nHotel Value 0.747\n\nRoom 0.837\n\nLocation 0.814\n\nCleanliness 0.764\n\nCheck in/front desk 0.850\n\nService 0.754\n\nBusiness service 0.737\n\nAverage 0.786\n\nBeer Appearance 0.750\n\nAroma 0.857\n\nPalate 0.857\n\nTaste 0.848\n\nOverall 0.704\n\nAverage 0.803\n\nCoffee Aroma 0.667\n\nTaste 0.677\n\nAcidity 0.667\n\nBody 0.600\n\nAverage 0.653\n\nV R S C AVER\n\nLong,Zhang, and Zhu 0.759 0.776 0.746 0.750 0.758\n\nOur Method 0.747 0.837 0.754 0.764 0.776\n\n0.700\n\n0.720\n\n0.740\n\n0.760\n\n0.780\n\n0.800\n\n0.820\n\n0.840\n\n0.860\n\nPr\nec\n\nis\nio\n\nn \n\nAspect\n\nHotel \n\nFig. 4 The results of our method and Long et al. method\n\n\n\nPage 16 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nLDA. In [24], the authors give 3 different methods, namely, unsupervised, semi-super-\nvised, and fully supervised methods. As our method can be considered as a semi-\nsupervised method, it is compared with PALE LAGER, a semi-supervised method, \nand with PALE LAGER, a supervised method given in [24].\n\nThe results in Fig.  5 shows that our method outperforms LDA with a large mar-\ngin, and slightly outperforms PALE LAGER (a semi-supervised method) and PALE \nLAGER (a supervised method).\n\nWe then search for the best threshold θ at which our method performs the best. The \nresults are shown in Fig. 6, where the threshold θ of about 0.15 is the best one.\n\nAspect ranking prediction\n\nUnlike the evaluation of the aspect extraction task that is done based on the sentence level, \nin this task, the result based on the review level is evaluated.\n\nThe mean square error measure (named �2\naspect ) is used for evaluating methods of min-\n\ning aspect rating.\n\nwhere K is the number of aspects, Q is the number of reviews, and r∗ij is the true ratings \n\nfor aspect aj within review’s text di.\nTo evaluate how well the predicted aspect ratings can preserve their relative order within \n\na review given the true ratings, the aspect correlation measure (named ρaspect ) is used:\n\n(9)�2\naspect =\n\n∑Q\ni=1\n\n∑K\nj=1\n\n(\n\nrij − r∗ij\n\n)2\n\nQ × K\n\n(10)ρaspect =\n\n∑Q\ni=1 ρri ,r∗i\n\nQ\n\n0.3000.320\n0.240\n\n0.750\n0.800 0.803\n\n0.000\n\n0.500\n\n1.000\n\nPr\nec\n\nis\nio\n\nn\n\nBeeradvocate\n\nLDA, K topics, semi-supervised\n\nLDA, 10 topics, semi-supervised\n\nLDA, 50 topics, semi-supervised\n\nPALE LAGER, semi-supervised\n\nPALE LAGER, fully-supervised\n\nOur Method\n\nFig. 5 The results of our method and LDA, PALE LAGER\n\n\n\nPage 17 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nwhere Q is the number of reviews, and ρri ,r∗i is the Pearson correlation between two vec-\ntors ri and r∗i of the inferred and the true ratings, respectively.\n\nThe two measures above are for evaluating the results for each review. The results on the \nwhole set of reviews are evaluated by using the so called aspect correlation across reviews \nmeasure ( ρreview):\n\nwhere ρ\n(\n\n−→rj ,\n−→\nr∗j\n\n)\n\n is the Pearson correlation between two vectors −→rj and \n−→\nr∗j of the inferred \n\nand rating.\nOur method is also compared with Long’s [14] and Wang’s [17]. Long proposed \n\ntwo methods based on the SVM classifier and the Bayesian Network classifier. Wang’s \nmethod is called Latent Rating Regression (LRR) which infers aspect ratings and aspect \nweights simultaneously.\n\nThe performance results are shown in Table  6. Our method performs much better \nthan Long’s method and Wang’s method on all three measures.\n\nEstimating aspect weight\n\nFor evaluating the correctness of estimated weights by our method, the overall rating is \ncalculated and compared with the true overall rating given by the user. The estimated \noverall rating is given by the following formula:\n\nwhere rij is the rating of the j-th aspect of the review i and αij is the estimated weight.\n\n(11)ρreview =\n\n∑K\nj=1 ρ\n\n(\n\n−→rj ,\n−→\nr∗j\n\n)\n\nK\n\n(12)ŷi =\n\nK\n∑\n\nj=1\n\nrijαij\n\n0.000\n\n0.200\n\n0.400\n\n0.600\n\n0.800\n\n1.000\n\n0.05 0.08 0.1 0.15 0.2 0.3 0.4 0.5\n\nPr\nec\n\nis\nio\n\nn\n\nAspect Evaluation with θ\n\nAppearance Aroma taste\n\npalate overall\n\nFig. 6 Aspect evaluation with θ\n\n\n\nPage 18 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nOur method is compared with Wang’s method [17] based on the �2\noverallrating . Table 7 \n\npresents the mean square errors of overall rating for the three data sets. As can be seen \nin the table, our results are comparable to Wang’s.\n\nConclusion\nThis paper dealed with three important sub-tasks of the opinion mining problem, that \nare (1) extracting aspects mentioned in the reviews of a product by using conditional \nprobability of words, (2) inferring the user’s rating for each identified aspect based on \nNaïve Bayes classifier, (3) estimating the weight placed on each aspect by the users by \nusing the occurrences of word that discuss the aspect within a review and the frequency \nof text sentences that discuss the same aspect across all reviews.\n\nOur method does not require to know the overall ratings and is as not complicated as \nsome other previous methods. However, it still works very well on real world datasets in \ncomparison with other state of the art methods.\n\nIn the future, the problem of aspect mining from unlabeled data will be considered. \nIn addition, the proposed model will be applied to other domains such as movie, digital \ncamera businesses to validate its generalized effectiveness.\n\nAbbreviations\npLSA: Probabilistic Latent Semantic Analysis; LDA: Latent Dirichlet allocation; HMM: Hidden Markov Model; CRF: Condi-\ntional Random Field.\n\nAuthors’ contributions\nTNTN proposed method and performed experiments, HNTT supervised the programming and wrote draft manuscript. \nVAN wrote a part of the manuscript and corrected after received reviews. All authors read and approved the final \nmanuscript.\n\nAuthor details\n1 Department of E-Commerce, Vietnam Electric Power University, 235 Hoang Quoc Viet, Hanoi, Vietnam. 2 Institute \nof Information Technology, Vietnam Academy of Science and Technology, Hanoi, Vietnam. \n\nAcknowledgements\nThis research is funded by the project “Building a System for Prediction and Management of Information Spreading in \nSocial Networks in Vietnam” under Grant VAST01.01/17-18\n\nCompeting interests\nData mining, big data, machine learning and natural language processing.\n\nTable 6 Comparison with other models for referring aspect ratings\n\nMethod �\n2\naspect\n\nρaspect ρreview\n\nLong et al. with SVM 0.286 0.557 0.708\n\nLong et al. with BN 0.441 0.429 0.591\n\nLRR 0.896 0.464 0.618\n\nOur method 0.101 0.583 0.757\n\nTable 7 MSE of overall rating prediction\n\nMethod Product datasets\n\nHotel Beer Coffee\n\nLRR 0.905 0.856 1.234\n\nThe proposed method 0.1456 0.1423 0.1904\n\n\n\nPage 19 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nFunding\nNot applicable.\n\nAvailability of data and materials\nAll data used in this study are publicly available and accessible in the source Tripadvisor.com.\n\nAppendix\nSee Tables 8, 9, 10.\n\nTable 8 Aspect word set of Hotel data\n\nAspect Aspect words\n\nValue Hotel, charge, cost, discount, dollars\n\nRoom Bathroom, bathrooms, bed, beds, bath, floor, floors, chair, chairs, balcony, \nshower, lobby, noise, pool, queen, couple, Sheraton, coffee, desk, hotel, \nsuite, tv, view, water, window, carpet, closet, doors, furniture, king, pillows, \nsink, toilet, tub, toiletries, …\n\nLocation Airport, area, center, downtown, hotel, market, place, places, restaurant, \nshop, shops, shopping, show review, street, view, views, neighborhood, \nsquare, waterfront, …\n\nCleanliness Hotel, floor, shelf, desk, chair, bag, door, lobby, stairs, …\n\nCheck in/front desk Desk, clerk, lounge, luggage, reception, checkout, …\n\nService Bar, bars, coffee, concierge, food, park, parking, restaurant, wine, buffet, …\n\nBusiness service Tv, television, wireless, hotwire, cable, computer, connection, free, freeway, \n…\n\nTable 9 Aspect word set of Beer data\n\nAspect Aspect words\n\nAppearance Black, Body, brown, bubble, bud, copper, lace, lacing, dots, drip, dust, back, finger, fizzy, fluff, \ngolden, half, layer, orange, straw, surface, top, white, yellow…\n\nAroma Bacon, banana, basil, caramel, cheese, cream, dry citrus, fruitiness, honey, light, malt, malts, meat, \nmint, nose, pear, perfume, pill, pine, roast, sandalwood, smoke, smoky, spice, sweet, sweetness, \nyeast…\n\nPalate Alcohol, body, carbonation, cream, Drinkability, dry, hoppy, light, round, spring, summer, …\n\nTaste Alcohol, avalanche, balance, bitterness, body, bread, burn, caramel, carbonation, cheese, chocolate, \nclear, cocoa, coffee, complexity, flavors, flavors, fruit, fruitiness, ginger, grains, malt, matiness, \nmeat, Medium-dry, oats, pear, roast, smoke, smoothness, spring, subtleties, summer, sweet, \nthroat, toffee, tongue, vanilla, wood, …\n\nOverall Beer, beers, bottle, drink, beverage, style, glass, sense, quality, brewery, level, lace, brewpub …\n\nTable 10 Aspect word set of Coffee data\n\nAspect Aspect words\n\nAroma Bran, brew, butter, charr, chocolate, citrus, fruit, \nhoney, love, lover, organic, press, quality, \nsmooth, lemon, smoke, stuff, …\n\nTaste Bitter, bitterness, chocolate, honey, salt, fresh-\nness, brew, love, lover, mild, organic, press, \nquality, roaster, smooth, soft, sour, stuff, sweet, \nsweetness, syrup, …\n\nAcidity Acidic, acidness, sourness, …\n\nBody Love, press, smooth, richness, thick, thin, soft, …\n\n\n\nPage 20 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 12 December 2017 Accepted: 14 February 2019\n\nReferences\n 1. Park S, Lee K, Song J. Contrasting opposing views of news articles on contentious issues. In: Proceedings of the 49th \n\nannual meeting of the association for computational linguistics (ACL-2011). 2011.\n 2. van den Camp M, van den Bosch A. The socialist network. Decis Support Syst. 2012;53:761–9.\n 3. Li SK, Guan Z, Tang LY, et al. Exploiting consumer reviews for product feature ranking. J Comput Sci Technol. \n\n2012;27(3):635–49. https ://doi.org/10.1007/s1139 0-012-1250-z.\n 4. Lin C, He Y, Everson R, Ruger S. Weakly supervised joint sentiment-topic detection from text. IEEE Trans Knowl \n\nData Eng. 2012;24(6):1134–45.\n 5. Zhan J, Loh HT, Liu Y. Gather customer concerns from online product reviews—a text summarization approach. \n\nExpert Syst Appl. 2009;36:2107–15.\n 6. Dang Y, Zhang Y, Chen H. A lexicon-enhanced method for sentiment classification: an experiment on online \n\nproduct reviews. IEEE Intell Syst. 2010;25(4):46–53.\n 7. Pang B, Lee L. A sentiment education: sentiment analysis using subjectivity summarization based on minimum \n\ncuts. In: Proceedings of the 42nd annual meeting on association for Computational Linguistics. 2004. p. 271.\n 8. Taboada M, Brooke J, Tofiloski M, Voll K, Stede M. Lexicon-based methods for sentiment analysis. Comput Lin-\n\nguistics. 2011;37(2):267–307.\n 9. Turney PD. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. \n\nIn: ACL ‘02 Proceedings of the 40th annual meeting on association for computational linguistics. p. 417–24.\n 10. Hu M, Liu B. Mining and summarizing customer reviews. In: Proceedings of the Tenth ACM SIGKDD international \n\nconference on knowledge discovery and data mining, KDD’04, New York: ACM; 2004, p. 168–77.\n 11. Liu B. Sentiment analysis and opinion mining. Synth Lect Human Lang Technol. 2012;5(1):1–67.\n 12. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. In: HLT ‘05 Proceedings of \n\nthe conference on Human Language Technology and Empirical Methods in Natural Language Processing. p. \n339–46.\n\n 13. Zhu J, Wang H, Tsou BK, Zhu M. Multi-aspect opinion polling from textual reviews. In: Proceedings of ACM interna-\ntional conference on information and knowledge management (CIKM-2009). 2009.\n\n 14. Long C, Zhang J, Zhut X. A review selection approach for accurate feature rating estimation. In: Proceedings of Col-\ning 2010: Poster volume. 2010.\n\n 15. Moghaddam S, Ester M. Opinion digger: an unsupervised opinion miner from unstructured product reviews. In: \nProceeding of the ACM conference on Information and knowledge management (CIKM-2010). 2010.\n\n 16. Chen Li, Wang Feng. Preference-based clustering reviews for augmenting e-commerce recommendation. Knowl \nBased Syst. 2013;50:44–59.\n\n 17. Wang H, Lu Y, Zhai C. Latent aspect rating analysis on review text data: a rating regression approach. In: Proceedings \nof the 16th ACM SIGKDD international conference on knowledge discovery and data mining, KDD’10, New York: \nACM; 2010. p. 783–92.\n\n 18. Ravi K, Ravi V. A survey on opinion mining and sentiment analysis: tasks, approaches and applications. Knowl Based \nSyst. 2015;89:14–46.\n\n 19. Santorini B. Part-of-speech tagging guidelines for the Penn Treebank Project, University of Pennsylvania, School of \nEngineering and Applied Science, Dept. of Computer and Information Science. 1990.\n\n 20. Cilibrasi RL, Vitanyi PMB. The google similarity distance on Knowledge and Data Engineering, IEEE transactions. 2007; \n370–83.\n\n 21. Hofmann T. Probabilistic latent semantic indexing. In: Proceedings of the 22nd annual international ACM SIGIR \nconference on Research and development in information SIGIR’99. New York: ACM; 1999. p. 50–7.\n\n 22. Blei DM, Ng AY, Jordan MI. Latent dirichlet allocation. J Mach Learn Res. 2003;3:993–1022.\n 23. Brody S, Elhadad N. An unsupervised aspect-sentiment model for online reviews. In: Human language technolo-\n\ngies: the annual conference of the north american chapter of the association for computational linguistics, HLT’10, \nStroudsburg; 2010. p. 804–12.\n\n 24. McAuley J, Leskovec J, Jurafsky D. Learning attitudes and attributes from multi-aspect review. In: International \nconference on data mining (ICDM). 2012.\n\n 25. Sauper C, Barzilay R. Auto-matic aggregation by joint modeling of aspects and values. J Artif Int Res. \n2013;46(1):89–127.\n\n 26. Li H, Lin R, Hong R, Ge Y. Generative models for mining latent aspects and their ratings from short reviews. In: 2015 \nIEEE international conference on data mining. p. 241–50.\n\n 27. Ding X, Liu B, Yu PS. A holistic lexicon-based approach to opinion mining. In: Proceedings of the conference on web \nsearch and web data mining (WSDM-2008). 2008.\n\n 28. Kim SM, Hovy E. Determining the sentiment of opinions. In: Proceedings of international conference on computa-\ntional linguistics (COLING-2004).\n\n 29. Yan Z, Xing M, Zhang D, Ma B. EXPRS: an extended pagerank method for product feature extraction from online \nconsumer reviews, Inf. Manage. 2015.\n\n 30. Penalver-Martinez I, Garcia-Sanchez F, Valencia-Garcia R, Rodriguez-Garcia MA, Moreno V, Fraga A, Sanchez-Cer-\nvantes JL. Feature-based opinion mining through ontologies. Expert Syst Appl. 2014;41(13):5995–6008.\n\nhttps://doi.org/10.1007/s11390-012-1250-z\n\n\nPage 21 of 21Nguyen Thi Ngoc et al. J Big Data (2019) 6:22 \n\n 31. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \nreviews using Gini Index feature selection method and SVM classifier. World Wide Web. 2017;20(2):135–54.\n\n 32. Pham DH, Le AC. Learning multiple layers of knowledge representation for aspect based sentiment analysis. Data \nKnowl Eng. 2017;114:26–39.\n\n 33. Dao TT, Thanh TD, Hai TN, Ngoc VH. Building Vietnamese topic modeling based on core terms and applying in text \nclassification. In: Proc. of the fifth IEEE international conference on communication systems and network technolo-\ngies. 2015. P. 1284–88.\n\n 34. Yu J, Zha ZJ, Wang M, Chua TS. Aspect ranking: identifying important product aspects from online consumer \nreviews. In: Proceedings of the 49th Annual meeting of the association for computational linguistics: human \nlanguage technologies. Volume 1, HLT’11, Stroudsburg, PA, USA, 2011. Association for Computational Linguistics. p. \n1496–505.\n\n 35. Archak N, Ghose A, Ipeirotis PG. Show me the money!: Deriving the pricing power of product features by mining \nconsumer reviews. In: Proceedings of the 13th ACM SIGKDD in-ternational conference on Knowledge discovery and \ndata min-ing, KDD’07, New York: ACM; 2007. p. 56–65.\n\n\n\tMining aspects of customer’s review on the social network\n\tAbstract \n\tIntroduction\n\tRelated work\n\tProblem definition\n\tExtracting aspect\n\tInferring aspect rate\n\tEstimating aspect weight\n\n\tMethod\n\tExtracting aspect\n\tInferring aspect rating and estimating aspect weight\n\n\tResults and discussion\n\tData set\n\tInferring aspect rating task\n\tAspect ranking prediction\n\tEstimating aspect weight\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNTM3LTAxOS0wMTg0LTUucGRm0", "metadata_author": "Tu Nguyen Thi Ngoc ", "metadata_title": "Mining aspects of customer’s review on the social network", "metadata_creation_date": "2019-02-26T14:27:28Z", "keyphrases": [ "Mining aspects", "social network", "customer", "review" ] }, { "@search.score": 1, "content": "\nRESEARCH Open Access\n\nAugmented reality virtual glasses try-on\ntechnology based on iOS platform\nBoping Zhang\n\nAbstract\n\nWith the development of e-commerce, network virtual try-on, as a new online shopping mode, fills the gap that\nthe goods cannot be tried on in traditional online shopping. In the work, we discussed augmented reality virtual\nglasses try-on technology on iOS platform to achieve optimal purchase of online glasses, improving try-on speed of\nvirtual glasses, user senses of reality, and immersion. Face information was collected by the input device-monocular\ncamera. After face detection by SVM classifier, the local face features were extracted by robust SIFT. Combined with\nSDM, the feature points were iteratively solved to obtain more accurate feature point alignment model. Through\nthe head pose estimation, the virtual model was accurately superimposed on the human face, thus realizing the\ntry-on of virtual glasses. The above research was applied in iOS glasses try-on APP system to design the try-on system\nof augmented reality virtual glasses on iOS mobile platform. It is proved that the method can achieve accurate\nidentification of face features and quick try-on of virtual glasses.\n\nKeywords: Virtual try-on, Virtual glasses, Augmented reality, Computer vision, Pose estimation, iOS\n\n1 Introduction\nNetwork virtual try-on is a new way of online shopping.\nWith the development of e-commerce, it broadens the\nexternal propaganda channels of merchants to enhance\nthe interaction between consumers and merchants.\nVirtual try-on fills the gap that the goods cannot be\ntried on in traditional online shopping. As an important\npart of network virtual try-on, virtual glasses try-on\ntechnology has become a key research issue in this field\nrecently [1–4]. During virtual glasses try-on process,\nconsumers can select their favorite glasses by compar-\ning the actual wearing effects of different glasses in the\nonline shopping. The research key of virtual glasses\ntry-on system is the rapid achievement of experiential\nonline shopping.\nAR (augmented reality) calculates the position and\n\nangle of camera image in real time while adding corre-\nsponding images. The virtual world scene is superim-\nposed on a screen in real world for real-time\ninteraction [5]. Using computer technology, AR simu-\nlates physical information (vision, sound, taste, touch,\netc.) that is difficult to experience within certain time\n\nand space of real world. After superimposition of phys-\nical information, the virtual information is perceived by\nhuman senses in real world, thus achieving sensory ex-\nperience beyond reality [6].\nBased on AR principle, virtual glasses try-on technol-\n\nogy achieves optimal purchase of user online glasses and\nquick try-on of virtual glasses, improving the senses of\nreality and immersion. Monocular camera is used as the\ninput device to discuss try-on technology of AR glasses\non iOS platform. Face information is collected by mon-\nocular camera. After face detection by SVM (support\nvector machine) classifier, the local features of faces are\nextracted by robust SIFT (scale-invariant feature trans-\nform). Combined with SDM (supervised descent\nmethod), the feature points were iteratively solved to ob-\ntain more accurate feature point alignment model.\nThrough the head pose estimation, the virtual glasses\nmodel was accurately superimposed on the human face,\nthus realizing the try-on of virtual glasses. The above re-\nsearch is applied in iOS glasses try-on APP system to de-\nsign the try-on system of AR glasses on iOS mobile\nplatform. It is proved that the method can achieve ac-\ncurate identification of face features and quick try-on of\nvirtual glasses.Correspondence: bopingzhang@yeah.net\n\nSchool of Information Engineer, Xuchang University, Xuchang 461000,\nHenan, China\n\nEURASIP Journal on Image\nand Video Processing\n\n© The Author(s). 2018 Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and\nreproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 \nhttps://doi.org/10.1186/s13640-018-0373-8\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13640-018-0373-8&domain=pdf\nhttp://orcid.org/0000-0001-7835-7622\nmailto:bopingzhang@yeah.net\nhttp://creativecommons.org/licenses/by/4.0/\n\n\n2 Research status of network virtual try-on\ntechnology\nGlasses try-on system was first applied in the USA.\nGlasses companies such as Camirror, Smart Look, Ipoint\nKisok, and Xview pioneered the online try-on function [7].\nUsers freely feel the wearing effect, enhancing the online\nshopping experience. Recently, online try-on function is\nexplored by domestic and foreign glasses sellers, such as\nMeijing [8], Kede [9] and Biyao [10].\nVirtual glasses try-on system involves computer vision,\n\naugmented reality, and image processing technology.\nRecently, research hotspots are speed, experience, and\nimmersion of try-on. At present, research results can be di-\nvided into four categories, namely 2D image superposition,\n3D glasses superimposed on 2D face images, 3D face mod-\neling, and AR technology based on video stream [11–14].\nHuang [15] introduced virtual optician system based on\n\nvision, which detects user’s face before locating user’s eyes.\nThree points are selected from face and glasses images.\nTwo corresponding isosceles triangles are formed for af-\nfine transformation, thus estimating the pose and scale of\nface in real time. This method realizes real-time head mo-\ntion tracking. However, the glasses model easily produces\nunrealistic deformation, affecting the realism of the\nglasses.\nAR technology is also applied in the virtual glasses\n\ntry-on system. Cheng et al. [16] selected a monocular\nCCD (charge-coupled device) camera as the input sensor\nto propose AR technology design based on the inter-\naction of marker and face features. Virtual glasses try-on\nsystem is established based on Android mobile platform,\nachieving good results. During virtual try-on process, we\nuse 2D image overlay or 3D modeling approach. There\nare still different defects although all kinds of virtual\nglasses try-on techniques have certain advantages. The\nsuperposition of 2D images is unsatisfactory in the sense\nof reality. Besides, the 3D modeling takes too long to\nmeet the real-time requirements of online shopping.\n\nIn-depth research is required to realize accurate tracking\nand matching. These problems can be solved by\nAR-based glasses try-on technology to a large extent,\nthus providing new ideas for virtual try-on technology.\n\n3 Methods of face recognition\nIt is necessary to integrate virtual objects into real envir-\nonment for the application of AR technology in virtual\nglasses try-on system, wherein face recognition is the\nprecondition for virtual glasses try-on system. During\ntry-on process, it is necessary to detect the face in each\nframe of the video. However, the problems of posture, il-\nlumination, and occlusion can increase the omission and\nfalse ratios of face detection. The real time of detection\nis an important indicator of system performance to en-\nhance people’s experience senses.\nGeneral face recognition process consists of face de-\n\ntection, tracking, feature extraction, dimension reduc-\ntion, and matching recognition (see Fig. 1) [17].\nIn Fig. 1, face detection is the first step to realize face\n\nrecognition. Its purpose is to automatically find face re-\ngion in an input image. If there is a face area, the spe-\ncific location and range of face needs to be located. Face\ndetection is divided into image-based and video-based\ndetection. If the input is a still image, each image is de-\ntected; if the input is a video, face detection is performed\nthroughout the video sequence.\nFeature extraction is based on face detection, and the\n\ninput is the detected face image. Common features are\nLBP (local binary patterns), HOG (histogram of oriented\ngradient), Gabor, etc. HOG [18] describes the edge fea-\ntures. Due to insensitiveness to illumination changes and\nsmall displacements, it describes the overall and local in-\nformation of human face. LBP [19] shows the local tex-\nture changes of an image, with brightness invariance.\nGabo feature [20] captures the local structural content\nof spatial position, direction selectivity, and spatial fre-\nquency. It is suitable for description of human faces.\n\nFig. 1 Face recognition process\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 2 of 19\n\n\n\nFeature dimension reduction is described as follows.\nFace feature is generally high-dimensional feature vector.\nFace recognition of high-dimensional feature vector\nincreases time and space complexity. Besides, it is difficult\nto effectively judge the description ability of high-dimen-\nsional face features. The high-dimensional face feature\nvector can be projected to the low-dimensional subspace.\nThe low-dimensional subspace information can complete\nface feature identification. After feature extraction, the ori-\nginal features are recombined to reduce vector dimension\nof face feature.\nAfter the previous links, we compare the existing tar-\n\ngets in face database and the faces to be identified based\non certain matching strategy, making final decision.\nMatching recognition can be represented by offline\nlearning and online matching models.\n\n3.1 SVM-based face detection\nFace detection is the premise of virtual glasses try-on\ntechnology. Recently, scholars proposed face detection\nmethods, such as neural network, SVM (support vector\nmachine), HMM (hidden Markov model), and AdaBoost.\nIn the work, the classic SVM algorithm is used for face\ndetection. SVM algorithm is a machine learning method\nbased on statistical theory. Figure 2 shows the network\nstructure of SVM [21]. SVM algorithm can be regarded\nas a three-layer feedforward neural network with a hid-\nden layer. Firstly, the input vector is mapped from\nlow-dimensional input space to the high-dimensional\nfeature space by nonlinear mapping. After that, the opti-\nmal hyperplane with the largest interval is constructed\nin the high-dimensional feature space.\nIt is denoted that the input vector of SVM x= (x1, x2,…, xn).\n\nEquation (1) shows the network output of output layer\nbased on x.\n\ny xð Þ ¼ sgn\nXN train\n\ni¼1\nyi∂\n\n�\ni K xi; x\n\n� �þ b�\n� �\n\nð1Þ\n\nwherein the inner product K(x(i), x) is a kernel function\nsatisfying the Mercer condition. Common kernel func-\ntions consist of polynomial, Gauss, and Sigmoid kernel\n\nfunctions. The Gaussian kernel function Kðx; zÞ ¼ e\njjx−zjj2\n2σ2 ,\n\nand σ is the width function.\nOptimization problem of quadratic function (Eq. (2)) is\n\nsolved to obtain the optimal parameter vector ∂�\n\n¼ ð∂�1; ∂�2;…; ∂�N train\nÞT in discriminant function.\n\nmin\n1\n2\nð\nXN train\n\ni¼1\n\nXN train\n\ni¼1\n∂i∂ jy\n\niy jK xi; x j\n� �\n\n−\nXN train\n\ni¼1\n∂i ð2Þ\n\ns:t:\nXNtrain\n\ni¼1\n\n∂iyi i ¼ 1; 2;…;N train\n\n0≤∂i≤C\n\nThe training sample xi corresponding to ∂i > 0 is used\nas a support vector. The optimization parameter b∗ can\nbe calculated by Eq. (3).\n\nb� ¼ 1\nNsv\n\nX\ni∈SV\n\nyi−\nX\n\nj∈SV\n∂�jK xi; x j\n\n� �� �\nð3Þ\n\nSVM classifier is used to determine whether the de-\ntected image is a human face. If it is not human face,\nthen the image is discarded. If it is, then the image is\nretained to output the detection result. Figure 3 shows\nthe detection process.\n\n3.2 Face recognition based on SIFT\nAfter face detection, face features are extracted for face\nrecognition, providing conditions for face alignment. In\nthe work, the robust SIFT algorithm is used for local fea-\nture extraction [22]. The algorithm finds feature points in\ndifferent scale spaces. It is irrelevant to rotation, scale, and\nbrightness changes. Besides, the algorithm has certain sta-\nbility to noise, affine transformation, and angle change.\n\n3.2.1 Basic principle of SIFT algorithm\nIn the process of feature construction by SIFT algorithm,\nit is necessary to deal with multiple details, achieving faster\noperation and higher positioning accuracy. Figure 4 shows\nflow block diagram of SIFT algorithm [21]. The generation\nprocess of local feature is described as follows [22]:\n\nFig. 2 SVM network structure\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 3 of 19\n\n\n\n① Detect extreme points\nGaussian differential functions are used for image search\n\non all scales, thus identifying potential fixed points.\n② Position key points\nThe scale on candidate position of model is confirmed.\n\nThe stability degree determines the selection of key points.\n③ Determine the direction of key points\nUsing the gradient direction histogram, each key point\n\nis assigned a direction with the highest gradient value to\ndetermine the main direction of key point.\n④ Describe the key points\nThe local gradients of image are calculated and repre-\n\nsented by a kind of symbol.\n\n3.2.2 Key point matching\n3.2.2.1 Scale space Scale space introduces a scale par-\nameter into image matching model. The continuously\nvariable scale parameter is used to obtain the scale space\nsequence. After that, the main contour of scale space is\n\ntaken as the feature vector to extract the edge features\n[23]. The larger scale leads to the more blurred image.\nTherefore, scale space can simulate the formation\nprocess of target on the retina of the human eye.\nScale space of image can be expressed as Eq. (4).\n\nL x; y; σð Þ ¼ G x; y; σð Þ � I x; yð Þ ð4Þ\nIn Eq. (4), G(x, y, σ) is the Gaussian function, I(x, y) the\n\noriginal image, and * the convolution operation.\n\n3.2.2.2 Establishing Gaussian pyramid\n\nG x; y; σð Þ ¼ 1\n2πσ2\n\ne− x−d=2ð Þ2þ y−b=2ð Þ2ð Þ=2σ2 ð5Þ\n\nIn Eq. (5), d and b are the dimensions of Gaussian\ntemplate, (x, y) is the pixel location, and σ the scale space\nfactor.\nGaussian pyramid is established according to Eq. (5),\n\nincluding Gaussian blur and down-sampling (see Fig. 5).\nIt is observed that the pyramids with different sizes con-\nstitute tower model from bottom to top. The original\nimage is used for the first layer, the new image obtained\nby down-sampling for the second layer. There are n\nlayers in each tower. The number of layers can be calcu-\nlated by Eq. (6).\n\nn ¼ log2 minf p; qð Þg−d dϵ 0; log2 minf p; qð Þg½ �\nð6Þ\n\nIn Eq. (6), p and q are the sizes of the original image and d\nis the logarithm of minimum dimension of tower top image.\n\n3.2.2.3 Gaussian difference pyramid After scale\nnormalization of maxima and minima of the Gaussian La-\nplace function σ2∇2G, we obtain the most stable image fea-\ntures using other feature extraction functions. The\nGaussian difference function is approximated to the Gauss-\nian Laplace function σ2∇2G after scale normalization. The\nrelationship is described as follows:\n\n∂G\n∂σ\n\n¼ σ2∇ 2G ð7Þ\n\nDifferential is approximately replaced by the difference:\n\nσ2∇ 2G ¼ ∂G\n∂σ\n\n≈\nG x; y; kσð Þ−G x; y; σð Þ\n\nkσ−σ\nð8Þ\n\nTherefore,\n\nG x; y; kσð Þ−G x; y; σð Þ ≈ k−1ð Þσ2∇ 2G ð9Þ\nIn Eq. (9), k − 1 is a constant.\nIn Fig. 6, the red line is the DoG operator curve; the\n\nblue line the Gauss-Laplacian curve. In extreme detection\n\nFig. 3 The detection process of SVM classifier\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 4 of 19\n\n\n\nmethod, the Laplacian operator is replaced by the DoG\noperator [24] (see Eq. (10).\n\nD x; y; σð Þ ¼ G x; y; kσð Þ−G x; y; σð Þð Þ � I x; yð Þ\n¼ L x; y; kσð Þ−L x; y; σð Þ ð10Þ\n\n3.2.2.4 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\n\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the ad-\njacent points to judge whether it is large or small (see\nFig. 6). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and\nlower scale spaces to detect extreme points.\nIn the calculation, the Gaussian difference image is the\n\ndifference between the adjacent upper and lower images\nin each group of the Gaussian pyramid (see Fig. 7).\n\n3.2.2.5 Spatial extreme detection In Gaussian differ-\nence space, local extreme points constitute the key\npoints. When searching for key points, we compare the\nimages between two adjacent layers in the same group.\nAfter that, each pixel point is compared with all the\nadjacent points to judge whether it is large or small\n(see Fig. 8). The red intermediate detection point is com-\npared with 26 points in the surrounding, upper, and lower\nscale spaces to detect extreme points.\nIf there are N extreme points in each group, then we\n\nneed N+ 2-layer DoG pyramid and N+ 3-layer Gaussian\npyramid (see Fig. 8). Due to edge response, the extreme\npoints generated in this case are not all stable.\n\n3.2.2.6 Key point matching At first, the key point is\ncharacterized by position, scale, and direction. To main-\ntain the invariance of perspective and illumination\nchanges, the key point should be described by a set of vec-\ntors. Then, the descriptor consists of key points and other\ncontributive pixels. Besides, the independent characteristic\n\nFig. 4 SIFT algorithm flow chart\n\nFig. 5 Gaussian pyramid\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 5 of 19\n\n\n\nof descriptor is guaranteed to improve the probability of\ncorrect matching of feature points.\nThe gradient value of key point is calculated. The gra-\n\ndient value and direction are determined by Eq. (11).\n\nm x; yð Þ ¼\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi\nN xþ 1; yð Þ−N x−1; yð Þð Þ2 þ N x; yþ 1ð Þ−N x; y−1ð Þð Þ2\n\nq\n\nθ x; yð Þ ¼ α tan2\nN x; yþ 1ð Þ−N x; y−1ð Þ\nN xþ 1; yð Þ−N x−1; yð Þ\n\n� �\nð11Þ\n\nIn Eq. (11), N represents the scale space value of key point.\nGradient histogram statistics. The gradient and direc-\n\ntion of pixels in the neighborhood are represented by\nhistogram. The direction ranges from 0 to 360°. There is a\n\nFig. 6 Comparison of Gauss-Laplacian and DoG\n\nFig. 7 Gaussian pyramid of each group\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 6 of 19\n\n\n\nsquare column for every 10°, forming 36 columns [25]\n(see Fig. 9). In feature point field, the peak represents the\ngradient direction. The histogram of maximum is the\nmain direction of key point. Meanwhile, the histogram\nwith peak value greater than 80% of main direction is se-\nlected for auxiliary direction to improve the matching\nrobustness.\nAfter successful matching of key points, the entire al-\n\ngorithm is not over yet. This is because substantial mis-\nmatched points appear in the matching process. These\nmismatched points are eliminated by Ransac method in\nSIFT matching algorithm [26].\n\n3.2.3 Face recognition experiment\nTo evaluate the algorithm, the experiment is conducted\nbased on face infrared database provided by Terravic Re-\nsearch Corporation. There are a total of 20 infrared\nimage sequences with head rotation, glasses, hats, and\nlight-illuminated pictures. Three pairs of images are se-\nlected from each face, with a total of 60 pairs. Figure 10\nshows the selected 120 images. In the work, the classic\n\nSIFT matching algorithm is used as the initial matching\nmethod to manually determine matching accuracy and\nmismatch rate of each group. In other words, the match-\ning performance is described by accuracy and error de-\ngrees. Accuracy is defined by the ratio of the number of\ncorrect matches in total number. Error degree is the ra-\ntio of the number difference (between key and matched\npoints) in the total number of key points.\nThese 120 samples are conducted with abstract match-\n\ning contrast according to the variables including head\nrotation angle, illumination transformation, glasses, and\nhat wearing. Meanwhile, other variables remain the\nsame. Figures 11, 12, 13, and 14 show the matching re-\nsults, respectively:\n\n1. Matching results when head rotation angle changes\n2. Matching results when wearing glasses\n3. Matching results when wearing a hat\n4. Matching results when light and shade change\n\nThe experimental data are shown in Table 1.The ex-\nperimental image and Table 1 show:\n① SIFT matching performance is more easily affected\n\nby wearing glasses than head rotation angle, light illu-\nmination, darkness, and wearing hat.\n② In the case of the same number of matches, the\n\nsuccess rate of SIFT matching is higher than that of the\nHarris matching method [27].\nThe overall trend of results can be well presented al-\n\nthough there are inevitable errors due to the finiteness\nof experimental samples.\n\n3.3 Face alignment\nFace alignment is the positioning of face feature points.\nAfter face image detection, the SIFT algorithm automat-\nically positions the contour points of the eyebrows, eyes,\nnose, and mouth. In the try-on process of AR glasses,\nthe eyes are positioned to estimate the head posture.\nThe pose estimation is applied to the tracking registra-\ntion subsystem of glasses, thus producing perspective\n\nFig. 8 The detection of DoG space extreme point\n\nFig. 9 The histogram of the main direction\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 7 of 19\n\n\n\ntransformation. However, the pose estimation is easily af-\nfected by the positioning of face feature points, resulting\nin estimation error. The feature points are accurately posi-\ntioned to achieve good effect of head pose estimation.\nAt present, there are many face alignment algorithms.\n\nSDM is a method of finding function approximation\nproposed by Zhu et al [28] by calculating the average\nface, and local features around each feature point are ex-\ntracted to form feature vector descriptor. The offset be-\ntween average and real face is calculated to obtain the\nstep size and motion direction for iteration. The current\nface feature points are converged to the optimal position\nby repeated iterations.\nFigure 15 shows the SDM-based face alignment process.\n\nThe face alignment process is described as follows.\n\n3.3.1 Image normalization\nThe image is normalized to achieve face alignment, thus\nimproving the efficiency of training. The face image to be\ntrained is manually labeled with feature points. After rea-\nsonable translation, rotation, and scaling transformation,\nthe image is aligned to the first sample. The sample size is\nunified to arrange the original data information with con-\nfused, reducing interference other than shape factors. Fi-\nnally, the calculated average face is placed on the sample\nas the estimated face. The average is aligned with the ori-\nginal face image in the center.\n\nIt is denoted that x∗ is the optimal solution in face fea-\nture point location, x0 the initialization feature point,\nd(x) ∈ Rn × 1 the coordinates of n feature points in the\nimage, and h the nonlinear feature extraction function\nnear each feature point. If the SIFT features of 128 di-\nmensions are extracted from each feature point, then\nh(d(x)) ∈ R128n × 1. The SIFT feature extracted at x∗ can\nbe expressed as ∅∗ = h(d(x∗)). Then, the face feature\npoint alignment is converted into the operation of solv-\ning Δx, which minimizes Eq. (12).\n\nf x0 þ Δxð Þ ¼ hðd x0 þ Δxð Þk k22 ð12Þ\nThe step size Δx is calculated based on the SDM\n\nalgorithm.\n\nxk ¼ xk þ Δxk ð13Þ\nIf Rk and bk are the paths of each iteration, then\n\nEq. (11) can converge the feature point from the initial\nvalue x0 to x∗.\n\nxkþ1 ¼ xk−1 þ Rk−1∅k−1 þ bk−1 ð14Þ\nDuring training process, {di} is the set of face images,\n\n{di} the set of manually labeled feature points, and x0 the\nfeature point of each image. Face feature point location\nis transformed into a linear regression problem. For the\nproblem, the input feature is the SIFT feature ∅i\n\n0 at x0;\n\nFig. 10 Sample sequence set\n\nFig. 11 Matching results when head rotation angle changes\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 8 of 19\n\n\n\nthe result the iteration step size Δxi� ¼ xi� þ Δxi0 from x0\nto x∗; and the objective function Eq. (15).\n\nargminR0b0\n\nX\ndi\n\nX\nxi\n\nΔxi�−R0∅i\n�−b0\n\n\t\t \t\t2\n2 ð15Þ\n\nIn this way, R0 and b0 from the training set are iterated\nto obtain Rk and Rk. The two parameters are used for\nthe test phase to achieve the alignment of test images.\n\n3.3.2 Local feature extraction of SIFT algorithm\nIn the work, the principal component analysis is used to\nreduce the dimension of image [29], the impact of\nnon-critical dimensions, and the amount of data, thus\nimproving the efficiency. After the dimension reduction,\nthe local feature points are extracted from the face\nimage. To improve the alignment accuracy of feature\npoints, the robust SIFT algorithm is applied for local fea-\nture extraction. Section 3.2.2 introduces the extraction\nprocess in detail.\n\n3.3.3 SDM algorithm alignment result\nTraining samples are selected from IBUG and LFW face\ndatabases. The former contains 132 face images. Each\nimage is labeled with 71 face feature points, which are\nsaved in pts file. The latter consists of the sets of test and\ntraining samples, wherein, the set of test sample contains\n206 face images. Each image is labeled with 71 face feature\npoints, which are saved in pts file. The set of training\n\nsample contains 803 face images. Each image is labeled\nwith 68 face feature points. Figures 16 and 17 show frontal\nand lateral face alignment results, respectively.\n\n3.4 Face pose estimation\nBased on computer vision, the pose of object refers to\nits orientation and position relative to the camera. The\npose can be changed by moving the camera or object.\nGeometric model of camera imaging determines the re-\nlationship between 3D geometric position of certain\npoint on head surface and corresponding point of image.\nThese geometric model parameters are camera parame-\nters. In most cases, these parameters are obtained by ex-\nperiments. This process is called labeling [27, 29].\nCamera labeling determines the geometric and optical\nproperties, 3D position, and direction of camera relative\nto certain world coordinate system.\nThe idea of face pose estimation is described as fol-\n\nlows. Firstly, we find the projection relationship between\n2D coordinates on face image and 3D coordinates of\ncorresponding points on 3D face model. Then, the mo-\ntion coordinates of camera are calculated to estimate\nhead posture.\nA 3D rigid object has two movements relative to the camera:\n① Translation movement\nThe camera is moved from current spatial position\n\n(X,Y, Z) to new spatial position (X′,Y′, Z′), which is called\n\nFig. 12 Matching results when wearing glasses\n\nFig. 13 Matching results when wearing a hat\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 9 of 19\n\n\n\ntranslation. Translation vector is expressed as τ = (X′ −X,\nY′ −Y, Z′ −Z).\n② Rotary movement\nIf the camera is rotated around the XYZ axis, the rota-\n\ntion has six degrees of freedom. Therefore, pose estima-\ntion of 3D object means finding six numbers (three for\ntranslation and three for rotation).\n\n3.4.1 Feature point labelling\nThe 2D coordinates of N points are determined to calcu-\nlate 3D coordinates of points, thus obtaining 3D pose of\nobject in an image.\nTo determine the 2D coordinates of N points, we se-\n\nlect the points with rigid body invariance, such as the\nnose tip, corners of eyes, and mouth. In the work, there\nare six points including the nose tip, chin, left, and right\ncorners of eyes and mouth.\nSFM (Surrey Face Model) is used as general 3D face\n\nmodel to obtain 3D coordinates corresponding to se-\nlected 2D coordinates [30]. By manual labeling, we ob-\ntain the 3D coordinates (x, y, z) of six points for pose\nestimation. These points are called world coordinates in\nsome arbitrary reference/coordinate system.\n\n3.4.2 Camera labeling\nAfter determining world coordinates, the camera is reg-\nistered to obtain the camera matrix, namely focal length\nof camera, optical center, and radial distortion parame-\nters of image. Therefore, camera labeling is required. In\nthe work, the camera is labeled by Yang and Patras [31]\nto obtain the camera matrix.\n\n3.4.3 Feature point mapping\nFigure 18 shows the world, camera, and image coordin-\nate systems. In Fig. 18, O is the center of camera, c the\noptical center of 2D image plane, P the point in world\ncoordinate system, and P′ the projection of P on image\nplane. P′ can be determined according to the projection\nof the P point.\nIt is denoted that the world coordinate of P is (U,V, W).\n\nBesides, the known parameters are the rotation matrix R\n\nFig. 14 Matching results when light and shade change\n\nTable 1 Match result analysis table\n\nVariate Number of\nmatches\n\nTotal number\nof key points\n\nMatch\nratio\n\nFalse\nmatch rate\n\nHead rotation 18 158 0.129 0.871\n\nWearing glasses 15 167 0.099 0.901\n\nWearing a hat 21 106 0.247 0.753\n\nLight and shade\nchange\n\n45 281 0.191 0.809\nFig. 15 The face alignment process based on SDM\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 10 of 19\n\n\n\n(matrix 3 × 3) and translation vector τ (vector 3 × 1) from\ncamera to world coordinate. It is possible to determine\nposition O(X, Y, Z) of P in camera coordinate system.\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ R\n\nu\nv\nw\n\n2\n4\n\n3\n5þ τ⇒\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼ Rjτ½ �\n\nu\nv\nw\n\n2\n4\n\n3\n5 ð16Þ\n\nEquation (16) is expanded as follows:\n\nx\ny\nz\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð17Þ\n\nIf plenty of points are mapped to (X, Y, Z) and (U,V, W),\nthe above problem is transformed into a system of linear\nequations with unknown (τx, τy, τz) . Then, the system of\nlinear equations can be solved.\nFirstly, the six points on 3D model are manually la-\n\nbeled to derive their world coordinates (U, V, W). Equa-\ntion (18) is used to determine 2D coordinates (X, Y) of\nsix points in image coordinate system.\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 x\n\ny\nZ\n\n2\n4\n\n3\n5 ð18Þ\n\nwhere fx and fy are the focal lengths in the x and y direc-\ntions, (cx, cy) is the optical center, and S the unknown scaling\nfactor. If P in 3D is connected to O, then P′ where light in-\ntersects image plane is the same image connecting all points\nin the center of the camera produced by P along the ray.\nEquation (18) is converted to the following form:\n\nS\nX\nY\nZ\n\n2\n4\n\n3\n5 ¼\n\nr00 r01 r02 τx\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð19Þ\n\nThe image and world coordinates are known in the\nwork. Therefore, Eqs. (18) and (19) are transformed into\nthe following form:\n\nx\ny\n1\n\n2\n4\n\n3\n5 ¼ S\n\nf x 0 0\n0 f y 0\n0 0 1\n\n2\n4\n\n3\n5 r00 r01 r02 τx\n\nr10 r11 r12 τy\nr20 r21 r22 τz\n\n2\n4\n\n3\n5\n\nu\nv\nw\nl\n\n2\n664\n\n3\n775 ð20Þ\n\nIf the correct poses R and τ are known, then the 2D\nposition of 3D facial point on image can be predicted by\nprojecting the 3D point onto the image (see Eq. (20)).\nThe 2D facial feature points are known. Pose estimation\ncan be performed by calculating the distance between\nthe projected 3D point and 2D facial feature. If the pose\nis correctly estimated, the 3D points projected onto\nimage plane will almost coincide with the 2D facial fea-\ntures. Otherwise, the re-projection error can be mea-\nsured. The least square method is used to calculate the\nsum of squares of the distance between the projected 3D\nand 2D facial feature points.\n\n3.5 Tracking registration system\nTracking registration technology is the process of align-\ning computer-generated virtual objects with scenes in\nthe real world. At present, there are two tracking regis-\ntration techniques. The first superimposes certain point\nof face feature with a point of virtual glasses based on\nthe face feature point tracking method [32]. The second\nis based on the geometric transformation relation track-\ning method. Face geometry and virtual glasses model are\nconducted with affine transformation. Virtual glasses\nmodel moves with the movement of human head, mak-\ning corresponding perspective changes and realizing 3D\ntry-on effect [33]. For the first technique, the virtual\nglasses cannot be changed with the movement of user\nhead, causing poor user experience. The second tech-\nnique has good tacking effect. The virtual glasses will be\ndistorted with overlarge head corner. Combined with the\ntwo methods, the glasses model is conducted with per-\nspective transformation using six degrees of freedom ob-\ntained by pose estimation in Section 3.3. After face\nsuperposition, accurate tracking is realized through bet-\nter stereoscopic changes.\n\nFig. 16 The picture of front face alignment\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 11 of 19\n\n\n\n3.5.1 Affine transformation method of glasses try-on\nIn Fig. 19, the center between two corners of the eye is\ncalculated according to the distance between them. An\nisosceles right triangle ABC is defined [34]. The coordi-\nnates of the triangle are A(a1, a2), B(b1, b2), and C(c1, c2).\nIf the threshold is determined by experiment ahead of\ntime, the coordinates of C are as follows.\n\nC c1; c2ð Þ ¼ b1−b2 þ a2; b1 þ b2−a2ð Þ ð21Þ\n\nDuring try-on process, the glasses model is matched\nto the eye of user using the affine transformation\nEq. (22).\n\nx0 ¼ axþ byþ c y0 ¼ dxþ eyþ f ð22Þ\n\nIn the glasses model, the vertices of isosceles right\ntriangle are priori, with the coordinates of (x1, y1),\n(x2, y2), and (x3, y3). The vertices of isosceles right\ntriangle on user face (x01; y\n\n0\n1 ), (x\n\n0\n2; y\n\n0\n2 ), and (x03; y\n\n0\n3 ) can be\n\ndetected in motion. The affine transformation parameter\nh = (a, b, c, d, e, f )T.\n\nx01\ny01\nx02\ny02\nx03\ny03\n\n2\n6666664\n\n3\n7777775\n¼\n\nx1 y1 1 0 0 0\n0 0 0 x1 y1 1\nx2 y2 1 0 0 0\n0 0 0 x2 y2 1\nx3 y3 1 0 0 0\n0 0 0 x3 y3 1\n\n2\n6666664\n\n3\n7777775\n\na\nb\nc\nd\ne\nf\n\n2\n6666664\n\n3\n7777775\n\nð23Þ\n\nEquation (23) is abbreviated as P = Ah. Finally, the af-\nfine transformation parameter h (h = (ATA)−1) is calcu-\nlated by least square method. If h is applied to the\nisosceles right triangle, then the image of glasses will be\nprojected onto the right position of the face.\n\n3.5.2 Perspective transformation method of glasses try-on\nAffine transformation can realize the tracking of 3D\nmodel. The tracked glasses are prone to deformation be-\ncause the affine transformation has the characteristics of\nflatness and parallelism based on three-point transform-\nation [35]. The six degrees of freedom are obtained from\nhead pose estimation. After perspective transformation,\nthe glasses are superimposed with eye feature points to\nachieve real-time tracking effect. When the head moves,\nthe space model of glasses should conform to human\nvisual law, with certain deformation. It is realized by per-\nspective transformation [36] (Fig. 20).\n\nFig. 17 The picture of side face alignment\n\nFig. 18 Three coordinate systems\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 12 of 19\n\n\n\n3.6 Virtual model generation system\nIn the work, the 3D glasses model is built in 3ds max\nand exported to 3DS format file. The 3DS data file can-\nnot display the 3D model in OpenGL in real time.\nFirstly, the 3DS model is conducted with analytic oper-\nation. Only by transferring the operational data to the\nOpenGL function can we draw the virtual glasses model\nin this case [37].\n\n3.7 Virtual and real synthesis system\nTo achieve the perfect combination of virtual glasses\nand realistic scenes, virtual glasses must be positioned to\nthe exact position in the real world at first. This process\nis achieved by integrating markers with natural features.\nFigure 21 shows the overall structure of fused glasses\ntry-on subsystem [38].\n\n4 iOS system application\nTo verify the effectiveness of proposed system method, we\ndevelop a mobile “AR Glasses Try-on Sales System” for\niOS platform. This system comprehensively uses the\n\ncommon controls of iOS. Besides, the controls are recon-\nstructed and optimized to improve the operating efficiency\nof the system. Meanwhile, most function blocks are modi-\nfied to minimize the application, dependencies, and main-\ntenance procedure of third-party framework. The system\nrealizes the basic functions including browsing of glasses\nproducts, user registration, login, goods collection, adding\nto carts, user address adding, modifying, deleting, goods\npurchasing, integrated Alipay payment, and order man-\nagement [39, 40]. In addition, it is embedded with glasses\ntry-on, photographing, recording video, uploading, and\nsharing WeChat and Weibo. Meanwhile, the system has\nits social platform for users to browse try-on effects of\nothers. The same glasses are quickly tried on, thus meet-\ning the needs of vast users (Fig. 22).\nCommodity browsing module is the core of the system,\n\ncovering commodity browsing, screening, try-on, photo-\ngraphing, uploading, and sharing. The try-on and quick\ntry-on subsystems need to call face recognition method in\nPart 3.\n\n4.1 Menu module\nMenu module is the framework module of the whole ap-\nplication. All the sub-modules are switched by a Menu-\nViewController controller. This control contains the\nviews of menu, home page, favorites, shopping cart,\norder, coupon, photo wall, setting modules, initialization,\nand the switching method of controllers. The menu page\nis not displayed in the default startup page. User calls up\nthe menu page by clicking the upper-left icon of the de-\nfault page or sliding to the right in the home page.\nMenu page adopts the traditional frosted glass method.\n\nFirstly, the UIImage object is obtained by taking a screen\nshot and then processed by the frosted glass tool. The\nfrosted glass UImage is used as the background of the\nmenu to realize the translucent frosted glass effect.\nMenu view is the leftmost view of MenuViewControl-\n\nler. The view is located at the top of the entire applica-\ntion. The menu can be switched out in all modules. It is\nachieved mainly by the table. The rows are selected by\nthe table to trigger the effect of selected rows.\nThe module title in the menu is clicked to trigger the\n\nproxy event of table, thus calling the method of selecting\ncurrent module for module switching. The switchable\nmodules mainly include user, commodity, collection,\nshopping cart, order, coupon, photo wall, and setting.\n\n4.2 User registration module\nUser registration module is used for the management of\nregistered users. Registered users enjoy the VIP promo-\ntion activities and prices. Unregistered users enter the\nregistration page by clicking the registration button.\n\nFig. 19 The isosceles triangle made on face and eyeglass images\n\nFig. 20 The quadrilateral made on face and eyeglass images\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 13 of 19\n\n\n\n4.3 Commodity module\nCommodity module is the key of “AR glasses sales sys-\ntem,” including commodity browsing, selection, and add-\ning to cart.\n\n4.3.1 Commodity browsing\nAll the products are browsed in the login or non-login\nstatus. In commodity browsing, the big images of glasses\nare slid to browse the front of product image and leg\nstyle. The detail page can be slid to view more commod-\nity information. More commodity information is loaded\nby sliding up.\nThe pull-up and pull-down are proxy methods based\n\non the table and its parent class (UIScrollView).\n(void)scrollViewDidScroll:(UIScrollView *)scrollView\nWhen the height of parent container offset is greater\n\nthan 20% of table height, the pull-down refresh is called.\nThe pull-up refresh is called when the height difference\n\nbetween the height of parent container and the sum of\ntable height and offset exceeds 10% of the screen height.\n\n4.3.2 Commodity screening\nIn commodity browsing, users quickly find products\nmeeting their needs and click to select multiple items. If\none item is selected, the system will feed back the num-\nber of eligible products in time. The display result but-\nton is clicked to display the screening result. Screening\nresults can be cleared by clicking on the cross on the\nright of blue subtitle.\nList screening is realized by modified and nested tables.\n\nThe segment head of custom table is used as first-level\nscreening title. Second-level screening catalog is achieved\nby nested table and custom table cells. By nesting\nsecond-level screening catalog, we obtain third-level\nscreening catalog. The subtitle of first-level catalog is\nrefreshed by recording the selected filter item in real time.\n\nFig. 21 General structure of system\n\nFig. 22 The structure of “AR Glasses Sales System”\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 14 of 19\n\n\n\nSimultaneously, the server is synchronized to get the\nremaining product information.\nIn the screening tool class, the record of third-level\n\nmenu is complicated. In implementation, the third-level\noptions are recorded for local summary, updating selected\nor unselected state. Full summary is performed by reusing\nlocal summary, indicating in the first-level menu.\n\n4.3.3 Glasses try-on\nIn the system, the face data are captured by the camera\nfor further processing. Firstly, user’s face is located at 30–\n50 cm right ahead the front camera of mobile phone. The\nface is slightly rotated, without getting out of capture area\nof camera. In addition, user should not keep his/her back\nto the light during the try-on process because the light af-\nfects the capture effect of the camera. When the camera\ndoes not capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the en-\ngine re-recognizes the face information. The quick try-on\nbutton is clicked to enter the quick try-on page. User can\nput the phone 30–50 cm in front of him. Then, his/her\nface appears on the screen of the mobile phone. Mean-\nwhile, the system automatically recognizes user face data\nto put on glasses. The product details are viewed by\n\nsliding the glasses or clicking on the left detail button.\nFigure 23 shows the try-on process in detail.\nIn the try-on process, the third-party oepnframeworks is\n\nused to modify the class library according to the require-\nments. Section 3 introduces face recognition, alignment,\ntracking registration and head pose estimation, and virtual\nmodel generation methods. Combined with these\nmethods, the interface is packaged to increase the stability\nof system, reducing the dependence on third-party con-\ntrols. The part embedded in openframeworks starts with\nthe main function. Using openframeworks, the window is\ninitialized to call the Appdeleagte class of openframe-\nworks. This class is compatible with UIKit library in iOS.\nThe ofApp will be initialized to call engine loading model.\n\n4.3.3.1 AFNetworking In iOS development, the NSURL-\nConnection of XCode is competent to submit a request to\na simple page of Web site, thus obtaining the response\nfrom server. However, most web pages to be visited are\nprotected by authority. The pages cannot be visited by a\nsimple URL. This involves the processing of Session and\nCookie. Here, NSURLConnection can be used to realize\naccess, with larger complexity and difficulty.\nAFNetworking is more suitable to process requests to\n\nWeb sites, including detailed Sessions and Cookies\n\nFig. 23 The detailed flow chart of eyeglass try-on\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 15 of 19\n\n\n\nproblems. It can be used to send HTTP requests and re-\nceive HTTP responses. However, it does not cache ser-\nver responses or execute the JAvascript code in the\nHTML page. Meanwhile, AFNetworking has built-in\nJSON, plist, and XML file parsing for convenient\napplication.\nSome interfaces of the library are packaged to facilitate\n\nthe use of AFNetworking. The packaged AFNetworking\ncan record the operation due to disconnection request\nfailure. After networking, the request is re-initiated.\nWhen data needs to be requested, if get request is\n\ncalled, the following methods will be called:\n-(void)GET:(NSString *)URLString parameters:(id)para\n\nmeters WithBlock:(resultBlock)result;\nIf a image necessary upload pictures the following methods\n\nwill be call.\n// Upload pictures\n-(void)POST:(NSString *)URLString parameters:(id)para\n\nmeters WithData:(NSData *)data WithKey:(NSString *)key\nWithTypeO:(NSString*)pngOrMp4\nWithBlock:(resultBlock)result;\nIf a video necessary upload pictures the following\n\nmethods will be call.\n// Upload video\n-(void)POST:(NSString *)URLString parameters:(id)pa\n\nrameters WithDic:(NSDictionary *)dic WithTypeO:(NS\nString*)pngOrMp4 WithBlock:(resultBlock)result;\nIf post the following interface will be call.\n// post\n- (void)POST:(NSString *)URLString parameters:(id)\n\nparameters WithBlock:(resultBlock)result;\n\n4.3.3.2 SDWebImage SDWebImage is a framework for\nthird-party applications. It is used to implement asyn-\nchronous loading and caching of images. In this system,\nall network images are loaded using this framework. By\ndefining interface classes, we can easily implement asyn-\nchronous loading and caching of images.\n#import <Foundation/Foundation.h>\n@interface TGImageTool : NSObject\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView;\n+ (void)clear;\n@end\n#import “TGImageTool.h”\n#import “UIImageView+WebCache.h”\n@implementation TGImageTool\n+ (void)downloadImage:(NSString *)url placeholder:\n\n(UIImage *)place imageView:(UIImageView *)imageView\n{[imageView setImageWithURL:[NSURL URLWithStrin\n\ng:url] placeholderImage:place options:SDWebImageLowPri\nority | SDWebImageRetryFailed];}\n+ (void)clear\n{\n\n// 1. Clear the cached images in memory\n[[SDImageCache sharedImageCache] clearMemory];\n[[SDImageCache sharedImageCache] clearDisk];\n// 2. Cancel all download requests\n[[SDWebImageManager sharedManager] cancelAll];\n}\n@end\nThe image is loaded by the above tool class method.\n\nWhen the cache is implemented, the image will be auto-\nmatically added to the cache.\nThe clear method of tool class is called to clear the cache.\n\n4.3.3.3 JSONKit JSONKit is used in this system only\nwhen the order information is submitted. It transcodes\nthe complicated parameter information to JSON strings\nfor server application. The conversion method is de-\nscribed as follows.\nNSString *new=[dic JSONString]\n\n4.3.4 Adding to cart\nThe satisfied glasses are added to shopping cart by clicking\non the “Add to Cart” button. The animation of “Add to\nCart” is realized by path and combined animation in the\nQuartzCore library.\n\n4.3.5 Buy a glasses immediately\nUser directly jumps to the page for purchasing the\nglasses without adding to cart. The function is realized\nby directly jumping to order information improvement\npage after summarizing commodity information.\n\n4.3.6 Taking photos or recording videos\nUsers with glasses can take off their glasses after logging\nin. VR glasses are tried on to take photos or record videos.\nThe try-on effects can also be watched after wearing\nglasses. The system provides functions of taking photos\nand recording videos. The photo/video button is used to\nswitch between taking pictures and recording videos. In\nthe work, this function is realized by modifying the engine\nin oepnframeworks. This system only involves the call.\nThe photos and videos are placed in the four preview\nareas below, where user can click to view the details.\n\n4.3.7 Uploading and sharing\nThe system provides uploading and sharing functions of\nphotos or videos to share satisfactory try-on results and\nwonderful moments with friends. “Share” button is\nclicked to upload videos to photo wall, friend circle,\nWeibo, or WeChat in the server. The photos or videos\nare deleted by clicking the “Delete” button.\nThe third-party AFNetworking method is used to up-\n\nload files. The files can be shared to Sina Weibo,\nWeChat circle, friends, and photo wall.\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 16 of 19\n\n\n\nThe sharing principle is to obtain the information of\nthe photo or video on server side. Then, the html5 page\nis generated, including image, video, like, and comment.\nThe URL is returned to the client and shared to WeChat\nand Sina Weibo.\nUsers can choose whether to share to photo wall at\n\nthe same time. Sharing to photo wall is to send a request\nto the server. The photo or video is backed up in the\ntable corresponding to database photo wall information.\nWhen being requested, the shared information can be\nobtained in the photo wall.\n\n4.4 Collection module\nIn the implementation of collection module, the cells of\ntable are reused in the home page. The data are replaced\nwith the data of favorite list. After logging in, the favorite\nitem is added to the collection list of personal information\nby clicking the gray heart button, which is convenient for\nnext viewing. “Collect” button is clicked to cancel the col-\nlected item, removing it from the collection list.\n\n4.5 Shopping cart module\nAfter logging in, the satisfactory item is added to shop-\nping cart in the try-on interface. In implementation, the\ncustom tool class is used to record the selected state.\nWhen clicking “Select All” button, all data in the table\nare selected. The selected state of “Select All” button is\nremoved to cancel certain item. Meanwhile, the sums of\nselected item quantities and unit prices are calculated.\nThe head position shows the number of items. “Settle”\nbutton at the bottom of table shows the total number of\nitems. Users can modify orders and postal addresses,\nwhile submitting orders and paying online.\n\n4.6 Order module\nAfter logging in, users can see their historical orders in\n“My Order.” There are two states in the order, including\npending (immediate payment) and successful payment.\n“Pay now” button is clicked to jump to payment interface.\nDuring order payment, it will jump to the immediate pay-\nment page of shopping module and then to Alipay.\n\n4.7 Coupon module\nThe coupon module is a channel through which mer-\nchants can distribute benefits to users. After logging in,\nusers check the coupons matching their own eligibilities.\nThere are three types of coupons received: available,\nused, and expired coupons. After reading coupon usage\nrules, users can select whether to use the coupon in the\ninterface of order information completion.\n\n4.8 Photo wall module\nThe photo wall is a display platform provided by the sys-\ntem to user. It is convenient for user to browse the\n\ntry-on results of others. Based on dynamic prompt func-\ntion, user quickly finds the favorite style of glasses.\nUser dynamic prompt function is implemented by de-\n\ntecting new messages. Once menu page pops up, a request\nis sent to the server, requesting a new unread message. If\nthere is a new message, it will show user avatars of last dy-\nnamic message and the number of new messages; other-\nwise, the prompt box is not displayed.\nWhile seeing the favorite try-on results, users can like,\n\ncomment, forward, or view the same item and try it on\nquickly.\n“View commodity” button is clicked to view the de-\n\ntailed information of glasses try-on results. The product\ninformation is uniquely determined according to the\nproduct ID. It is the same as quick try-on principle.\nUser can directly try on the same glasses worn by\n\nother users by clicking the quick try-on button. The\nphoto wall and product data are bound in the database\nat the beginning. Therefore, the product can be directly\nfound and tried on according to the product ID.\n\n4.9 Setting module\nThe setting module contains “check updates,” “clean up\npicture cache,” “about us,” “rating,” “feedback,” and “exit\ncurrent user.” Relatively, it is the application of native\ntable control, which is not described here.\n\n5 Results and discussion\nExperimental environment is described as follows.\nOperating system: iOS 9\nDevelopment tools: Xcode 6\nRelated libraries: OpenCV, MFC\nProgramming language: C language, Objective-C, C++\nFigure 24 shows the partial operation interface of the\n\nsystem.\nAlthough a lot of jobs are done, there are still some\n\nshortcomings in the system:\n\n1. Equipped with a try-on engine, the system has\ncertain requirements on the performance of iPhone.\nThe higher configuration of iPhone leads to the\nmore accurate identification. At present, the models\nrunning smoothly are iPhone 5s and iPhone 6 and\niPhone 6 Plus.\n\n2. It is difficult for user to perform subsequent\noperations in the case of unstable network\nenvironment, especially for failed login.\n\n3. In the system, the face data are captured by the\ncamera for further processing. Firstly, user face is\nlocated at 30–50 cm right ahead the front camera\nof the mobile phone. The face is slightly rotated,\nwithout getting out of the capture area of the camera.\n\n4. User should not keep his/her back to the light\nduring the try-on process because the light affects\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 17 of 19\n\n\n\ncapture effect of the camera. When the camera does\nnot capture face data, face position will be adjusted\nby a prompt. As user wiggles in front of camera, the\nengine re-recognizes user face information.\n\n5. At present, only the Chinese version of “AR Glasses\nSales System” has been developed. There is no\ncorresponding English version.\n\n6 Conclusions\nIn the work, we discussed augmented reality virtual\nglasses try-on technology. Face information was collected\nby monocular camera. After face detection by SVM classi-\nfier, the local face features were extracted by robust SIFT.\nCombined with SDM, the feature points were iteratively\nsolved to obtain more accurate feature point alignment\nmodel. Through the head pose estimation, the virtual\nglasses model was accurately superimposed on the human\nface, thus realizing the try-on of virtual glasses. This the-\noretical research was applied in iOS platform for the\ntry-on of virtual glasses, thus providing best services for\nuser selection. Experiments showed that the virtual glasses\nhad realistic effect and high try-on speed and user satisfac-\ntion. Consequently, AR-based glasses try-on technology\nprovided new idea for virtual try-on technology. Camera\ncapture under complex light conditions will be further\nstudied. App running test on iPhone 7 and above will be\ncarried out. Multilingual versions will be developed.\n\nAbbreviations\nAPP: Application; AR: Augmented reality; CCD: Charge-coupled device;\nHMM: Hidden Markov model; HOG: Histogram of oriented gradient;\nIBUG: Intelligent Behaviour Understanding Group; iOS: iPhone OS; LBP: Local\nbinary patterns; LFW: Labeled Faces in the Wild; SDM: Supervised descent\nmethod; SFM: Surrey Face Model; SIFT: Scale-invariant feature transform;\nSVM: Support vector machines; VR: Virtual reality\n\nAcknowledgements\nThis work is partially supported by Shanxi Province Universities Science and\nTechnology Innovation Project (2017107) and Shanxi Province Science\nFoundation for Youths (201701D12111421).\nThanks to the editor and reviewers.\n\nFunding\nThe paper is subsidized by science and technology key project of Henan\nProvince, China. NO.172102210462\n\nAvailability of data and materials\nData will not be shared; reason for not sharing the data and materials is that\nthe work submitted for review is not completed. The research is still ongoing, and\nthose data and materials are still required by my team for further investigations.\n\nAuthor’s contributions\nBZ designed the research, analyzed the data, and wrote and edited the\nmanuscript. The author read and approved the final manuscript.\n\nAuthor’s information\nBoping Zhang, female, is currently an Associate Professor at the School of\nInformation Engineering, Xuchang University, China. She received master’s\ndegree from Zhengzhou University, China, in 2006. Her current research\ninterests include computer vision, image processing, virtual reality, and\npattern recognition.\n\nEthics approval and consent to participate\nNot applicable.\n\nConsent for publication\nNot applicable.\n\nCompeting interests\nThe author declares that she has no competing interests. The author confirms\nthat the content of the manuscript has not been published or submitted for\npublication elsewhere.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published\nmaps and institutional affiliations.\n\nReceived: 15 August 2018 Accepted: 5 November 2018\n\nReferences\n1. DITTO. http://www.ditto.com/\n2. O. Deniz, M. Castrillon, J. Lorenzo, et al., Computer vision based eyewear\n\nselector. Journal of Zhejiang University-SCIENCE C (Computers & Electronics)\n11(2), 79–91 (2010)\n\n3. Gongxin Xie. A transformation road for the glasses industry[J]. China Glasses,\n2014,03:112–113\n\n4. Liu Cheng, Wang Feng, QI Changhong, et al. A method of virtual glasses\ntry-on based on augmented reality[J]. Industrial Control Computer, 2014,\n27(12):66–69\n\n5. Boping Zhang. Design of mobile augmented reality game based on image\nrecognition[J]. EURASIP Journal on Image and Video Processing, 2017, 20:2–20\n\na b c d\nFig. 24 The part of interface for system operation\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 18 of 19\n\nhttp://www.ditto.com/\n\n\n6. Yan Lei, Yang Xiaogang, et al. Mobile augmented reality system design and\napplication based on image recognition[J], Journal of Image and Graphics,\n2016, 21(2):184–191\n\n7. Niswar A, Khan I R, Farbiz F. Virtual try-on of eyeglasses using 3D model of\nthe head[C]. International Conference on Virtual Reality Continuum and ITS\nApplications in Industry. New York: ACM; 2011:435–438.\n\n8. Meijing[OL]. http://www.meijing.com/tryinon.html\n9. Kede [OL]. http://www.kede.com/frame\n10. Biyao [OL]. http://www.biyao.com/home/index.html\n11. Li Juan, Yang Jie. Eyeglasses try-on based on improved Poisson equations.\n\n2011 Conference on Multimedia Technology. New York: ICMT 2011. 2011;\n3058–3061.\n\n12. DU Yao,WANG Zhao-Zhong. Real-like virtual fitting for single image[J].\nComputer Systems Application, 2015, 24(4):19–20\n\n13. Y. Lu, W. Shi-Gang, et al., Technology of virtual eyeglasses try-on system\nbased on face pose estimation[J]. Chinese Optics 8(4), 582–588 (2015)\n\n14. Yuan M, Khan I R, Farbiz F, et al. A mixed reality virtual clothes try-on\nsystem[J]. IEEE Transactions on Multimedia. 2013;15(8):1958-968.\n\n15. Huang W Y, et al. Vision-based virtual eyeglasses fitting system[C]. IEEE,\nInternational Symposium on Consumer Electronics. New York: IEEE. 2013;45–46\n\n16. Wang Feng, Qi Changhong, Liu Cheng, Jiang Wei, Ni Zhou, Zou Ya.\nReconstruction of 3D head model based on orthogonal images [J]. Journal\nof Southeast University (Natural Science Edition). 2015;45(1):36-40.\n\n17. Zhang B. Cluster Comput. 2017. https://doi.org/10.1007/s10586-017-1330-5.\n18. Maatta J, Hadid A, Pietikainen M. Face spoofing detection from single images\n\nusing texture and local shape analysis[J]. IET Biometrics, 2012, 1(1):3–10\n19. Lin Y, Lv F, Zhu S, et al. Large-scale image classification: fast feature\n\nextraction and svm training[C]. Computer Vision and Pattern Recognition\n(CVPR), 2011 IEEE Conference on. New York: IEEE; 2011:1689–1696\n\n20. Lowe D G, Lowe D G. Distinctive image features from scale-invariant\nkeypoints[J]. Int. J. Comput. Vis., 2004, 60(2):91–110\n\n21. Zhang Boping. Research on automatic recognition of color multi\ndimensional face images under variable illumination[J]. Microelectronics &\nComputer, 2017,34(5) :128–132\n\n22. MING An-Long MA Hua-dong. Region-SIFT descriptor based\ncorrespondence between multiple cameras[J]. CHIN ESE JOURNA L OF\nCOMPUTERS, 2008, 12(4):650–662\n\n23. He Kai, Wang Xiaowen, Ge Yunfeng. Adaptive support-weight stereo\nmatching algorithm based on SIFT descriptors[J]. Journal of Tiajin University,\n2016, Vol.49(9):978–984\n\n24. D.G. Lowe, Distinctive image features from scale-invariant key points. Int. J.\nComput. Vis. 60(2), 91–110 (2004)\n\n25. Chen Guangxi, Gong Zhenting, et al. Fast image recognition method Bsded\non locality-constrained linear coding[J]. Computer Science, 2016, vol. 43(5),\n308–314\n\n26. Bai Tingzhu, Hou Xibao. An improved image matching algorithm base on\nSIFT[J]. Transaction of Beijing Institute of Technology, 2013, 33(6):622–627\n\n27. Xiong X, Tome F D L. Supervised descent method and its applications to face\nalignment[C].Computer Vision and Pattern Recognition. New York: IEEE; 2013:\n532–539\n\n28. Zhu JE, et al. Real-Time Non-rigid Shape Recovery Via Active Appearance\nModels for Augmented Reality (Proc. Of 9th European Conference on\nComputer Vision, Graz, 2006), pp. 186–197\n\n29. Huber P, Hu G, Tena R, et al. A multiresolution 3D morphable face model\nand fitting framework[C]. Visapp. 2015\n\n30. Zhang Z. A flexible new technique for camera calibration[J]. IEEE Transactions\non Pattern Analysis&Machine Intelligence, 2000, 22(11):1330–1334\n\n31. Yang H, Patras I. Sieving Regression Forest Votes for Facial Feature\nDetection in the Wild[C]. New York: ICCV; 2013:1936–1943\n\n32. Dantone M, Gall J, Fanelli G, et al. Real-time facial feature detection using\nconditional regression forests[C]. Computer Vision and Pattern Recognition.\nNew York: IEEE; 2012:2578–2585\n\n33. Google Release online AR mobile games Ingress[OL]. http://www.csdn.net/\narticle/2012-11-16/2811943-google-launches-ingress\n\n34. D. Shreiner, G. Sellers, J.M. Kessenich, B.M. Licea-Kane, OpenGL Programming\nGuide: The Official Guide to Learning OpenGL, 8th edn. (Addison-Wesley\nProfessional, United States, 2013)\n\n35. J. Kim, S. Forsythe, Adoption of virtual try-on technology for online apparel\nshopping. J. Interact. Mark. 22, 45–59 (2008)\n\n36. A. Merle, S. Senecal, A. St-Onge, Whether and how virtual try-on influences\nconsumer responses to an apparel web site. Int. J. Electron. Commer. 16,\n41–64 (2012)\n\n37. Niswar, A.; Khan, I.R.; Farbiz, F. In Virtual try-on of eyeglasses using 3d model\nof the head, Proceedings of the 10th International Conference on Virtual\nReality Continuum and Its Applications in Industry. New York: ACM; 2011.\npp 435–438\n\n38. Koestinger M, Wohlhart P, Roth PM, Bischof H. Annotated facial landmarks\nin the wild: A large-scale, real-world database for facial landmark\nlocalization. First IEEE International Workshop on Benchmarking Facial\nImage Analysis Technologies, 2011\n\n39. Q. Zhou, Multi-layer affective computing model based on emotional\npsychology. Electron. Commer. Res. 18(1), 109–124 (2018). https://doi.org/\n10.1007/s10660-017-9265-8\n\n40. Q. Zhou, Z. Xu, N.Y. Yen, User sentiment analysis based on social network\ninformation and its application in consumer reconstruction intention.\nComput. Hum. Behav. (2018) https://doi.org/10.1016/j.chb.2018.07.006\n\nZhang EURASIP Journal on Image and Video Processing (2018) 2018:132 Page 19 of 19\n\nhttp://www.meijing.com/tryinon.html\nhttp://www.kede.com/frame\nhttp://www.biyao.com/home/index.html\nhttps://doi.org/10.1007/s10586-017-1330-5\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttp://www.csdn.net/article/2012-11-16/2811943-google-launches-ingress\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1007/s10660-017-9265-8\nhttps://doi.org/10.1016/j.chb.2018.07.006\n\n\tAbstract\n\tIntroduction\n\tResearch status of network virtual try-on technology\n\tMethods of face recognition\n\tSVM-based face detection\n\tFace recognition based on SIFT\n\tBasic principle of SIFT algorithm\n\tKey point matching\n\tFace recognition experiment\n\n\tFace alignment\n\tImage normalization\n\tLocal feature extraction of SIFT algorithm\n\tSDM algorithm alignment result\n\n\tFace pose estimation\n\tFeature point labelling\n\tCamera labeling\n\tFeature point mapping\n\n\tTracking registration system\n\tAffine transformation method of glasses try-on\n\tPerspective transformation method of glasses try-on\n\n\tVirtual model generation system\n\tVirtual and real synthesis system\n\n\tiOS system application\n\tMenu module\n\tUser registration module\n\tCommodity module\n\tCommodity browsing\n\tCommodity screening\n\tGlasses try-on\n\tAdding to cart\n\tBuy a glasses immediately\n\tTaking photos or recording videos\n\tUploading and sharing\n\n\tCollection module\n\tShopping cart module\n\tOrder module\n\tCoupon module\n\tPhoto wall module\n\tSetting module\n\n\tResults and discussion\n\tConclusions\n\tAbbreviations\n\tAcknowledgements\n\tFunding\n\tAvailability of data and materials\n\tAuthor’s contributions\n\tAuthor’s information\n\tEthics approval and consent to participate\n\tConsent for publication\n\tCompeting interests\n\tPublisher’s Note\n\tReferences\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczEzNjQwLTAxOC0wMzczLTgucGRm0", "metadata_author": "Boping Zhang", "metadata_title": "Augmented reality virtual glasses try-on technology based on iOS platform", "metadata_creation_date": "2018-11-23T11:51:48Z", "keyphrases": [ "Augmented reality virtual glasses", "iOS platform", "technology" ] }, { "@search.score": 1, "content": "\nORIGINAL RESEARCH\n\nDiscriminated by an algorithm: a systematic review\nof discrimination and fairness by algorithmic decision-\nmaking in the context of HR recruitment and HR\ndevelopment\n\nAlina Köchling1\n• Marius Claus Wehner1\n\nReceived: 15 October 2019 / Accepted: 1 November 2020 / Published online: 20 November 2020\n\n� The Author(s) 2020\n\nAbstract Algorithmic decision-making is becoming increasingly common as a new\n\nsource of advice in HR recruitment and HR development. While firms implement\n\nalgorithmic decision-making to save costs as well as increase efficiency and\n\nobjectivity, algorithmic decision-making might also lead to the unfair treatment of\n\ncertain groups of people, implicit discrimination, and perceived unfairness. Current\n\nknowledge about the threats of unfairness and (implicit) discrimination by algo-\n\nrithmic decision-making is mostly unexplored in the human resource management\n\ncontext. Our goal is to clarify the current state of research related to HR recruitment\n\nand HR development, identify research gaps, and provide crucial future research\n\ndirections. Based on a systematic review of 36 journal articles from 2014 to 2020,\n\nwe present some applications of algorithmic decision-making and evaluate the\n\npossible pitfalls in these two essential HR functions. In doing this, we inform\n\nresearchers and practitioners, offer important theoretical and practical implications,\n\nand suggest fruitful avenues for future research.\n\nKeywords Fairness � Discrimination � Perceived fairness � Ethics �\nAlgorithmic decision-making in HRM � Literature review\n\n1 Introduction\n\nAlgorithmic decision-making in human resource management (HRM) is becoming\n\nincreasingly common as a new source of information and advice, and it will gain\n\nmore importance due to the rapid growth of digitalization in organizations.\n\n& Alina Köchling\n\nalina.koechling@hhu.de\n\n1 Faculty of Business Administration and Economics, Heinrich-Heine-University Düsseldorf,\n\nUniversitätsstrasse 1, 40225 Dusseldorf, Germany\n\n123\n\nBusiness Research (2020) 13:795–848\n\nhttps://doi.org/10.1007/s40685-020-00134-w\n\nhttp://orcid.org/0000-0001-7039-9852\nhttp://orcid.org/0000-0002-1932-3155\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s40685-020-00134-w&amp;domain=pdf\nhttps://doi.org/10.1007/s40685-020-00134-w\n\n\nAlgorithmic decision-making is defined as automated decision-making and remote\n\ncontrol, as well as standardization of routinized workplace decisions (Möhlmann\n\nand Zalmanson 2017). Algorithms, instead of humans, make decisions, and this has\n\nimportant individual and societal implications in organizational optimization\n\n(Chalfin et al. 2016; Lee 2018; Lindebaum et al. 2019). These changes in favor\n\nof algorithmic decision-making make it easier to discover hidden talented\n\nemployees in organizations and review a large number of applications automatically\n\n(Silverman and Waller 2015; Carey and Smith 2016; Savage and Bales 2017). In a\n\nsurvey of 200 artificial intelligence (AI) specialists from German companies, 79%\n\nstated that AI is irreplaceable for competitive advantages (Deloitte 2020). Several\n\ncommercial providers, such as Google, IBM, SAP, and Microsoft, already offer\n\nalgorithmic platforms and systems that facilitate current human resource (HR)\n\npractices, such as hiring and performance measurements (Walker 2012). In turn,\n\nwell-known and large companies, such as Vodafone, Intel, Unilever, and Ikea, apply\n\nalgorithmic decision-making in HR recruitment and HR development (Daugherty\n\nand Wilson 2018; Precire 2020).\n\nThe major driving forces for algorithmic decision-making are savings in both\n\ncosts and time, minimizing risks, enhancing productivity, and increasing certainty in\n\ndecision-making (Suen et al. 2019; McDonald et al. 2017; McColl and Michelotti\n\n2019; Woods et al. 2020). Besides these economic reasons, firms seek to diminish\n\nthe human biases (e.g., prejudices and personal beliefs) by applying algorithmic\n\ndecision-making, thereby increasing the objectivity, consistency, and fairness of the\n\nHR recruitment as well as HR development processes (Langer et al. 2019;\n\nFlorentine 2016; Raghavan et al. 2020). For example, Deloitte argues that the\n\nalgorithmic decision-making system always manages each application with the\n\nsame attention according to the same requirements and criteria (Deloitte 2018). At\n\nfirst glance, algorithmic decision-making seems to be more objective and fairer than\n\nhuman decision-making (Lepri et al. 2018).\n\nHowever, there is a possible threat of discrimination and unfairness by relying\n\nsolely on algorithmic decision-making (e.g., (Lee 2018; Lindebaum et al. 2019;\n\nSimbeck 2019)). In general, discrimination is defined as the unequal treatment of\n\ndifferent groups based on gender, age, or ethnicity instead of on qualitative\n\ndifferences, such as individual performance (Arrow 1973). Algorithms produce\n\ndiscrimination or biased outcomes if they are trained on inaccurate (Kim 2016),\n\nbiased (Barocas and Selbst 2016), or unrepresentative input data (Suresh and Guttag\n\n2019). Consequently, algorithms are vulnerable to produce or replicate biased\n\ndecisions if their input (or training) data are biased (Chander 2016).\n\nComplicating this issue, biases and discrimination are often only recognized after\n\nalgorithms have made a decision. As a prominent example stemming from the\n\ncurrent debate around transparency, bias, and fairness in algorithmic decision-\n\nmaking (Dwork et al. 2012; Lepri et al. 2018; Diakopoulos 2015), the hiring\n\nalgorithms applied by the American e-commerce specialist Amazon yielded an\n\nextreme disadvantage of female applicants, which finally led Amazon to shut down\n\nthe complete algorithmic decision-making for their hiring decision (Dastin 2018;\n\nMiller 2015). Thus, the lack of transparency and accountability of the input data, the\n\nalgorithm itself, and the factors influencing algorithmic outcomes are potential\n\n796 Business Research (2020) 13:795–848\n\n123\n\n\n\nissues associated with algorithmic decision-making (Citron and Pasquale 2014;\n\nPasquale 2015). Another question remains whether applicants and/or employees\n\nperceive the algorithmic decision-making to be fair. Previous studies showed that\n\napplicants’ and employees’ acceptance of algorithmic decision-making is lower in\n\nHR recruitment and HR development compared to common procedures conducted\n\nby humans (Kaibel et al. 2019; Langer et al. 2019; Lee 2018).\n\nConsequently, there is a discrepancy between the enthusiasm about algorithmic\n\ndecision-making as a panacea for inefficiencies and labor shortages on one hand and\n\nthe threat of discrimination and unfairness of algorithmic decision-making on the\n\nother side. While the literature in the field of computer science has already\n\naddressed the issues of biases, knowledge about the potential downsides of\n\nalgorithmic decision-making is still in its infancy in the field of HRM despite its\n\nimportance due to increased digitization and automation in HRM. This heteroge-\n\nneous state of research on discrimination and fairness raises distinct challenges for\n\nfuture research. From a practical point of view, it is problematic if large and well-\n\nknown companies implement algorithms without being aware of the possible pitfalls\n\nand negative consequences. Thus, to move the field forward, it is paramount to\n\nsystematically review and synthesize existing knowledge about biases and\n\ndiscrimination in algorithmic decision-making and to offer new research avenues.\n\nThe aim of this study is threefold. First, this review creates an awareness of\n\npotential biases and discrimination resulting from algorithmic decision-making in\n\nthe context of HR recruitment and HR development. Second, this study contributes\n\nto the current literature by informing both researchers and practitioners about the\n\npotential dangers of algorithmic decision-making in the HRM context. Finally, we\n\nguide future research directions with an understanding of existing knowledge and\n\ngaps in the literature. To this end, the present paper conducts a systematic review of\n\nthe current literature with a focus on HR recruitment and HR development. These\n\ntwo HR functions deal with the potential of future and current employees and the\n\n(automatic) prediction of person-organization fit, career development, and future\n\nperformance (Huselid 1995; Walker 2012). Decisions made by algorithms and AI in\n\nthese two important HR areas have serious consequences for individuals, the\n\ncompany, and society concerning ethics and both procedural and distributive\n\nfairness (Ötting and Maier 2018; Lee 2018; Tambe et al. 2019; Cappelli et al. 2020).\n\nOur study contributes to the existing body of research in several ways. First, the\n\nsystematic literature review contributes to the literature by highlighting the current\n\ndebate on ethical issues associated with algorithmic decision-making, including bias\n\nand discrimination (Barocas and Selbst 2016). Second, our research provides\n\nillustrative examples of various algorithmic decision-making tools used in HR\n\nrecruitment, HR development, and their potential for discrimination and perceived\n\nfairness. Moreover, our systematic review underlines the fact that it is a timely topic\n\ngaining enormous importance. Companies will face legal and reputational risk if\n\ntheir HR recruitment and HR development methods turn out to be discriminatory,\n\nand applicants and employees may consider the algorithmic selection or develop-\n\nment process to be unfair.\n\nFor this reason, companies need to know that the use of algorithmic decision-\n\nmaking can yield to discrimination, unfairness, and dissatisfaction in the context of\n\nBusiness Research (2020) 13:795–848 797\n\n123\n\n\n\nHRM. We offer an understanding of how discrimination might arise when\n\nimplementing algorithmic decision-making. We try to give guidance on how\n\ndiscrimination and perceived unfairness could be avoided and provide detailed\n\ndirections for future research in the existing literature, especially in the HRM field.\n\nMoreover, we identify several research gaps, mainly a lacking focus on perceived\n\nfairness.\n\nThe paper is organized as follows: first, we give an understanding of key terms\n\nand definitions. Afterward, we present the methodology of our systematic literature\n\nreview accompanied by a descriptive analysis of the reviewed literature. This is\n\nfollowed by an illustration of the current state of knowledge on algorithmic\n\ndecision-making and subsequent discussion. Finally, we offer practical as well as\n\ntheoretical implications and outline future research avenues.\n\n2 Conceptual background and definitions\n\n2.1 Definition of algorithms\n\nThe Oxford Living Dictionary defines algorithms as ‘‘processes or sets of rules to be\n\nfollowed in calculations or other problem-solving operations, especially by a\n\ncomputer.’’ Möhlmann and Zalmanson (2017) refer to algorithmic decision-making\n\nas automated decision-making and remote control, and standardization of routinized\n\nworkplace decision. Thus, in this paper, we use the term algorithmic decision-\n\nmaking to describe a computational mechanism that autonomously makes decisions\n\nbased on rules and statistical models without explicit human interference (Lee\n\n2018). Algorithms are the basis for several AI decision tools.\n\nAI is an umbrella term for a wide array of models, methods, and prescriptions\n\nused to simulate human intelligence, often when it comes to collecting, processing,\n\nand acting on data. AI applications can apply rules, learn over time through the\n\nacquisition of new data and information, and adapt to changes in the environment\n\n(Russell and Norvig 2016). AI includes several different research areas, such as\n\nmachine learning (ML), speech and image recognition, and natural language\n\nprocessing (NLP) (Kaplan and Haenlein 2019; Paschen et al. 2020).\n\nAs mentioned, the basis for many AI decision-making tools used in HR are ML\n\nalgorithms, which can be categorized into three major types: supervised, unsuper-\n\nvised, and reinforcement learning (Lee and Shin 2020). Supervised ML algorithms\n\naim to make predictions (often divided into classification- or regression-type\n\nproblems), given the input data and desired outputs considered as the ground truth.\n\nHuman experts often provide these labels and thus provide the algorithm with the\n\nground truth. To replicate human decisions or to make predictions, the algorithm\n\nlearns patterns from the labeled data and develops rules, which can be applied for\n\nfuture instances for the same problem (Canhoto and Clear 2020). In contrast, in\n\nunsupervised ML, only input data are given, and the model learns patterns from the\n\ndata without a priori labeling (Murphy 2012). Unsupervised ML algorithms capture\n\nthe structural behaviors of variables in the input data for theme analysis or grouping\n\n798 Business Research (2020) 13:795–848\n\n123\n\n\n\ndata (Canhoto and Clear 2020). Finally, reinforcement learning, as a separate group\n\nof methods, is not based on fixed input/output data. Instead, the ML algorithm learns\n\nbehavior through trial-and-error interactions with a dynamic environment (Kael-\n\nbling et al. 1996).\n\nFurthermore, instead of grouping ML models as supervised, unsupervised, or\n\nreinforcement type learning, the methodologies of algorithms may also be used to\n\ncategorize ML models. Examples are probabilistic models, which may be used in\n\nsupervised or unsupervised settings (Murphy 2012), or deep learning models (Lee\n\nand Shin 2020), which rely on artificial neural networks and perform complex\n\nlearning tasks. In supervised settings, neural network models often determine the\n\nrelationship between input and output using network structures containing the so-\n\ncalled hidden layers, meaning phases of transformation of the input data. Single\n\nnodes of these layers (neurons) were first modeled after neurons in the human brain,\n\nand they resemble human thinking (Bengio et al. 2017). In other settings, deep\n\nlearning may be used, for instance, to (1) process information through multiple\n\nstages of nonlinear transformation; or (2) determine features, representations of the\n\ndata providing an advantage for, e.g., prediction tasks (Deng and Yu 2014).\n\n2.2 Reason for biases\n\nFor any estimation bY of a random variable Y , bias refers to the difference between\n\nthe expected values of bY and Y and is also referred to as systematic error\n\n(Kauermann and Kuechenhoff 2010; Goodfellow et al. 2016). Cognitive biases,\n\nspecifically, are systematic errors in human judgment when dealing with uncertainty\n\n(Kahneman et al. 1982). These cognitive biases are thought to be transferred to\n\nalgorithmic evaluations or predictions, where bias may refer to ‘‘computer systems\n\nthat systematically and unfairly discriminate against certain individuals or groups in\n\nfavor of others’’ (Friedman and Nissenbaum 1996, p. 332).\n\nAlgorithms are often characterized as ‘‘black box’’. In the context of HRM,\n\nCheng and Hackett (2019) characterize algorithms as ‘‘glass boxes’’, since some,\n\nbut not all, components of the theory are reflective. In this context, the consideration\n\nand distinction of the three core elements are necessary, namely, transparency,\n\ninterpretability, and explainability (Roscher et al. 2020). Transparency is concerned\n\nwith the ML approach, while interpretability is concerned with the ML model in\n\ncombination with the data, which means the making sense of the obtained ML\n\nmodel (Roscher et al. 2020). Finally, explainability comprises the model, the data,\n\nand human involvement (Roscher et al. 2020). Concerning the former, transparency\n\ncan be distinguished at three different levels: ‘‘[…] at the level of the entire model\n\n(simulatability), at the level of individual components, such as parameters\n\n(decomposability), and at the level of the training (algorithmic transparency)’’\n\n(Roscher et al. 2020, p. 4). Interpretability concerns the characteristics of an ML\n\nmodel that need to be understood by a human (Roscher et al. 2020). Finally, the\n\nelement of explainability is paramount in HRM. Contextual information of human\n\nand their knowledge from the domain of HRM are necessary to explain the different\n\nsets of interpretations and derive conclusions about the results of the algorithms\n\nBusiness Research (2020) 13:795–848 799\n\n123\n\n\n\n(Roscher et al. 2020). Especially in HRM, in which ML algorithms are increasingly\n\nused for prediction of variables of interest to the HR department (e.g., personality\n\ncharacteristics, employee satisfaction, and turnover intentions), it is essential to\n\nunderstand how the ML algorithm operates (e.g., how the ML algorithm uses data\n\nand weighs specific criteria) and the underlying reasons for the produced decision.\n\nIn the following, we will outline the main reasons for biases in algorithmic\n\ndecision-making and briefly summarize different biases, namely historical, repre-\n\nsentation, technical, and emergent bias. One of the main reasons for bias in\n\nalgorithmic decision-making is the quality of input data, because algorithms learn\n\nfrom historical data as an example; thus, the learning process depends on the\n\nexposed examples (Friedman and Nissenbaum 1996; Barocas and Selbst 2016;\n\nDanks and London 2017). The input data are usually historical. Consequently, if the\n\ninput data set is biased in one way or another, the subsequent analysis is biased, as\n\nwell (keyword: ‘‘garbage in, garbage out’’). For example, if the input data of an\n\nalgorithm include implicit or explicit human judgments, stereotypes, or biases, an\n\naccurate algorithmic output will inevitably entail these human judgments, stereo-\n\ntypes, and prejudices (Diakopoulos 2015; Suresh and Guttag 2019; Barfield and\n\nPagallo 2018). This bias usually exists before the creation of the system and may not\n\nbe apparent at first glance. In turn, the algorithm replicates these preexisting biases,\n\nbecause it treats all information, in which a certain kind of discrimination or bias is\n\nembedded, as a valid example (Barocas and Selbst 2016; Lindebaum et al. 2019). In\n\nthe worst case, the algorithm can yield racist or discriminatory outputs (Veale and\n\nBinns 2017). Algorithms exhibit these tendencies, even if it is not the intention of\n\nthe manual programming since they compound the historical biases of the past.\n\nThus, any predictive algorithmic decision-making tool built on historical data may\n\ninherit historical biases (Datta et al. 2015).\n\nAs an example from the recruitment process, if an algorithm is trained on\n\nhistorical employment data, integrating an implicit bias that favors white men over\n\nHispanics, then, without even being fed data on gender or ethnicity, an algorithm\n\nmay recognize patterns in the data, which expose an applicant as a member of a\n\ncertain protected group, which, historically, is less likely to be chosen for a job\n\ninterview. This, in turn, may lead to a systematic disadvantage of certain groups,\n\neven if the designer has no intention of marginalizing people based on these\n\ncategories and if the algorithm is not directly given this information (Barocas and\n\nSelbst 2016).\n\nAnother reason for biases in algorithms related to the input data is that certain\n\ngroups or characteristics are mostly underrepresented or sometimes overrepre-\n\nsented, which is also called representation bias (Barocas and Selbst 2016; Suresh\n\nand Guttag 2019; Barfield and Pagallo 2018). Any decision based on this kind of\n\nbiased data might lead to disadvantages of groups of individuals who are\n\nunderrepresented or overrepresented (Barocas and Selbst 2016). Another reason\n\nfor representation bias can be the absence of specific information (Barfield and\n\nPagallo 2018). Thus, not only the selection of measurements but also the\n\npreprocessing of the measurement data might yield to bias. ML models often\n\nevolve in several steps of feature engineering or model testing, since there is no\n\nuniversally best model (as shown in the ‘‘no free lunch’’ theorems, [see Wolpert and\n\n800 Business Research (2020) 13:795–848\n\n123\n\n\n\nMacready (1997)]. Here, the choice of the benchmark or rather the value indicating\n\nthe performance of the model is optimized through rotations of different\n\nrepresentations of the data and methods for prediction. For example, representative\n\nbias might occur if females in comparison to males are underrepresented in the\n\ntraining data of an algorithm. Hence, the outcome could be in favor of the\n\noverrepresented group (i.e., males) and, hence, lead to discriminatory outcomes.\n\nTechnical bias may arise from technical constraints or technical consideration for\n\nseveral reasons. For example, technical bias can originate from limited ‘‘[…]\n\ncomputer technology, including hardware, software, and peripherals’’ (Friedman\n\nand Nissenbaum 1996, p. 334). Another reason could be a decontextualized\n\nalgorithm that does not manage to treat all groups fairly under all important\n\nconditions (Friedman and Nissenbaum 1996; Bozdag 2013). The formalization of\n\nhuman constructs to computers can be another problem leading to technical bias.\n\nHuman constructs, such as judgments or intuitions, are often hard to quantify, which\n\nmakes it difficult or even impossible to translate them to the computer (Friedman\n\nand Nissenbaum 1996). As an example, the human interpretation of law can be\n\nambiguous and highly dependent on the specific context, making it difficult for an\n\nalgorithmic system to correctly advise in litigation (c.f., Friedman and Nissenbaum\n\n1996).\n\nIn the context of real users, emergent bias may arise. Typically, this bias occurs\n\nafter the construction as a result of changed societal knowledge, population, or\n\ncultural values (Friedman and Nissenbaum 1996). Consequently, a shift in the\n\ncontext of use might yield to problems and an emergent bias due to two reasons,\n\nnamely ‘‘new societal knowledge’’ and ‘‘mismatch between users and system\n\ndesign’’ (see Table 1 in Friedman and Nissenbaum 1996, p. 335). If it is not possible\n\nto incorporate new knowledge in society into the system design, emergent bias due\n\nto new societal knowledge occurs. The mismatch between users and system design\n\ncan occur due to changes in state-of-the-art-research or due to different values. Also,\n\nemergent bias can occur if a population uses the system with different values than\n\nthose assumed in the design process (Friedman and Nissenbaum 1996). Problems\n\noccur, for example, when users originate from a cultural context that avoids\n\ncompetition and promotes cooperative efforts, while the algorithm is trained to\n\nreward individualistic and competitive behavior (Friedman and Nissenbaum 1996).\n\n2.3 Fairness and discrimination in information systems\n\nLeventhal (1980) describes fairness as equal treatment based on people’s\n\nperformance and needs. Table 1 offers an overview of the different fairness\n\ndefinitions. Individual fairness means that, independent of group membership, two\n\nindividuals who are perceived to be similar by the measures at hand should also be\n\ntreated similarly (Dwork et al. 2012). Rising from the micro-level onto the meso-\n\nlevel, Dwork et al. (2012) also proposed another measure of fairness, that is, group\n\nfairness, in which entire (protected) groups of people are required to be treated\n\nsimilarly (statistical parity). Hardt et al. (2016) extended these notions by including\n\ntrue outcomes of predicted variables to achieve fair treatment. In their sense, false-\n\nBusiness Research (2020) 13:795–848 801\n\n123\n\n\n\npositives/negatives are sources of disadvantage and should be equal among groups\n\nmeans equal opportunity for false-positives/negatives (Hardt et al. 2016).\n\nUnfair treatment of certain groups of people or individual subjects yields to\n\ndiscrimination. Discrimination is defined as the unequal treatment of different\n\ngroups (Arrow 1973). Discrimination is very similar to unfairness. Discriminatory\n\ncategories can be strongly correlated with non-discriminatory categories, such as\n\nage (i.e., discriminatory) and years of working experience (non-discriminatory)\n\n(Persson 2016). Also, there is a difference between implicit and explicit\n\ndiscrimination. Implicit discrimination is based on implicit attitudes or stereotypes\n\nand often unintentional (Bertrand et al. 2005). In contrast, explicit discrimination is\n\na conscious process due to an aversion to certain groups of people. In HR\n\nrecruitment and HR development, discrimination means the not-hiring or support of\n\na person due to characteristics not related to that person’s productivity in the current\n\nposition (Frijters 1998).\n\nThe HR literature, especially the literature on personnel selection, is concerned\n\nwith fairness in hiring decisions, because every selection measure of individual\n\ndifferences is inevitably discriminatory (Cascio and Aguinis 2013). However, the\n\nquestion arises ‘‘whether the measure discriminates unfairly’’ (Cascio and Aguinis\n\n2013, p. 183). Hence, the actual fairness of prediction systems needs to be tested\n\nbased on probabilities and estimates, which we refer to as objective fairness. In the\n\nselection context, the literature distinguishes between differential validity (i.e.,\n\ndifferences in subgroup validity) and differential prediction (i.e., differences in\n\nslopes and intercepts of subgroups), and both might lead to biased results (Meade\n\nand Fetzer 2009; Roth et al. 2017; Bobko and Bartlett 1978).\n\nIn HR recruitment and HR development, both objective fairness and subjective\n\nfairness perceptions of applicants and employees about the usage of algorithmic\n\ndecision-making need to be considered. In this regard, perceived fairness or justice\n\nis more a subjective and descriptive personal evaluation rather than an objective\n\nreality (Cropanzano et al. 2007). Subjective fairness plays an essential role in the\n\nrelationship between humans and their employers. Previous studies showed that the\n\nTable 1 Definitions of fairness\n\nName Author Definition\n\nIndividual\n\nfairness\n\nDwork et al.\n\n(2012)\n\n‘‘Similar’’ subjects should have ‘‘similar’’ classifications\n\nGroup\n\nfairness\n\nSubjects in protected and unprotected groups have an equal probability\n\nof being assigned positive\n\nP bY ¼ 1\n� �\n\n�\n\n�G ¼ 1Þ ¼ Pð bY ¼ 1jG ¼ 0Þ\n\nEqual\n\nopportunity\n\nHardt et al.\n\n(2016)\n\nFalse-negative rates should be equal\n\nP bY ¼ 0\n� �\n\n�\n\n�Y ¼ 1;G ¼ 1Þ ¼ Pð bY ¼ 0jY ¼ 1;G ¼ 0Þ\n\nY 2 0; 1f g is a random variable describing, e.g., the recidivism of a subject, bY its estimator and G 2\nf0; 1g; describes whether a subject is a member of a certain protected group (G ¼ 1Þ or not ðG ¼ 0Þ\n\n802 Business Research (2020) 13:795–848\n\n123\n\n\n\nlikelihood of conscientious behavior and altruisms is higher for employees who feel\n\ntreated fairly (Cohen-Charash and Spector 2001). Conversely, unfairness can have\n\nconsiderable adverse consequences. For example, in the recruitment context,\n\nfairness perceptions of candidates during the selection process have important\n\nconsequences for decision to stay in the applicant pool or accept a job offer (Bauer\n\net al. 2001). Therefore, it is crucial to know how people feel about algorithmic\n\ndecision-making taking over managerial decisions formerly made by humans, since\n\nthe fairness perceptions during the recruitment process and/or training process have\n\nessential and meaningful effects on attitudes, performance, morale, intentions, and\n\nbehavior (e.g., the acceptance or rejection of a job offer or job turnover, job\n\ndissatisfaction, and reduction or elimination of conflicts) (Gilliland 1993; McCarthy\n\net al. 2017; Hausknecht et al. 2004; Cropanzano et al. 2007; Cohen-Charash and\n\nSpector 2001). Moreover, negative experiences might damage the employer�s\nimage. Several online platforms offer the possibility of rating companies and their\n\nrecruitment and development process (Van Hoye 2013; Woods et al. 2020).\n\nConsidering justice and fairness in the organizational context (Gilliland 1993),\n\nthere are three core dimensions of justice: distributive, procedural, and interactional.\n\nThe three dimensions tend to be correlated. Distributive justice deals with the\n\noutcome that some humans receive and some do not (Cropanzano et al. 2007). Rules\n\nthat can lead to distributive justice are ‘‘[…] equality (to each the same), equity (to\n\neach in accordance with contributions, and need (to each in accordance with the\n\nmost urgency)’’ (Cropanzano et al. 2007, p. 37). To some extent, especially\n\nconcerning equity, this can be connected with individual fairness and group fairness\n\nfrom Dwork et al. (2012) and equal opportunities from Hardt et al. (2016).\n\nProcedural justice means that the process is consistent with all humans, not\n\nincluding bias, accurate, and consistent with the ethical norms (Cropanzano et al.\n\n2007; Leventhal 1980). Consistency plays an essential role in procedural justice,\n\nmeaning that all employees and all candidates need to receive the same treatment.\n\nAdditionally, the lack of bias, accuracy, representation of all parties, correction, and\n\nethics play an important role in achieving a high procedural justice (Cropanzano\n\net al. 2007). In contrast, interactional justice is about the treatment of humans,\n\nmeaning the appropriateness of the treatment from another member of the company,\n\nthe treatment with dignity, courtesy, and respect, and informational justice (share of\n\nrelevant information) (Cropanzano et al. 2007).\n\nIn general, algorithmic decision-making increases the standardization of\n\nprocedures, so that decisions should be more objective and less biased, and errors\n\nshould occur less frequently (Kaibel et al. 2019), since information processing by\n\nhuman raters can be unsystematic, leading to contradictory and insufficient\n\nevidence-based decisions (Woods et al. 2020). Consequently, procedural justice and\n\ndistributive justice are higher using algorithmic decision-making, because the\n\nprocess is more standardized, which still not means that it is without bias.\n\nHowever, especially in the context of an application or an employee evaluation, it\n\nis not only about how fair the procedure itself is (according to fairness measures),\n\nbut it is also about how people involved in the decision process perceive the fairness\n\nof the whole process. Often the personal contact, which characterizes the\n\nBusiness Research (2020) 13:795–848 803\n\n123\n\n\n\ninteractional fairness, is missing when using algorithmic decision-making. It is\n\ndifficult to fulfill all three fairness dimensions.\n\n3 Methods\n\nThis systematic literature review aims at offering a coherent, transparent, and\n\nreliable picture of existing knowledge and providing insights into fruitful research\n\navenues about the discrimination potential and fairness when using algorithmic\n\ndecision-making in HR recruitment and HR development. This is in line with other\n\nsystematic literature reviews that organize, evaluate, and synthesize knowledge in a\n\nparticular field and provide an overall picture of knowledge and suggestions for\n\nfuture research (Petticrew and Roberts 2008; Crossan and Apaydin 2010; Siddaway\n\net al. 2019). To this end, we followed the systematic literature review approach\n\ndescribed by Siddaway et al. (2019) and Gough et al. (2017) to ensure a methodical,\n\ntransparent, and replicable approach.1\n\n3.1 Search terms and databases\n\nWe engaged in an extensive keyword searching, which we derived in an iterative\n\nprocess of search and discussion between the two authors of this study (see\n\n‘‘Appendix’’ for the employed keywords). According to our research question, we\n\nfirst defined individual concepts to create search terms. We considered different\n\nterminology, including synonyms, singular/plural forms, different spellings, broader\n\nvs. narrow terms, and classification terms of databases to categorize contents\n\n(Siddaway et al. 2019) (see Table 2 for a complete list of employed keywords and\n\nsearch strings). Our priority was to achieve the balance between sensitivity and\n\nspecificity to get broad coverage of the literature and to avoid the unintentional\n\nomission of relevant articles (Siddaway et al. 2019).\n\nAs the first source of data, we used the social science citation index (SSCI) to\n\nensure broad coverage of scholarly literature. This database covers English-\n\nlanguage peer-reviewed journals in business and management. As part of the Web\n\nof Knowledge, the database includes all journals with an impact factor, which is a\n\nreasonable proxy for the most important publications in the field. We completed our\n\nsearch with the EBSCO Business Source Premier database to add further breadth.\n\nSince electronic databases are not fully comprehensive, we additionally searched in\n\nthe reference section of the considered papers and manually searched for articles\n\n(Siddaway et al. 2019).\n\nWe considered scholarly articles from a high-quality source of evidence (peer-\n\nreviewed and published) journals in English and excluded book reviews, comments,\n\nand editorial notes. Moreover, we searched for unpublished articles in conference\n\nproceedings from renowned conferences, such as AOM, EURAM, ACM, and IEEE,\n\nand contacted the authors to prevent publication bias and to gain further valuable\n\n1 We thank the anonymous reviewer for this valuable recommendation.\n\n804 Business Research (2020) 13:795–848\n\n123\n\n\n\ninsights (Siddaway et al. 2019; Lipsey and Wilson 2001; Ferguson and Brannick\n\n2012). In April 2020, this search approach resulted in 3207 articles.\n\n3.2 Screening, eligibility process, and inclusion process\n\nFollowing this initial identification, we manually screened each article (title and\n\nabstract) to evaluate whether its content was fundamental relevant to impact bias,\n\ndiscrimination, or fairness of algorithmic decision-making in HRM, especially in\n\nrecruitment, selection, development, and training in particular. The process of\n\nTable 2 Overview of search terms, databases, and results\n\nSearch string Database Resultsa\n\nTITLE: (‘‘algorithm* OR algorithmic model* OR data-algorithm*OR algorithmic decision-making OR\n\nalgorithmic decision* OR artificial intelligence OR facial expression tool* OR facial expression\n\nprocessing* OR language processing* OR natural language processing* OR recommender system* OR\n\nsearch engine* OR data*OR data set*’’)\n\nTOPIC: (‘‘discrimination* OR discriminat* OR classification* OR ‘‘classification problem*’’ OR\n\n‘‘classification scheme*’’ OR ‘‘algorithmic discrimination*’’ OR ‘‘algorithmic bias discrimination*’’\n\nOR ‘‘preventing discrimination*’’ OR anti-discrimination* OR non-discrimination* OR gender, age,\n\nsex, sexism, origin OR ‘‘difference* among demographic group*’’ OR ethic* OR ‘‘ethical\n\nimplication*’’ OR ‘‘data mining discrimination*’’ OR ‘‘unfair treatment*’’ OR fair* OR unfair* OR\n\n‘‘perceived fairness’’ OR ‘‘algorithmic fairness’’ OR ‘‘fairness word*’’ OR ‘‘fairness speech*’’ OR\n\n‘‘fairness recommendation*’’ OR equal* OR equit* OR inequal* OR ‘‘equal opportunit*’’ OR\n\ntransparen* OR legal* OR right* OR truth OR impartial* OR correct*OR evaluat* OR judgement* OR\n\n‘‘algorithmic judgement*’’ OR ‘‘human judgement*’’ OR ‘‘mechanical judgement*’’ OR rank* OR\n\nrate* OR measure* OR valuation* OR bias* OR ‘‘algorithmic bias*’’ OR ‘‘national bias*’’ OR gender-\n\nbias* OR ‘‘decision-making bias*’’ OR ‘‘human bias* OR ‘‘technical bias*’’ OR ‘‘implicit bias* in\n\nalgorithm*’’ OR ‘‘dealing with bias*’’ OR ‘‘pattern distortion*’’ OR pre-justice* OR tendenc* OR\n\nprone*OR justiceb OR adverse impactb) AND TOPIC: (‘‘Human Resource*’’ OR ‘‘Human Resource\n\nManagement’’ OR Management OR ‘‘applicant selection*’’ OR ‘‘employee selection*’’ OR ‘‘algorithm-\n\nbased HR decision-making’’ OR ‘‘recruitment process* OR ‘‘application process*’’ OR ‘‘selection\n\nprocess*’’ OR recruitment* OR online-recruitment* OR ‘‘personnel decision*’’, OR ‘‘personnel\n\nselection*’’ OR ‘‘people analytic*’’ OR ‘‘HR analytic*’’ OR ‘‘job advertisement*’’ OR ‘‘online\n\npersonalization*’’)\n\nDOCUMENT TYPES = (ARTICLE)\n\nAND\n\nLANGUAGES = (ENGLISH)\n\nSSCI\n\npsychology, psychology experimental, psychology\n\nmultidisciplinary science, ethics, law, psychology\n\napplied, operations research management science,\n\ncomputer science artificial intelligence, computer\n\nscience interdisciplinary applications, computer\n\nscience information systems, management,\n\nbusiness, behavioral science, social sciences\n\ninterdisciplinary, sociology, social issues,\n\nhumanities interdisciplinary\n\n2892\n\narticles\n\nScholarly (Peer Reviewed) Journals,\n\nAcademic Journal, Article English\n\nEBSCO Business Source Premier 244\n\narticles\n\naResults show the gross hits per search string and database for scholarly articles\nbRobustness check\n\nBusiness Research (2020) 13:795–848 805\n\n123\n\n\n\nrelevance screening resulted in 102 articles that were deemed to be substantially\n\nrelevant.\n\nSecond, we conducted the eligibility stage by reading the full text and shifting\n\nfrom sensitivity to specificity. Studies eligible for our review (1) had to be\n\nconsistent with our definition of algorithmic decision-making as well as with our\n\ndefinitions of fairness, bias, or discrimination (2), and the content had to refer to\n\nHRM (3). The list of studies that we excluded at the eligibility stage is available\n\nupon request. The two authors independently checked each paper to increase the\n\nreliability of the research results. We applied this structured approach to ensure a\n\nhigh level of objectivity.\n\nAfterward, the actual review started, and we synthesized and assessed our\n\nfindings. We analyzed the material abductively following a set of predefined\n\ncategories without, however, relying on preexisting codes to extract all relevant\n\ninformation. Analytic categories were, for example, ‘‘research design,’’ ‘‘field of the\n\njournal,’’ ‘‘research geography,’’ or ‘‘year of publication,’’ and ‘‘key findings.’’\n\nAgain, the authors filled these categories with their inductively generated codes.\n\nOur systematic review used the Preferred Reporting Items for Systematic\n\nReviews (PRISMA) recommendations, including assessment of research content as\n\nwell as a detailed report of the number of records identified through the search and\n\nthe number of studies included and excluded in the review. Figure 1 presents a\n\nPRISMA flow diagram to provide a succinct summary of the process (Siddaway\n\net al. 2019; Moher et al. 2009).\n\n3.3 Robustness check\n\nWe implemented a robustness check to offer a reliable and coherent picture of the\n\ndiscrimination potential and fairness when using algorithmic decision-making in\n\nHR recruitment and HR development. With the robustness check, we want to ensure\n\nthat all relevant articles were included in the literature review. We conducted the\n\nrobustness check 3 months after the actual search process with two additional\n\nkeywords, namely: ‘‘justice’’ and ‘‘adverse impact’’ (see Table 2). The search in the\n\ndatabase SSCI resulted in 632 articles and the EBSCO search in 690 articles. We\n\nmanually screened each article (title and abstract) to assess whether the content was\n\nessentially relevant to bias, discrimination, or the fairness of algorithmic decision-\n\nmaking in HRM, especially recruitment, selection, training, and development. The\n\nmajority of articles dealt with the fairness of algorithmic decision-making, but had\n\nno reference to HR. After manually screening each article, the process of relevance\n\nscreening resulted in eight articles for the eligibility stage. We found that no further\n\narticles can be included in the literature review by reading the full text. Since out of\n\nthese eight articles, three articles were already included in the literature review (Lee\n\n2018; Tambe et al. 2019; Yarger et al. 2019), two articles were excluded in the\n\neligibility stage of the initial search process (Hoffmann 2019; Sumser 2017) (no\n\nreference to HRM and comment), and the remaining three articles neither discussed\n\nfairness nor the HR recruitment and/or HR development context (Varghese et al.\n\n1988; Horton 2017; Gil-Lafuente and Oh 2012). The robustness check verified that\n\nthe literature review offers a reliable and transparent picture of the current literature\n\n806 Business Research (2020) 13:795–848\n\n123\n\n\n\nregarding the discrimination potential and fairness when using algorithmic decision-\n\nmaking in HR recruitment and HR development.\n\n3.4 Limitations of the research process\n\nThis approach is not without limitations. First, the reliance on two databases might\n\nbe regarded as a limitation; however, the approach of selecting two broad and\n\ncommon databases contributed to the validity and replicability of our findings due to\n\nthe extensive coverage of high-impact, peer-reviewed journals in these databases\n\n(Podsakoff et al. 2005). Second, our review focused on two essential HR functions\n\nthat have severe consequences for individuals and society concerning ethics, namely\n\nHR recruitment and HR development. We did not consider other areas of HRM,\n\nsince the focus of other HR functions is mainly the automation process (e.g., pay or\n\nanother administrative task). Thus, the situation is different in HR recruitment and\n\nHR development, because societal decisions are made, which have crucial\n\nconsequences for the individual applicants and employees, such as job offer or\n\npromotion opportunities. Especially when it comes to decisions about individuals\n\nRecords identified through \ndatabase searching\n\n(n = 3,136)\nSc\nre\nen\nin\ng\n\nIn\ncl\nud\n\ned\nEl\nig\nib\nili\nty\n\nId\nen\ntif\nic\nat\nio\nn\n\nAdditional records identified \nthrough other sources\n\n(n = 71)\n\nRecords after duplicates removed\n(n = 3,204)\n\nRecords screened\n(n = 3,204)\n\nRecords excludeda\n\n(n = 3,102)\n\nFull-text articles \nassessed for eligibility\n\n(n = 102)\n\nFull-text articles \nexcluded, with reasonsb\n\n(n = 66)\n\nStudies included in \nliterature review\n\n(n = 36)\n\nFig. 1 PRISMA flow diagram illustrating the process. aTopic did not fit, mostly no HR and/or fairness,\nno obvious discrimination context, bMostly no HR and/or fairness, no discrimination context after\nreading the full text or not meeting the inclusion criteria\n\nBusiness Research (2020) 13:795–848 807\n\n123\n\n\n\nand their potential, objective and perceived fairness is paramount (Ötting and Maier\n\n2018; Lee 2018).\n\nMoreover, only articles written in the English-language were part of the literature\n\nreview. Even though this procedure is accepted practice and there is some evidence\n\nthat including only English articles does not bias the results, it should be noted that\n\nnon-English articles were not included because English is the dominant language in\n\nresearch (Morrison et al. 2012).\n\n4 Descriptive results\n\nThe following section shows the current research landscape. We summarize the\n\nmain characteristics of the identified articles in Table 3 and present the main\n\nfindings in Table 4. This table reports the name of authors, year of publication, the\n\nmain focus of the study (i.e., focus on bias, discrimination, fairness, or perceived\n\nfairness), applied method, the field of research, algorithmic decision-making\n\nsystem, HR context (i.e., recruitment- distinguished between recruitment and\n\nselection- or development), and the key findings. We analyze the main focus and the\n\nkey findings of the studies in the following sections. The table is sorted by the focus\n\nof the article and whether it is on bias as a trigger for unfairness and discrimination\n\nor specifically on fairness and discrimination.\n\nFigure 2 illustrates the distribution of publications over time and the research\n\nmethods used. The first identified article in our sample of literature was published in\n\n2014. From 2014 to 2016, only a few articles are published per year. From 2017,\n\ninterest in algorithmic decision-making and discrimination increased notably. As\n\nshown in Fig. 2, there was enormous interest in the topic in 2019.\n\nFrom a methodological perspective, another noteworthy result of this systematic\n\nreview is the predominance of non-empirical evidence, as Table 3 and Fig. 2 show\n\nthat the large majority of articles are non-empirical (i.e., conceptual paper, reviews,\n\nand case studies). A reason for this is that scientific investigation of discrimination\n\nby algorithmic decision-making represents a relatively new topic. However, the\n\nnumber of quantitative papers increased from 2018. Most of the studies focused on\n\nbias, discrimination, and objective fairness, while 12 studies examined perceived\n\nfairness perceptions of applicants and employees (see Table 1). Furthermore, the\n\nmajority of studies are located in the area of recruitment and selection, whereby\n\nthese studies mostly focus on selection. Twelve studies are located in the area of HR\n\ndevelopment. The majority of studies provided either no geographical specification\n\nor were conducted in the USA (see Table 3).\n\nThirteen articles originate from management, and fourteen articles originate\n\nfrom computer science, four articles originate from law, two from psychology, two\n\nfrom information systems, and one from the behavioral sciences. This distribution\n\nillustrates that the field does not have a core in business and management research\n\nand is rather interdisciplinary. Nevertheless, the majority of articles originating from\n\nmanagement were published in high-ranked journals, such as Journal of Business\nEthics, Human Resource Management Review, Management Science, Academy of\nManagement Annals, and Journal of Management. The majority of these studies\n\n808 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nO\nv\ner\nv\nie\nw\n\no\nf\nst\nu\nd\nie\ns\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nN\nai\nm\n\net\nal\n.\n(2\n0\n1\n6\n)\n\nT\nh\ne\nau\nto\nm\nat\ned\n\nan\nal\ny\nsi\ns\no\nf\nfa\nci\nal\n\nex\np\nre\nss\nio\nn\ns,\n\nla\nn\ng\nu\nag\ne,\n\nan\nd\n\np\nro\nso\nd\nic\n\nin\nfo\nrm\n\nat\nio\nn\no\nf\n\nin\nte\nrv\nie\nw\nee\ns\nin\n\na\n\njo\nb\nin\nte\nrv\nie\nw\n\nB\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nan\nal\ny\nsi\ns\no\nf\n\nin\nte\nrv\nie\nw\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nN\nL\nP\n,\nF\nE\nP\n\nS\nel\nec\nti\no\nn\n\nR\nec\no\nm\nm\nen\nd\ns\nto\n\nsp\nea\nk\n\nm\no\nre\n\nfl\nu\nen\ntl\ny\n,\nu\nse\n\nle\nss\n\nfi\nll\ner\n\nw\no\nrd\ns,\nan\nd\nsm\n\nil\ne\n\nm\no\nre\n\nS\nh\no\nw\ns\nth\nat\n\nth\ne\nst\nu\nd\nen\nts\n\nw\nh\no\nw\ner\ne\nra\nte\nd\nh\nig\nh\nly\n\nw\nh\nil\ne\nan\nsw\n\ner\nin\ng\nth\ne\n\nfi\nrs\nt\nin\nte\nrv\nie\nw\n\nq\nu\nes\nti\no\nn\n\nw\ner\ne\nal\nso\n\nra\nte\nd\nh\nig\nh\nly\n\no\nv\ner\nal\nl\n(i\n.e\n.,\nfi\nrs\nt\n\nim\np\nre\nss\nio\nn\nm\nat\nte\nrs\n)\n\nU\nS\nA\n\nC\nh\nen\ng\nan\nd\n\nH\nac\nk\net\nt\n(2\n0\n1\n9\n)\n\nA\ncr\nit\nic\nal\n\nre\nv\nie\nw\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\nin\n\nH\nR\nM\n\nB\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nsi\nn\ng\nle\n\nca\nse\n\nst\nu\nd\ny\n;\nre\nv\nie\nw\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nO\nrg\nan\niz\nat\nio\nn\ns\nh\nav\ne\nto\n\nin\ncr\nea\nse\n\nth\ne\np\ner\nce\niv\ned\n\nau\nth\nen\nti\nci\nty\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\n\nN\nee\nd\nto\n\nev\nal\nu\nat\ne\n\nal\ng\no\nri\nth\nm\ns\nfr\no\nm\n\na\n\nre\nse\nar\nch\n\np\ner\nsp\nec\nti\nv\ne\n\nL\nac\nk\nb\net\nw\nee\nn\np\nra\nct\nic\ne\n\nan\nd\nre\nse\nar\nch\n\nN\no\nt sp\nec\nifi\ned\n\nBusiness Research (2020) 13:795–848 809\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nM\nan\nn\nan\nd\nO\n’N\n\nei\nl\n\n(2\n0\n1\n6\n)\n\nE\nx\np\nla\nin\ns\nw\nh\ny\n\nal\ng\no\nri\nth\nm\ns\nar\ne\n\nn\no\nt\nn\neu\ntr\nal\n\nan\nd\n\no\nff\ner\ns\nso\nm\ne\n\nim\np\nli\nca\nti\no\nn\ns\nto\n\nre\nd\nu\nce\n\nth\ne\nri\nsk\n\no\nf\n\nb\nia\nse\ns\no\nf\n\nal\ng\no\nri\nth\nm\nic\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\nin\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nS\nel\nec\nti\no\nn\n\nA\nlg\no\nri\nth\nm\ns\nre\nfl\nec\nt\n\nh\nu\nm\nan\n\nb\nia\nse\ns\nan\nd\n\np\nre\nju\nd\nic\nes\n\nth\nat\n\nle\nad\n\nto\n\nm\nac\nh\nin\ne\nle\nar\nn\nin\ng\n\nm\nis\nta\nk\nes\n\nan\nd\n\nm\nis\nin\nte\nrp\nre\nta\nti\no\nn\ns\n\nB\nia\ns\nan\nd\np\nre\nju\nd\nic\ne\n\nca\nn\nn\no\nt\nb\ne\nco\nm\np\nle\nte\nly\n\nel\nim\n\nin\nat\ned\n\nfr\no\nm\n\nh\nir\nin\ng\n\nH\nR\np\nro\nfe\nss\nio\nn\nal\ns\nm\nu\nst\n\nco\nn\nsi\nd\ner\n\nth\ne\n\nco\nn\nse\nq\nu\nen\nce\ns\no\nf\nth\nes\ne\n\nsy\nst\nem\n\ns\nan\nd\nen\nsu\nre\n\nth\ney\n\nal\nw\nay\ns\nre\nfl\nec\nt\nth\ne\n\nb\nes\nt\nh\nu\nm\nan\n\nin\nte\nn\nti\no\nn\ns\n\nU\nS\nA\n\nK\nim\n\n(2\n0\n1\n7\n)\n\nE\nx\nam\n\nin\nes\n\nth\ne\nu\nse\n\no\nf\n\ncl\nas\nsi\nfi\nca\nti\no\nn\n\nsc\nh\nem\n\nes\n/d\nat\na\n\nal\ng\no\nri\nth\nm\ns\nin\n\nte\nrm\n\ns\no\nf\n\np\ner\nso\nn\nn\nel\n\nd\nec\nis\nio\nn\ns\n\nS\nh\no\nw\ns\nli\nm\nit\nat\nio\nn\ns\nin\n\nex\nis\nti\nn\ng\nla\nw\n\nan\nd\n\nim\np\nro\nv\nem\n\nen\nt\n\np\nro\np\no\nsa\nls\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nen\ner\nal\n,\n\ncl\nas\nsi\nfi\nca\nti\no\nn\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\ntr\nai\nn\nin\ng\nan\nd\n\nd\nev\nel\no\np\nm\nen\nt\n\nB\nec\nau\nse\n\no\nf\nth\ne\nn\nat\nu\nre\n\no\nf\n\nd\nat\na\nm\nin\nin\ng\n\nte\nch\nn\niq\nu\nes\n,\nem\n\np\nlo\ny\ner\n\nre\nli\nan\nce\n\no\nn\nth\nes\ne\nto\no\nls\n\np\no\nse\ns\nn\no\nv\nel\n\nch\nal\nle\nn\ng\nes\n\nto\n\nw\no\nrk\np\nla\nce\n\neq\nu\nal\nit\ny\n\nU\nS\nA\n\n810 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\no\nse\nn\nb\nla\nt\net\n\nal\n.\n\n(2\n0\n1\n6\n)\n\nE\nx\nam\n\nin\nes\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nth\no\nro\nu\ng\nh\n\nev\nal\nu\nat\nio\nn\nin\n\nth\ne\n\nca\nse\n\no\nf\nU\nb\ner\n\nd\nri\nv\ner\ns\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nv\nal\nu\nat\nio\nn\n\nsy\nst\nem\n\ns\n\nD\nev\nel\no\np\nm\nen\nt\n\nT\nh\ne\nn\nee\nd\nto\n\nex\ner\nci\nse\n\nq\nu\nal\nit\ny\nco\nn\ntr\no\nl\no\nv\ner\n\na\n\nla\nrg\ne\nd\nis\nag\ng\nre\ng\nat\ned\n\nw\no\nrk\nfo\nrc\ne\nm\nay\n\np\ner\nm\nit\n\nth\ne\nco\nn\nti\nn\nu\ned\n\nu\nse\n\no\nf\n\nra\nti\nn\ng\n\nS\ny\nst\nem\n\ns\nu\nn\nd\ner\n\nex\nis\nti\nn\ng\n\nem\np\nlo\ny\nm\nen\nt\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\nla\nw\n\nN\no\nt sp\nec\nifi\ned\n\nS\nav\nag\ne\nan\nd\nB\nal\nes\n\n(2\n0\n1\n7\n)\n\nE\nx\nam\n\nin\nes\n\nh\no\nw\n\nv\nid\neo\n\ng\nam\n\ne\n\nal\ng\no\nri\nth\nm\ns\nar\ne\n\nin\nco\nrp\no\nra\nte\nd\nin\nto\n\nth\ne\njo\nb\nh\nir\nin\ng\n\np\nro\nce\nss\n\nS\nh\no\nw\ns\nth\ne\nd\neb\nat\ne\n\no\nv\ner\n\nw\nh\net\nh\ner\n\nth\nes\ne\nal\ng\no\nri\nth\nm\ns\n\nd\nis\ncr\nim\n\nin\nat\ne\n\nB\n,\nD\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nam\n\nifi\nca\nti\no\nn\n\nS\nel\nec\nti\no\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nV\nid\neo\n\ng\nam\n\nes\nin\n\nin\nit\nia\nl\n\nh\nir\nin\ng\nst\nag\nes\n\nca\nn\n\np\ner\nm\nit\nn\no\nn\n-\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\nev\nal\nu\nat\nio\nn\no\nf\nal\nl\nth\ne\n\nca\nn\nd\nid\nat\nes\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 811\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nW\nil\nli\nam\n\ns\net\n\nal\n.\n\n(2\n0\n1\n8\n)\n\nE\nx\nam\n\nin\nes\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nth\nro\nu\ng\nh\nth\ne\nu\nse\n\no\nf\n\nal\ng\no\nri\nth\nm\ns\nin\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\np\nro\nce\nss\nes\n\nP\nro\np\no\nse\ns\nst\nra\nte\ng\nie\ns\n\nfo\nr\nth\ne\np\nre\nv\nen\nti\no\nn\n\no\nf\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nm\nu\nlt\nip\nle\n\nca\nse\n\nst\nu\nd\nie\ns\n\nIn\nfo\nrm\n\nat\nio\nn\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nA\nlg\no\nri\nth\nm\nic\n\np\nre\nd\nic\nti\no\nn\n\nca\nn\nin\ncl\nu\nd\ne\nin\nju\nst\nic\nes\n\nT\nh\ne\np\nre\nd\nic\nti\no\nn\ns\nh\nav\ne\nto\n\nb\ne\nch\nec\nk\ned\n\nfo\nr\nb\nia\ns\n\nan\nd\nsh\no\nu\nld\n\nb\ne\n\nco\nrr\nec\nte\nd\nto\n\nav\no\nid\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nU\nS\nA\n\nL\nam\n\nb\nre\nch\nt\nan\nd\n\nT\nu\nck\ner\n\n(2\n0\n1\n9\n)\n\nE\nx\nam\n\nin\nes\n\ng\nen\nd\ner\n\nb\nia\ns\nin\n\nd\nel\niv\ner\ny\no\nf\n\njo\nb\nad\ns\n\nC\no\nn\nd\nu\nct\ns\nfi\nel\nd\nte\nst\n\no\nf\nh\no\nw\n\nan\n\nal\ng\no\nri\nth\nm\n\nd\nel\niv\ner\ned\n\nad\ns\n\np\nro\nm\no\nti\nn\ng\njo\nb\n\no\np\np\no\nrt\nu\nn\nit\nie\ns\nin\n\nth\ne\nsc\nie\nn\nce\n,\n\nte\nch\nn\no\nlo\ng\ny\n,\n\nen\ng\nin\nee\nri\nn\ng\nan\nd\n\nm\nat\nh\nfi\nel\nd\ns\n\nB\n,\nD\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nfi\nel\nd\nte\nst\n\nM\nan\nag\nem\n\nen\nt\n\nR\nec\no\nm\nm\nen\nd\ner\n\nsy\nst\nem\n\ns\n\nR\nec\nru\nit\nm\nen\nt\n\nF\new\n\ner\nw\no\nm\nen\n\nsa\nw\n\nS\nT\nE\nM\n\nad\ns\nth\nan\n\nm\nen\n\nA\nn\nal\ng\no\nri\nth\nm\n\nth\nat\n\nsi\nm\np\nly\n\no\np\nti\nm\niz\nes\n\nco\nst\n-\n\nef\nfe\nct\niv\nen\nes\ns\nin\n\nad\n\nd\nel\niv\ner\ny\nw\nil\nl\nd\nel\niv\ner\n\nad\ns\nth\nat\n\nw\ner\ne\nin\nte\nn\nd\ned\n\nto\nb\ne\ng\nen\nd\ner\n\nn\neu\ntr\nal\n\nin\n\nan\nap\np\nar\nen\ntl\ny\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\nw\nay\n\n1\n9\n1 co\nu\nn\ntr\nie\ns\n\n812 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\naj\nja\nd\nia\nn\ni\net\n\nal\n.\n\n(2\n0\n1\n9\n)\n\nP\nre\nd\nic\nti\no\nn\no\nf\nfu\ntu\nre\n\nw\no\nrk\n\no\nu\ntc\no\nm\nes\n\nsu\nch\n\nas\nv\no\nlu\nn\nta\nry\n\ntu\nrn\no\nv\ner\n,\n\nin\nv\no\nlu\nn\nta\nry\n\ntu\nrn\no\nv\ner\n,\nan\nd\n\nv\nal\nu\ne-\nad\nd\ned\n\nb\nas\ned\n\no\nn\nw\no\nrk\n\nh\nis\nto\nry\n\ncr\nit\ner\nia\n\n(w\no\nrk\n\nex\np\ner\nie\nn\nce\n\nre\nle\nv\nan\nce\n,\nte\nn\nu\nre\n\nh\nis\nto\nry\n,\n\nat\ntr\nib\nu\nti\no\nn\ns\nfo\nr\n\np\nre\nv\nio\nu\ns\n\ntu\nrn\no\nv\ner\n)\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne,\n\nst\nat\nis\nti\nca\nl\n\nte\nst\ns;\n\nm\nac\nh\nin\ne\n\nle\nar\nn\nin\ng\n\nP\nsy\nch\no\nlo\ng\ny\n\nG\nen\ner\nal\n\nS\nel\nec\nti\no\nn\n\nA\nlg\no\nri\nth\nm\nic\nm\net\nh\no\nd\ns\nar\ne\n\no\nft\nen\n\nca\nli\nb\nra\nte\nd\n\nex\ncl\nu\nsi\nv\nel\ny\nto\n\na\n\nsp\nec\nifi\nc\nap\np\nli\nca\nti\no\nn\n\np\no\no\nl\n\nP\nai\nri\nn\ng\nes\nta\nb\nli\nsh\ned\n\nth\neo\nry\n\nle\nad\ns\nto\n\nb\net\nte\nr\n\nre\nsu\nlt\ns\nth\nan\n\nu\nn\niq\nu\ne\n\nw\no\nrd\n\nap\np\nli\nca\nti\no\nn\ns\n\nU\nS\nA\n\nY\nar\ng\ner\n\net\nal\n.\n\n(2\n0\n1\n9\n)\n\nC\nri\nti\nca\nl\nan\nal\ny\nsi\ns\no\nf\n\nta\nle\nn\nt\nac\nq\nu\nis\nit\nio\nn\n\nso\nft\nw\nar\ne\nan\nd\nit\ns\n\np\no\nte\nn\nti\nal\n\nfo\nr\n\nfo\nst\ner\nin\ng\neq\nu\nit\ny\n\nin\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nfo\nr\n\nu\nn\nd\ner\nre\np\nre\nse\nn\nte\nd\n\nIT\np\nro\nfe\nss\nio\nn\nal\ns\n\nB\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nIn\nfo\nrm\n\nat\nio\nn\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nH\nu\nm\nan\n\nex\np\ner\nti\nse\n\nis\nst\nil\nl\n\nn\nec\nes\nsa\nry\n\nE\nv\nen\n\nw\nel\nl-\nin\nte\nn\nti\no\nn\ned\n\nal\ng\no\nri\nth\nm\ns\nar\ne\nn\no\nt\n\nn\neu\ntr\nal\n\nan\nd\nsh\no\nu\nld\n\nb\ne\n\nau\nd\nit\ned\n\nfo\nr\nm\no\nra\nll\ny\n\nan\nd\nle\ng\nal\nly\n\nu\nn\nac\nce\np\nta\nb\nle\n\nd\nec\nis\nio\nn\ns\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 813\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\nag\nh\nav\nan\n\net\nal\n.\n\n(2\n0\n2\n0\n)\n\nD\no\ncu\nm\nen\nts\nan\nd\n\nan\nal\ny\nze\ns\nth\ne\n\ncl\nai\nm\ns\nan\nd\n\np\nra\nct\nic\nes\n\no\nf\n\nco\nm\np\nan\nie\ns\n\no\nff\ner\nin\ng\n\nal\ng\no\nri\nth\nm\ns\nfo\nr\n\nem\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nm\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nS\nel\nec\nti\no\nn\n\nT\nar\ng\net\n\nv\nar\nia\nb\nle\ns\nan\nd\n\ntr\nai\nn\nin\ng\nd\nat\na:\n\nm\no\nst\no\nf\n\nth\ne\nv\nen\nd\no\nrs\n\no\nff\ner\n\ncu\nst\no\nm\niz\nab\nle\n\nas\nse\nss\nm\nen\nt\n\nV\nal\nid\nat\nio\nn\n:\nv\nen\nd\no\nr’\ns\n\nw\neb\nsi\nte\ns\no\nft\nen\n\nd\no\nn\no\nt\n\ncl\nar\nif\ny\nw\nh\net\nh\ner\n\nth\ney\n\nv\nal\nid\nat\ne\nth\nei\nr\nm\no\nd\nel\ns\n\nS\nev\ner\nal\n\nco\nu\nn\ntr\nie\ns\n\nS\nán\nch\nez\n-\n\nM\no\nn\ned\ner\no\net\n\nal\n.\n\n(2\n0\n2\n0\n)\n\nE\nx\nam\n\nin\nes\n\nh\no\nw\nth\nre\ne\n\nau\nto\nm\nat\ned\n\nh\nir\nin\ng\n\nsy\nst\nem\n\ns\nca\nn\nb\ne\n\nu\nn\nd\ner\nst\nan\nd\nan\nd\n\nat\nte\nm\np\nt\nto\n\nm\nit\nig\nat\ne\nb\nia\ns\nan\nd\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\nin\n\nth\ne\nU\nK\n\nB\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nca\nse\n\nst\nu\nd\ny\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nE\nm\np\nlo\ny\nm\nen\nt\n\nas\nse\nss\nm\nen\nt\n\nS\nel\nec\nti\no\nn\n\nO\nft\nen\n\nla\nck\n\no\nf\n\nin\nfo\nrm\n\nat\nio\nn\no\nn\nh\no\nw\n\nth\ne\nsy\nst\nem\n\nw\no\nrk\ned\n\nC\nla\nim\n\ns\nan\nd\nv\nal\nid\nat\nio\nn\n\nar\ne\no\nft\nen\n\nv\nag\nu\ne\n\nU\nK\n\n814 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\nto\nn\ne\net\nal\n.\n(2\n0\n1\n5\n)\n\nR\nev\nie\nw\ns\nse\nv\ner\nal\n\no\nf\n\nth\ne\np\nri\nm\nar\ny\n\nfo\nrc\nes\n\nth\nat\n\nar\ne\n\np\nre\nse\nn\nti\nn\ng\n\nch\nal\nle\nn\ng\nes\n\nfo\nr\n\nH\nR\nre\nse\nar\nch\n\nan\nd\n\np\nra\nct\nic\ne\n\nT\nri\nes\n\nto\nan\nsw\n\ner\nth\ne\n\nq\nu\nes\nti\no\nn\nw\nh\net\nh\ner\n\neH\nR\nM\n\nin\nfl\nu\nen\nce\ns\n\no\nrg\nan\niz\nat\nio\nn\nal\n\nef\nfe\nct\niv\nen\nes\ns\nan\nd\n\nw\nh\net\nh\ner\n\nit\nen\nab\nle\ns\n\no\nrg\nan\niz\nat\nio\nn\ns\nto\n\nac\nh\nie\nv\ne\nth\nei\nr\nH\nR\n\ng\no\nal\ns\n\nB\n,\nP\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nS\nev\ner\nal\n\nel\nec\ntr\no\nn\nic\n\nH\nR\nM\n\nto\no\nls\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nT\nh\ner\ne\nar\ne\nst\nil\nl\na\nn\nu\nm\nb\ner\n\no\nf\nq\nu\nes\nti\no\nn\ns\nab\no\nu\nt\n\nw\nh\net\nh\ner\n\nth\nes\ne\nn\new\n\nsy\nst\nem\n\ns\nen\nab\nle\n\no\nrg\nan\niz\nat\nio\nn\ns\nto\n\nac\nh\nie\nv\ne\nth\nei\nr\np\nri\nm\nar\ny\n\nH\nR\ng\no\nal\ns\n\nU\nS\nA\n\nW\no\no\nd\nru\nff\net\n\nal\n.\n\n(2\n0\n1\n8\n)\n\nE\nx\np\nlo\nre\ns\nh\no\nw\n\nm\nem\n\nb\ner\ns\no\nf\n\np\no\nte\nn\nti\nal\nly\n\naf\nfe\nct\ned\n\nco\nm\nm\nu\nn\nit\nie\ns\nin\n\nth\ne\nU\nS\nA\n\nfe\nel\n\nab\no\nu\nt\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\n\nB\n,\nP\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nw\no\nrk\nsh\no\np\n;\n\nin\nte\nrv\nie\nw\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nC\no\nn\nce\np\nt\no\nf\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\nis\n\nla\nrg\nel\ny\n\nu\nn\nfa\nm\nil\nia\nr\n\nL\nea\nrn\nin\ng\nab\no\nu\nt\n\nal\ng\no\nri\nth\nm\nic\n\n(u\nn\n)f\nai\nrn\nes\ns\nel\nic\nit\ned\n\nn\neg\nat\niv\ne\nfe\nel\nin\ng\ns\n\nC\no\nm\np\nan\ny\nh\nan\nd\nli\nn\ng\no\nf\n\nal\ng\no\nri\nth\nm\nic\n\nfa\nir\nn\nes\ns\n\nin\nte\nra\nct\ns\nsi\ng\nn\nifi\nca\nn\ntl\ny\n\nw\nit\nh\nu\nse\nr\ntr\nu\nst\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 815\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nL\nei\nch\nt-\nD\neo\nb\nal\nd\n\net\nal\n.\n(2\n0\n1\n9\n)\n\nId\nen\nti\nfi\nes\n\nch\nal\nle\nn\ng\nes\n\nar\nis\nin\ng\nfr\no\nm\n\nal\ng\no\nri\nth\nm\n-b\nas\ned\n\nH\nR\nd\nec\nis\nio\nn\n-\n\nm\nak\nin\ng\n\nA\nn\nal\ny\nze\ns\nh\no\nw\n\nal\ng\no\nri\nth\nm\n-b\nas\ned\n\nH\nR\n\nD\nec\nis\nio\nn\n-m\n\nak\nin\ng\n\nm\nay\n\nin\nfl\nu\nen\nce\n\nem\np\nlo\ny\nee\ns’\n\np\ner\nso\nn\nal\n\nin\nte\ng\nri\nty\n,\n\nan\nd\nac\nti\no\nn\ns\n\nB\n,\nP\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nD\nev\nel\no\np\nm\nen\nt\n\nA\nlg\no\nri\nth\nm\n-b\nas\ned\n\nd\nec\nis\nio\nn\n-m\n\nak\nin\ng\nca\nn\n\nh\nel\np\nm\no\nn\nit\no\nr\n\nem\np\nlo\ny\nee\ns\nm\no\nre\n\nef\nfe\nct\niv\nel\ny\nb\nu\nt\nca\nn\nb\ne\n\net\nh\nic\nal\nly\n\np\nro\nb\nle\nm\nat\nic\n\nS\nu\ng\ng\nes\nts\n\nfo\nu\nr\n\nm\nec\nh\nan\nis\nm\ns\nto\n\nre\nd\nu\nce\n\nn\neg\nat\niv\ne\nco\nn\nse\nq\nu\nen\nce\ns\n\nN\no\nt sp\nec\nifi\ned\n\nT\nam\n\nb\ne\net\n\nal\n.\n\n(2\n0\n1\n9\n)\n\nId\nen\nti\nfi\nes\n\nch\nal\nle\nn\ng\nes\n\nin\nu\nsi\nn\ng\nd\nat\na\n\nsc\nie\nn\nce\n\nte\nch\nn\niq\nu\nes\n\nfo\nr\n\nH\nR\nta\nsk\ns\n\nP\nro\np\no\nse\ns\np\nra\nct\nic\nal\n\nre\nsp\no\nn\nse\ns\nto\n\nth\nes\ne\n\nch\nal\nle\nn\ng\nes\n\nb\nas\ned\n\no\nn\nth\ne\np\nri\nn\nci\np\nle\ns\n\nca\nu\nsa\nl\nre\nas\no\nn\nin\ng\n,\n\nra\nn\nd\no\nm\niz\nat\nio\nn\nan\nd\n\nex\np\ner\nim\n\nen\nts\n,\nan\nd\n\nem\np\nlo\ny\nee\n\nco\nn\ntr\nib\nu\nti\no\nn\n\nB\n,\nP\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nw\no\nrk\nsh\no\np\n\nan\nd\nsu\nrv\ney\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nF\no\nu\nr\nch\nal\nle\nn\ng\nes\n\nid\nen\nti\nfi\ned\n:\nco\nm\np\nle\nx\nit\ny\n\no\nf\nH\nR\n\np\nh\nen\no\nm\nen\na,\n\nco\nn\nst\nra\nin\nts\nim\n\np\no\nse\nd\n\nb\ny\nsm\n\nal\nl\nd\nat\na\nse\nts\n,\n\nac\nco\nu\nn\nta\nb\nil\nit\ny\n\nq\nu\nes\nti\no\nn\ns\nas\nso\nci\nat\ned\n\nw\nit\nh\nfa\nir\nn\nes\ns\nan\nd\n\no\nth\ner\n\net\nh\nic\nal\n\nan\nd\nle\ng\nal\n\nco\nn\nst\nra\nin\nts\n,\nan\nd\n\np\no\nss\nib\nle\n\nad\nv\ner\nse\n\nem\np\nlo\ny\nee\n\nre\nac\nti\no\nn\ns\n\nU\nS\nA\n\n816 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nR\no\nse\nn\nb\nla\nt\net\n\nal\n.\n\n(2\n0\n1\n4\n)\n\nF\no\ncu\nse\ns\no\nn\nth\ne\nn\new\n\nto\no\nl’\ns\nem\n\np\nlo\ny\ner\ns\n\nu\nse\n\nto\nsi\nft\nth\nro\nu\ng\nh\n\njo\nb\nap\np\nli\nca\nti\no\nn\ns\n\nA\nd\nd\nre\nss\nes\n\nis\nsu\nes\n\no\nf\n\np\nri\nv\nac\ny\n,\nfa\nir\nn\nes\ns,\n\ntr\nan\nsp\nar\nen\ncy\n,\n\nac\ncu\nra\ncy\n,\nan\nd\n\nin\neq\nu\nal\nit\ny\nu\nn\nd\ner\n\nth\ne\nru\nb\nri\nc\no\nf\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nli\nte\nra\ntu\nre\n\nre\nv\nie\nw\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nE\nm\np\nlo\ny\ner\ns\np\no\nte\nn\nti\nal\nly\n\nh\nav\ne\nac\nce\nss\n\nto\nm\no\nre\n\nco\nm\np\nre\nh\nen\nsi\nv\ne\n\nel\nec\ntr\no\nn\nic\n\np\nro\nfi\nle\ns\no\nn\n\njo\nb\nca\nn\nd\nid\nat\nes\n\nth\nan\n\nh\nas\n\nb\nee\nn\ntr\nad\nit\nio\nn\nal\nly\n\nav\nai\nla\nb\nle\n\nto\nth\nem\n\n,\n\nw\nh\nic\nh\nca\nn\nex\np\no\nse\n\njo\nb\n\nca\nn\nd\nid\nat\nes\n\nto\na\ng\nre\nat\ner\n\nsc\nru\nti\nn\ny\no\nf\nth\nei\nr\n\np\ner\nso\nn\nal\n\nli\nv\nes\n\nN\no\nt sp\nec\nifi\ned\n\nB\nu\nrd\no\nn\nan\nd\n\nH\nar\np\nu\nr\n(2\n0\n1\n5\n)\n\nE\nx\nam\n\nin\nes\n\np\no\nte\nn\nti\nal\n\nfo\nr\n\nd\nis\ncr\nim\n\nin\nat\no\nry\n\np\nra\nct\nic\nes\n\nto\n\nd\nev\nel\no\np\nth\nro\nu\ng\nh\n\nin\nfo\nrm\n\nat\nio\nn\n\nin\nfr\nas\ntr\nu\nct\nu\nre\ns\nin\n\nw\nh\nic\nh\nu\nn\nfa\nir\nn\nes\ns\n\nan\nd\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nar\ne\nem\n\nb\ned\nd\ned\n\nin\nto\n\nth\ne\n\np\nre\nsc\nri\np\nti\nv\ne\n\np\nro\nce\nss\nes\n\nan\nd\n\nin\nfr\nas\ntr\nu\nct\nu\nre\ns\no\nf\n\nta\nle\nn\nt\nan\nal\ny\nti\ncs\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nL\naw\n\nG\nen\ner\nal\n\nS\nel\nec\nti\no\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nP\nro\nce\nss\nes\n\no\nf\np\nre\nd\nic\nti\nv\ne\n\nse\ng\nm\nen\nta\nti\no\nn\nca\nn\n\np\nro\nd\nu\nce\n\nin\neq\nu\nal\nit\nie\ns\n\nth\nro\nu\ng\nh\nth\ne\n\nse\ng\nm\nen\nta\nti\no\nn\no\nf\n\nem\np\nlo\ny\nee\n\ng\nro\nu\np\nin\ng\ns\n\nb\nas\ned\n\no\nn\nu\nn\nin\ntu\nit\niv\ne\n\nat\ntr\nib\nu\nte\ns\nth\nat\n\nar\ne\n\nev\ner\n-c\nh\nan\ng\nin\ng\n\nA\nu\nst\nra\nli\na\n\nBusiness Research (2020) 13:795–848 817\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nP\ner\nss\no\nn\n(2\n0\n1\n6\n)\n\nA\nn\nal\ny\nze\ns\nth\ne\n\np\nro\nb\nle\nm\ns\no\nf\n\nim\np\nli\nci\nt\nb\nia\ns\nin\n\nal\ng\no\nri\nth\nm\ns\n\nre\ng\nar\nd\nin\ng\n\nre\ncr\nu\nit\nm\nen\nt\n\np\nro\nce\nss\nes\n,\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n,\n\nan\nd\nu\nn\nfa\nir\nn\nes\ns\nb\ny\n\nu\nsi\nn\ng\nal\ng\no\nri\nth\nm\ns\n\nS\nh\no\nw\ns\np\no\nss\nib\nle\n\nso\nlu\nti\no\nn\ns\n\nB\n,\nD\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nal\nit\nat\niv\ne;\n\nex\np\nlo\nra\nti\nv\ne\n\nan\nal\ny\nsi\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nD\nat\na\nm\nin\nin\ng\n,\n\np\nro\nfi\nli\nn\ng\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nE\nm\np\nlo\ny\ner\ns\nm\nig\nh\nt\nm\nis\ns\n\nth\ne\nb\nes\nt\nca\nn\nd\nid\nat\nes\n,\nas\n\nth\ne\nem\n\np\nlo\ny\ned\n\nal\ng\no\nri\nth\nm\ns\nar\ne\ntu\nn\ned\n\nw\nit\nh\nli\nm\nit\ned\n\nan\nd\n\no\nu\ntd\nat\ned\n\nd\nat\na\n\nT\nh\ne\nri\nsk\n\no\nf\nd\nir\nec\ntl\ny\no\nr\n\nin\nd\nir\nec\ntl\ny\n\nd\nis\ncr\nim\n\nin\nat\nin\ng\n\nca\nn\nd\nid\nat\nes\n\nex\nis\nts\n\nN\no\nt sp\nec\nifi\ned\n\nV\nas\nco\nn\nce\nlo\ns\net\n\nal\n.\n\n(2\n0\n1\n7\n)\n\nP\nro\np\no\nse\ns\na\n\nst\nru\nct\nu\nre\nd\n\nap\np\nro\nac\nh\nto\n\nm\nit\nig\nat\ne\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nan\nd\nu\nn\nfa\nir\nn\nes\ns\n\nca\nu\nse\nd\nb\ny\nb\nia\ns\nin\n\nA\nI\nsy\nst\nem\n\ns\nin\n\nh\nir\nin\ng\nd\nec\nis\nio\nn\n\nsc\nen\nar\nio\ns\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nS\nel\nec\nti\no\nn\n\nP\no\nin\nts\no\nu\nt\nco\nn\nn\nec\nti\no\nn\ns\n\nb\net\nw\nee\nn\nb\nia\ns\no\nf\nA\nI\n\nan\nd\nth\ne\np\nro\nb\nle\nm\n\no\nf\n\nin\nd\nu\nct\nio\nn\n\nS\nh\no\nw\ns\nth\nat\n\nth\ner\ne\nis\n\na\n\nlo\ng\nic\nal\n\nth\neo\nry\n\no\nf\n\np\nre\nfe\nre\nn\nce\ns\n\nN\no\nt sp\nec\nifi\ned\n\n818 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nC\nh\nen\n\net\nal\n.\n(2\n0\n1\n8\n)\n\nIn\nv\nes\nti\ng\nat\nes\n\ng\nen\nd\ner\n-\n\nb\nas\ned\n\nin\neq\nu\nal\nit\nie\ns\n\nin\nth\ne\nco\nn\nte\nx\nt\no\nf\n\nre\nsu\nm\ne\nse\nar\nch\n\nen\ng\nin\nes\n,\n\nd\nis\ncr\nim\n\nin\nat\nio\nn\n\nB\n,\nD\n,\nF\n\nE\nm\np\nir\nic\nal\n-\n\nq\nu\nan\nti\nta\nti\nv\ne;\n\nst\nat\nis\nti\nca\nl\n\nte\nst\ns\n\nC\no\nm\np\nu\nte\nr\n\nsc\nie\nn\nce\n\nS\nea\nrc\nh\nen\ng\nin\nes\n\nR\nec\nru\nit\nm\nen\nt\n\nIn\nd\niv\nid\nu\nal\n\nfa\nir\nn\nes\ns:\nev\nen\n\nw\nh\nen\n\nco\nn\ntr\no\nll\nin\ng\nfo\nr\n\nal\nl\no\nth\ner\n\nv\nis\nib\nle\n\nca\nn\nd\nid\nat\ne\nfe\nat\nu\nre\ns,\n\nth\ner\ne\nis\n\na\nsl\nig\nh\nt\n\np\nen\nal\nty\n\nag\nai\nn\nst\nfe\nm\nal\ne\n\nca\nn\nd\nid\nat\nes\n\nG\nro\nu\np\nfa\nir\nn\nes\ns:\n\n8\n.5\n–\n1\n3\n.2\n%\n\no\nf\njo\nb\nti\ntl\ne/\n\nci\nty\n\np\nai\nrs\n\nsh\no\nw\n\nst\nat\nis\nti\nca\nll\ny\nsi\ng\nn\nifi\nca\nn\nt\n\ng\nro\nu\np\nu\nn\nfa\nir\nn\nes\ns\n\nU\nS\nA\n\nB\no\ng\nen\n\n(2\n0\n1\n9\n)\n\nA\nn\nal\ny\nsi\ns\no\nf\n\np\nre\nd\nic\nti\nv\ne\nto\no\nls\n\nac\nro\nss\n\nth\ne\nh\nir\nin\ng\n\np\nro\nce\nss\n\nto\ncl\nar\nif\ny\n\nw\nh\nat\n\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\nd\no\n,\n\nan\nd\nw\nh\ner\ne\nan\nd\n\nh\no\nw\n\nb\nia\ns\nca\nn\n\nen\nte\nr\nin\nto\n\nth\ne\n\np\nro\nce\nss\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n,\nh\nir\nin\ng\n\nal\ng\no\nri\nth\nm\ns\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n\nM\no\nst\nh\nir\nin\ng\nal\ng\no\nri\nth\nm\ns\n\nw\nil\nl\nd\nri\nft\nto\nw\nar\nd\nb\nia\ns\n\nb\ny\nd\nef\nau\nlt\n\nP\no\nte\nn\nti\nal\n\nto\nh\nel\np\nre\nd\nu\nce\n\nin\nte\nrp\ner\nso\nn\nal\n\nb\nia\ns\n\nsh\no\nu\nld\n\nn\no\nt\nb\ne\n\nd\nis\nco\nu\nn\nte\nd\n\nO\nn\nly\n\nto\no\nls\nth\nat\n\np\nro\nac\nti\nv\nel\ny\nta\nck\nle\n\nd\nee\np\ner\n\nd\nis\np\nar\nit\nie\ns\nw\nil\nl\n\no\nff\ner\n\nan\ny\nh\no\np\ne\nth\nat\n\np\nre\nd\nic\nti\nv\ne\nte\nch\nn\no\nlo\ng\ny\n\nca\nn\nh\nel\np\np\nro\nm\no\nte\n\neq\nu\nit\ny\n,\nra\nth\ner\n\nth\nan\n\ner\no\nd\ne\nit\n\nU\nS\nA\n\nBusiness Research (2020) 13:795–848 819\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nS\nim\n\nb\nec\nk\n(2\n0\n1\n9\n)\n\nD\nis\ncu\nss\nes\n\nth\ne\n\net\nh\nic\nal\n\nim\np\nli\nca\nti\no\nn\ns\no\nf\n\nth\ne\nap\np\nli\nca\nti\no\nn\no\nf\n\nso\np\nh\nis\nti\nca\nte\nd\n\nan\nal\ny\nti\nca\nl\n\nm\net\nh\no\nd\ns\nto\n\nq\nu\nes\nti\no\nn\ns\nin\n\nH\nR\n\nm\nan\nag\nem\n\nen\nt\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nco\nn\nce\np\ntu\nal\n\np\nap\ner\n\nM\nan\nag\nem\n\nen\nt\n\nG\nen\ner\nal\n\nR\nec\nru\nit\nm\nen\nt\n\nan\nd\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt\n\nT\nh\ne\nri\nsi\nn\ng\nd\nat\na\no\nn\n\nem\np\nlo\ny\nee\ns\nw\nil\nl\n\nL\nea\nd\nto\n\nan\n\nu\nn\np\nre\nce\nd\nen\nte\nd\n\ntr\nan\nsp\nar\nen\ncy\n\no\nf\n\nem\np\nlo\ny\nee\ns\n\nP\nro\np\no\nse\ns\nto\n\ntr\nan\nsf\ner\n\nk\ney\n\net\nh\nic\nal\n\nco\nn\nce\np\nts\n\nfr\no\nm\n\nm\ned\nic\nal\n\nre\nse\nar\nch\n,\n\nar\nti\nfi\nci\nal\n\nin\nte\nll\nig\nen\nce\n,\n\nle\nar\nn\nin\ng\nan\nal\ny\nti\ncs\n,\nan\nd\n\nco\nac\nh\nin\ng\nto\n\nH\nR\n\nan\nal\ny\nti\ncs\n\nN\no\nt sp\nec\nifi\ned\n\n820 Business Research (2020) 13:795–848\n\n123\n\n\n\nT\na\n\nb\nle\n\n3\nco\nn\nti\nn\nu\ned\n\nA\nu\nth\no\nr(\ns)\n,\ny\nea\nr\n\nM\nai\nn\nfo\ncu\ns\n\nB\n/D\n/F\n/P\nF\n\nM\net\nh\no\nd\n\nF\nie\nld\n\no\nf\n\nre\nse\nar\nch\n\nS\ny\nst\nem\n\nR\nec\nru\nit\nm\nen\nt,\n\nse\nle\nct\nio\nn\n,\n\nd\nev\nel\no\np\nm\nen\nt,\n\ntr\nai\nn\nin\ng\n\nK\ney\n\nfi\nn\nd\nin\ng\ns\n\nG\neo\ng\nra\np\nh\ny\n\nK\nel\nlo\ng\ng\net\n\nal\n.\n\n(2\n0\n2\n0\n)\n\nA\nn\nal\ny\nze\ns\nh\no\nw\n\nth\ne\n\nim\np\nle\nm\nen\nta\nti\no\nn\n\no\nf\nal\ng\no\nri\nth\nm\nic\n\nte\nch\nn\no\nlo\ng\nie\ns\nin\n\no\nrg\nan\niz\nat\nio\nn\ns\n\nm\nay\n\nre\nsh\nap\ne\n\no\nrg\nan\niz\nat\nio\nn\nal\n\nco\nn\ntr\no\nl\n\nB\n,\nD\n,\nF\n\nN\no\nn\n-e\nm\np\nir\nic\nal\n;\n\nli\nte\nra\ntu\nre\n\nre\nv\nie\nw\n\nM\nan\nag\nem\n\nen\nt\n\nA\nlg\no\nri\nth\nm\nic\n\nre\nco\nm\nm\nen\nd\nin\ng\n,\n\nre\nst\nri\nct\nin\ng\n,\n\nre\nco\nrd\nin\ng\n,\n\nra\nti\nn\ng\n,\n\nre\np\nla\nci\nn\ng\n,\nan\nd\n\nre\nw\nar\nd\nin\ng\n\nD\nev\nel\no\np\nm\nen\nt\n\nA\nlg\no\nri\nth\nm\nic\n\nco\nn\ntr\no\nl\nin\n\nth\ne\nw\no\nrk\np\nla\nce\n\no\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvSyVDMyVCNmNobGluZy1XZWhuZXIyMDIwX0FydGljbGVfRGlzY3JpbWluYXRlZEJ5QW5BbGdvcml0aG1BU3lzLnBkZg2", "metadata_author": "Alina Köchling ", "metadata_title": "Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development", "metadata_creation_date": "2020-11-19T15:45:16Z", "keyphrases": [ "systematic review", "algorithmic decision-making", "HR recruitment", "HR development", "discrimination", "fairness", "context" ] }, { "@search.score": 1, "content": "\nPrivacy preservation techniques in big \ndata analytics: a survey\nP. Ram Mohan Rao1,4*, S. Murali Krishna2 and A. P. Siva Kumar3\n\nIntroduction\nThere is an exponential growth in volume and variety of data as due to diverse applica-\ntions of computers in all domain areas. The growth has been achieved due to afford-\nable availability of computer technology, storage, and network connectivity. The large \nscale data, which also include person specific private and sensitive data like gender, zip \ncode, disease, caste, shopping cart, religion etc. is being stored in public domain. The \ndata holder can release this data to a third party data analyst to gain deeper insights and \nidentify hidden patterns which are useful in making important decisions that may help \nin improving businesses, provide value added services to customers [1], prediction, fore-\ncasting and recommendation [2]. One of the prominent applications of data analytics is \nrecommendation systems which is widely used by ecommerce sites like Amazon, Flip \nkart for suggesting products to customers based on their buying habits. Face book does \nsuggest friends, places to visit and even movie recommendation based on our interest. \nHowever releasing user activity data may lead inference attacks like identifying gender \nbased on user activity [3]. We have studied a number of privacy preserving techniques \nwhich are being employed to protect against privacy threats. Each of these techniques \nhas their own merits and demerits. This paper explores the merits and demerits of each \n\nAbstract \n\nIncredible amounts of data is being generated by various organizations like hospitals, \nbanks, e-commerce, retail and supply chain, etc. by virtue of digital technology. Not \nonly humans but machines also contribute to data in the form of closed circuit televi-\nsion streaming, web site logs, etc. Tons of data is generated every minute by social \nmedia and smart phones. The voluminous data generated from the various sources \ncan be processed and analyzed to support decision making. However data analytics \nis prone to privacy violations. One of the applications of data analytics is recommen-\ndation systems which is widely used by ecommerce sites like Amazon, Flip kart for \nsuggesting products to customers based on their buying habits leading to inference \nattacks. Although data analytics is useful in decision making, it will lead to serious \nprivacy concerns. Hence privacy preserving data analytics became very important. This \npaper examines various privacy threats, privacy preservation techniques and models \nwith their limitations, also proposes a data lake based modernistic privacy preservation \ntechnique to handle privacy preservation in unstructured data.\n\nKeywords: Data, Data analytics, Privacy threats, Privacy preservation\n\nOpen Access\n\n© The Author(s) 2018. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nSURVEY PAPER\n\nRam Mohan Rao et al. J Big Data (2018) 5:33 \nhttps://doi.org/10.1186/s40537-018-0141-8\n\n*Correspondence: \nrammohan04@gmail.com \n1 Department of Computer \nScience and Engineering, \nMLR Institute of Technology, \nHyderabad, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-018-0141-8&domain=pdf\n\n\nPage 2 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nof these techniques and also describes the research challenges in the area of privacy \npreservation. Always there exists a trade off between data utility and privacy. This paper \nalso proposes a data lake based modernistic privacy preservation technique to handle \nprivacy preservation in unstructured data with maximum data utility.\n\nPrivacy threats in data analytics\nPrivacy is the ability of an individual to determine what data can be shared, and employ \naccess control. If the data is in public domain then it is a threat to individual privacy \nas the data is held by data holder. Data holder can be social networking application, \nwebsites, mobile apps, ecommerce site, banks, hospitals etc. It is the responsibility of \nthe data holder to ensure privacy of the users data. Apart from the data held in public \ndomain, knowing or unknowingly users themself contribute to data leakage. For exam-\nple most of the mobile apps, seek access to our contacts, files, camera etc. and without \nreading the privacy statement we agree for all terms and conditions, there by contribut-\ning to data leakage.\n\nHence there is a need to educate the smart phone users regarding privacy and privacy \nthreats. Some of the key privacy threats include (1) Surveillance; (2) Disclosure; (3) Dis-\ncrimination; (4) Personal embracement and abuse.\n\nSurveillance\n\nMany organizations including retail, e-commerce, etc. study their customers buying \nhabits and try to come up with various offers and value added services [4]. Based on the \nopinion data and sentiment analysis, social media sites does provide recommendations \nof the new friends, places to visit, people to follow etc. This is possible only when they \ncontinuously monitor their customer’s transactions. This is a serious privacy threat as no \nindividual accepts surveillance.\n\nDisclosure\n\nConsider a hospital holding patient’s data which include (Zip, gender, age, disease) [5–7]. \nThe data holder has released data to a third party for analysis by anonymizing sensitive \nperson specific data so that the person cannot be identified. The third party data analyst \ncan map this information with the freely available external data sources like census data \nand can identify person suffering with some disorder. This is how private information of \na person can be disclosed which is considered to be a serious privacy breach.\n\nDiscrimination\n\nDiscrimination is the bias or inequality which can happen when some private informa-\ntion of a person is disclosed. For instance, statistical analysis of electoral results proved \nthat people of one community were completely against the party, which formed the gov-\nernment. Now the government can neglect that community or can have bias over them.\n\nPersonal embracement and abuse\n\nWhenever some private information of a person is disclosed, it can even lead to per-\nsonal embracement or abuse. For example, a person was privately undergoing medica-\ntion for some specific problem and was buying some medicines on a regular basis from a \n\n\n\nPage 3 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmedical shop. As part of their regular business model, the medical shop may send some \nreminder and offers related to these medicines over phone. If any family member has \nnoticed this, it will lead to personal embracement and even abuse [8].\n\nData analytics activity will affect data Privacy. Many countries are enforcing Privacy \npreservation laws. Lack of awareness is also a major reason for privacy attacks. For \nexample many smart phones users are not aware of the information that is stolen from \ntheir phones by many apps. Previous research shows only 17% of smart phone users are \naware of privacy threats [9].\n\nPrivacy preservation methods\nMany Privacy preserving techniques were developed, but most of them are based on \nanonymization of data. The list of privacy preservation techniques is given below.\n\n • K anonymity\n • L diversity\n • T closeness\n • Randomization\n • Data distribution\n • Cryptographic techniques\n • Multidimensional Sensitivity Based Anonymization (MDSBA).\n\nK anonymity [10]\n\nAnonymization is the process of modifying data before it is given for data analytics [11], \nso that de identification is not possible and will lead to K indistinguishable records if \nan attempt is made to de identify by mapping the anonymized data with external data \nsources. K anonymity is prone to two attacks namely homogeneity attack and back \nground knowledge attack. Some of the algorithms applied include, Incognito [12], Mon-\ndrian [13] to ensure Anonymization. K anonymity is applied on the patient data shown \nin Table 1. The table shows data before anonymization.\n\nK anonymity algorithm is applied with k value as 3 to ensure 3 indistinguishable \nrecords when an attempt is made to identify a particular person’s data. K anonymity is \napplied on the two attributes viz. Zip and age shown in Table 1. The result of applying \nanonymization on Zip and age attributes is shown in Table 2.\n\nTable 1 Patient data, before anonymization\n\nSno Zip Age Disease\n\n1 57677 29 Cardiac problem\n\n2 57602 22 Cardiac problem\n\n3 57678 27 Cardiac problem\n\n4 57905 43 Skin allergy\n\n5 57909 52 Cardiac problem\n\n6 57906 47 Cancer\n\n7 57605 30 Cardiac problem\n\n8 57673 36 Cancer\n\n9 57607 32 Cancer\n\n\n\nPage 4 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nThe above technique has used generalization [14] to achieve Anonymization. Suppose \nif we know that John is 27 year old and lives in 57677 zip codes then we can conclude \nJohn to have Cardiac problem even after anonymization as shown in Table  2. This is \ncalled Homogeneity attack. For example if John is 36 year old and it is known that John \ndoes not have cancer, then definitely John must have Cardiac problem. This is called as \nbackground knowledge attack. Achieving K anonymity [15, 16] can be done either by \nusing generalization or suppression. K anonymity can optimized if the minimal gener-\nalization can be done without huge data loss [17]. Identity disclosure is the major pri-\nvacy threat which cannot be guaranteed by K anonymity [18]. Personalized privacy is the \nmost important aspect of individual privacy [19].\n\nL diversity\n\nTo address homogeneity attack, another technique called L diversity has been proposed. \nAs per L diversity there must be L well represented values for the sensitive attribute (dis-\nease) in each equivalence class.\n\nImplementing L diversity is not possible every time because of the variety of data. L \ndiversity is also prone to skewness attack. When overall distribution of data is skewed \ninto few equivalence classes attribute disclosure cannot be ensured. For example if the \nentire records are distributed into only three equivalence classes then semantic close-\nness of these values may lead to attribute disclosure. Also L diversity may lead to simi-\nlarity attack. From Table 3 it can be noticed that if we know that John is 27 year old and \nlives in 57677 zip, then definitely John is under low income group because salaries of all \n\nTable 2 After applying anonymization on Zip and age\n\nSno Zip Age Disease\n\n1 576** 2* Cardiac problem\n\n2 576** 2* Cardiac problem\n\n3 576** 2* Cardiac problem\n\n4 5790* > 40 Skin allergy\n\n5 5790* > 40 Cardiac problem\n\n6 5790* > 40 Cancer\n\n7 576** 3* Cardiac problem\n\n8 576** 3* Cancer\n\n9 576** 3* Cancer\n\nTable 3 L diversity privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 6k Cardiac problem\n\n3 576** 2* 7k Cardiac problem\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 22k Cardiac problem\n\n6 5790* > 40 24k Cancer\n\n\n\nPage 5 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nthree persons in 576** zip is low compare to others in the table. This is called as similar-\nity attack.\n\nT closeness\n\nAnother improvement to L diversity is T closeness measure where an equivalence class \nis considered to have ‘T closeness’ if the distance between the distributions of sensi-\ntive attribute in the class is no more than a threshold and all equivalence classes have T \ncloseness [20]. T closeness can be calculated on every attribute with respect to sensitive \nattribute.\n\nFrom Table 4 it can be observed that if we know John is 27 year old, still it will be dif-\nficult to estimate whether John has Cardiac problem or not and he is under low income \ngroup or not. T closeness may ensure attribute disclosure but implementing T closeness \nmay not give proper distribution of data every time.\n\nRandomization technique\n\nRandomization is the process of adding noise to the data which is generally done by \nprobability distribution [21]. Randomization is applied in surveys, sentiment analy-\nsis etc. Randomization does not need knowledge of other records in the data. It can be \napplied during data collection and pre processing time. There is no anonymization over-\nhead in randomization. However, applying randomization on large datasets is not possi-\nble because of time complexity and data utility which has been proved in our experiment \ndescribed below.\n\nWe have loaded 10k records from an employee database into Hadoop Distributed File \nSystem and processed them by executing a Map Reduce Job. We have experimented to \nclassify the employees based on their salary and age groups. In order apply randomiza-\ntion we added noise in the form of 5k records which are randomly added to make a data-\nbase of 15k records and following observations were made after running Map Reduce \njob.\n\n • More number of Mappers and Reducers were used as data volume increased.\n • Results before and after randomization were significantly different.\n • Some of the records which are outliers remain unaffected with randomization and \n\nare vulnerable to adversary attack.\n • Privacy preservation at the cost of data utility is not appreciated and hence randomi-\n\nzation may not be suitable for privacy preservation especially attribute disclosure.\n\nTable 4 T closeness privacy preservation technique\n\nSno Zip Age Salary Disease\n\n1 576** 2* 5k Cardiac problem\n\n2 576** 2* 16k Cancer\n\n3 576** 2* 9k Skin allergy\n\n4 5790* > 40 20k Skin allergy\n\n5 5790* > 40 42k Cardiac problem\n\n6 5790* > 40 8k Flu\n\n\n\nPage 6 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nData distribution technique\n\nIn this technique, the data is distributed across many sites. Distribution of the data can \nbe done in two ways:\n\ni. Horizontal distribution of data\nii. Vertical distribution of data\n\nHorizontal distribution When data is distributed across many sites with same attrib-\nutes then the distribution is said to be horizontal distribution which is described in \nFig. 1.\n\nHorizontal distribution of data can be applied only when some aggregate functions or \noperations are to be applied on the data without actually sharing the data. For example, \nif a retail store wants to analyse their sales across various branches, they may employ \nsome analytics which does computations on aggregate data. However, as part of data \nanalysis the data holder may need to share the data with third party analyst which may \nlead to privacy breach. Classification and Clustering algorithms can be applied on dis-\ntributed data but it does not ensure privacy. If the data is distributed across different \nsites which belong to different organizations, then results of aggregate functions may \nhelp one party in detecting the data held with other parties. In such situations we expect \nall participating sites to be honest with each other [21].\n\nVertical distribution of data When Person specific information is distributed across \ndifferent sites under custodian of different organizations, then the distribution is called \nvertical distribution as shown in Fig. 2. For example, in crime investigations, the police \nofficials would like to know details of a particular criminal which include health, profes-\nsion, financial, personal etc. All this information may not be available at one site. Such a \ndistribution is called vertical distribution where each site holds few set of attributes of a \nperson. When some analytics has to be done data has to be pooled in from all these sites \nand there is a vulnerability of privacy breach.\n\nIn order to perform data analytics on vertically distributed data, where the attributes \nare distributed across different sites under custodian of different parties, it is highly \n\nFig. 1 Distribution of sales data across different sites\n\n\n\nPage 7 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\ndifficult to ensure privacy if the datasets are shared. For example, as part of a police \ninvestigation, the investigating officer wants to access some information about the \naccused from his employer, health department, bank to gain more insights about the \ncharacter of the person. In this process some of the personal and sensitive information \nof the accused may be disclosed to investigating officer leading to personal embarrass-\nment or abuse. Anonymization cannot be applied when entire records are not needed \nfor analytics. Distribution of data will not ensure privacy preservation but it closely \noverlaps with cryptographic techniques.\n\nCryptographic techniques\n\nThe data holder may encrypt the data before releasing the same for analytics. But \nencrypting large scale data using conventional encryption techniques is highly difficult \nand must be applied only during data collection time. Differential privacy techniques \nhave already been applied where some aggregate computations on the data are done \nwithout actually sharing the inputs. For example, if x and y are two data items then a \nfunction F(x, y) will be computed to gain some aggregate information from both x and \ny without actually sharing x and y. This can be applied on when x and y are held with \ndifferent parties as in the case of vertical distribution. However, if the data is at single \nlocation under the custodian of a single organization, then differential privacy can-\nnot be employed. Another similar technique called secure multiparty computation has \nbeen used but proved to be inadequate in privacy preservation. Data utility will be less \nif encryption is applied during data analytics. Thus encryption is not only difficult to \nimplement but it reduces the data utility [22].\n\nMultidimensional Sensitivity Based Anonymization (MDSBA)\n\nBottom up Generalization [23] and Top down Generalization [24] are the conventional \nmethods of Anonymization which were applied on well represented structured data \nrecords. However, applying the same on large scale data sets is very difficult leading to \n\nFig. 2 Vertical distribution of person specific data\n\n\n\nPage 8 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nissues of scalability and information loss. Multidimensional Sensitivity Based Anonymi-\nzation is a improved version of Anonymization proved to be more effective than conven-\ntional Anonymization techniques.\n\nMultidimensional Sensitivity Based Anonymization is an improved Anonymization \n[25] technique such that it can be applied on large data sets with reduced loss of informa-\ntion and predefined quasi identifiers. As part of this technique Apache MAP REDUCE \n[26] framework has been used to handle large data sets. In conventional Hadoop Distrib-\nuted Files System, the data will be divided into blocks of either 64 MB or 128 MB each \nand distributed across different nodes without considering the data inside the blocks. \nAs part of Multidimensional Sensitivity Based Anonymization [27] technique the data is \nsplit into different bags based on the probability distribution of the quasi identifiers by \nmaking use of filters in Apache Pig scripting language.\n\nMultidimensional Sensitivity Based Anonymization makes use of bottom up generali-\nzation but on a set of attributes with certain class values where class represents a sensi-\ntive attributes. Data distribution was made effectively when compared to conventional \nmethod of blocks. Data Anonymization was done using four quasi identifiers using \nApache Pig.\n\nSince the data is vertically partitioned into different groups, it can protect from back-\nground knowledge attack if the bag contains only few attributes. This method also \nmakes it difficult to map the data with external sources to disclose any person specific \ninformation.\n\nIn this method, the implementation was done using Apache Pig. Apache Pig is a script-\ning language, hence development effort is less. However, code efficiency of Apache Pig is \nrelatively less when compared to Map Reduce job because ultimately every Apache Pig \nscript has to be converted into a Map Reduce job. Multidimensional Sensitivity Based \nAnonymization [28] is more appropriate for large scale data but only when the data is at \nrest. Multidimensional Sensitivity Based Anonymization cannot be applied for stream-\ning data.\n\nAnalysis\nVarious privacy preservation techniques have been studied with respect to features \nincluding, type of data, data utility, attribute preservation and complexity. The compari-\nson of various privacy preservation techniques is shown in Table 5.\n\nTable 5 Comparison of privacy preservation techniques\n\nFeatures Privacy preservation techniques\n\nAnonymization \ntechniques\n\nCryptographic \ntechniques\n\nData \ndistribution\n\nRandomization MDSBA\n\nSuitability for unstructured data No No No No Yes\n\nAttribute preservation No No No Yes Yes\n\nDamage to data utility No No Yes No Yes\n\nVery complex to apply No Yes Yes Yes Yes\n\nAccuracy of results of data \nanalytics\n\nNo Yes No No No\n\n\n\nPage 9 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nResults and discussions\nAs part of systematic literature review, it has been observed that all existing mecha-\nnisms of privacy preservation are with respect to structured data. More than 80% of data \nbeing generated today is unstructured [29]. As such, there is a need to address following \nchallenges.\n\ni. Develop concrete solution to protect privacy in both structured and unstructured \ndata.\n\nii. Scalable and robust techniques to be developed to handle large scale heterogeneous \ndata sets.\n\niii. Data should be allowed to stay in its native form without need for transformation \nand data analytics can be carried out while ensuring privacy preservation.\n\niv. New techniques apart from Anonymization must be developed to ensure protection \nagainst key privacy threats which include identity disclosure, discrimination, surveil-\nlance etc.\n\nv. Maximizing data utility while ensuring data privacy.\n\nConclusion\nNo concrete solution for unstructured data has been developed yet. Conventional \ndata mining algorithms can be applied for classification and clustering problems but \ncannot be used in privacy preservation especially when dealing with person specific \ninformation. Machine learning and soft computing techniques can be used to develop \nnew and more appropriate solution to privacy problems which include identity dis-\nclosure that can lead to personal embarrassment and abuse.\n\nThere is a strong need for law enforcement by governments of all countries to \nensure individual privacy. European Union [30] is making an attempt to enforce pri-\nvacy preservation law. Apart from technological solutions, there is a strong need to \ncreate awareness among the people regarding privacy hazards to safeguard them-\nselves form privacy breaches. One of the serious privacy threats is smart phone. Lot \nof personal information in the form of contacts, messages, chats and files are being \naccessed by many apps running in our smart phone without our knowledge. Most \nof the time people do not even read the privacy statement before installing any app. \nHence there is a strong need to educate people on the various vulnerabilities which \ncan contribute to leakage of private information.\n\nWe propose a novel privacy preservation model based on Data Lake concept to \nhold variety of data from diverse sources. Data lake is a repository to hold data from \ndiverse sources in their raw format [31, 32]. Data ingestion from variety of sources can \nbe done using Apache Flume and an intelligent algorithm based on machine learning \ncan be applied to identify sensitive attributes dynamically [33, 34]. The algorithm will \nbe trained with existing data sets with known sensitive attributes and rigorous train-\ning of the model will help in predicting the sensitive attributes in a given data set [35]. \nAccuracy of the model can be improved by adding more layers of training leading \nto deep learning techniques [36]. Advanced computing techniques like Apache Spark \ncan be used in implementing privacy preserving algorithms which is a distributed \n\n\n\nPage 10 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nmassive parallel computing with in memory processing to ensure very fast processing \n[37]. The proposed model is shown in Fig. 3.\n\nData analytics is done on the data collected from various sources. If an ecommerce \nsite would like to perform data analytics, they need transactional data, website logs and \ncustomers opinion through social media pages. A Data lake is used to collect data from \ndifferent sources. Apache Flume is used to ingest data from social media sites, website \nlogs into Hadoop Distributed File System(HDFS). Using SQOOP relational data can be \nloaded into HDFS.\n\nIn Data lake the data can remain in its native form which is either structured or \nunstructured. When data has to be processed, it can be transformed into HIVE tables. A \nHadoop map reduce job using machine learning can be executed on the data to classify \nthe sensitive attributes [38]. The data can be vertically distributed to separate the sensi-\ntive attributes from rest of the data and apply tokenization to map the vertically distrib-\nuted data. The data without any sensitive attributes can be published for data analytics.\n\nAbbreviations\nCCTV: closed circuit television; MDSBA: Multidimensional Sensitivity Based Anonymization.\n\nAuthors’ contributions\nPRMR: as part of Ph.D. work I have done my literature survey and submitted my work in the form of a paper. SMK: \nsupported me in compiling the paper. APSK: suggested necessary amendments and helped in revising the paper. All \nauthors read and approved the final manuscript.\n\nAuthor details\n1 Department of Computer Science and Engineering, MLR Institute of Technology, Hyderabad, India. 2 Department \nof Computer Science and Engineering, Sri Venkateswara College of Engineering, Tirupati, Andhra Pradesh, India. \n3 Department of Computer Science and Engineering, JNTU Anantapur, Anantapuramu, Andhra Pradesh, India. 4 JNTU \nAnantapur, Anantapur, Andhra Pradesh, India. \n\nAcknowledgements\nI would like to thank my guides, for supporting my work and for suggesting necessary corrections.\n\nData Lake\n\nSqoop to load data from RDBMS\n\nApache \nFlume \nto load \nsocial \nmedia \ndata\n\nLoad data from\ndifferent sources\nand varie�es into\nHive Table for\nprocessing\n\nHadoop\nMap\nReduce\nJob to\nclassify\nsensi�ve\ndata\n\nNovel Privacy \nPreserva�on \nalgorithm \nbased on \nver�cal \ndistribu�on and \ntokeniza�on\n\nFig. 3 A Novel privacy preservation model based on vertical distribution and tokenization\n\n\n\nPage 11 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nIf any one is interested in our work, we are ready to provide more details of the map reduce job which we have \nexecuted and the data processing techniques applied. However the data is used in our work, is freely available in many \nrepositories.\n\nFunding\nNo Funding.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 21 March 2018 Accepted: 4 September 2018\n\nReferences\n 1. Ducange Pietro, Pecori Riccardo, Mezzina Paolo. A glimpse on big data analytics in the framework of marketing \n\nstrategies. Soft Comput. 2018;22(1):325–42.\n 2. Chauhan Arun, Kummamuru Krishna, Toshniwal Durga. Prediction of places of visit using tweets. Knowl Inf Syst. \n\n2017;50(1):145–66.\n 3. Yang D, Bingqing Q, Cudre-Mauroux P. Privacy-preserving social media data publishing for personalized ranking-\n\nbased recommendation. IEEE Trans Knowl Data Eng. 2018. ISSN (Print):1041-4347, ISSN (Electronic):1558-2191.\n 4. Liu Y et al. A practical privacy-preserving data aggregation (3PDA) scheme for smart grid. IEEE Trans Ind Inf. 2018.\n 5. Duncan GT et al. Disclosure limitation methods and information loss for tabular data. In: Confidentiality, disclosure \n\nand data access: theory and practical applications for statistical agencies. 2001. p. 135–166.\n 6. Duncan GT, Diane L. Disclosure-limited data dissemination. J Am Stat Assoc. 1986;81(393):10–8.\n 7. Lambert Diane. Measures of disclosure risk and harm. J Off Stat. 1993;9(2):313.\n 8. Spiller K, et al. Data privacy: users’ thoughts on quantified self personal data. Self-Tracking. Cham: Palgrave Macmil-\n\nlan; 2018. p. 111–24.\n 9. Hettig M, Kiss E, Kassel J-F, Weber S, Harbach M. Visualizing risk by example: demonstrating threats arising from \n\nandroid apps. In: Smith M, editor. Symposium on usable privacy and security (SOUPS), Newcastle, UK, July 24–26, \n2013.\n\n 10. Bayardo RJ, Agrawal A. Data privacy through optimal k-anonymization. In: Proceedings 21st international confer-\nence on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 11. Iyengar S. Transforming data to satisfy privacy constraints. In: Proceedings of the eighth ACM SIGKDD international \nconference on knowledge discovery and data mining. New York: ACM; 2002.\n\n 12. LeFevre K, DeWitt DJ, Ramakrishnan R. Incognito: efficient full-domain k-anonymity. In: Proceedings of the 2005 \nACM SIGMOD international conference on management of data. New York: ACM; 2005.\n\n 13. LeFevre K, DeWitt DJ, Ramakrishnan R. Mondrian multidimensional k-anonymity. In: Proceedings of the 22nd inter-\nnational conference (ICDE’06) on data engineering, 2006. New York: ACM; 2006.\n\n 14. Samarati, Pierangela, and Latanya Sweeney. In: Protecting privacy when disclosing information: k-anonymity and its \nenforcement through generalization and suppression. Technical report, SRI International, 1998.\n\n 15. Sweeney Latanya. Achieving k-anonymity privacy protection using generalization and suppression. In J Uncertain \nFuzziness Knowl Based Syst. 2002;10(05):571–88.\n\n 16. Sweeney Latanya. k-Anonymity: a model for protecting privacy. Int J Uncertain, Fuzziness Knowl Based Syst. \n2002;10(05):557–70.\n\n 17. Williams R. On the complexity of optimal k-anonymity. In: Proc. 23rd ACM SIGMOD-SIGACT-SIGART symp. principles \nof database systems (PODS). New York: ACM; 2004.\n\n 18. Machanavajjhala A et al. L-diversity: privacy beyond k-anonymity. In: Proceedings of the 22nd international confer-\nence on data engineering (ICDE’06), 2006. Piscataway: IEEE; 2006.\n\n 19. Xiao X, Yufei T. Personalized privacy preservation. In: Proceedings of the 2006 ACM SIGMOD international confer-\nence on Management of data. New York: ACM; 2006.\n\n 20. Rubner Y, Tomasi T, Guibas LJ. The earth mover’s distance as a metric for image retrieval. Int J Comput Vision. \n2000;40(2):99–121.\n\n 21. Aggarwal CC, Philip SY. A general survey of privacy-preserving data mining models and algorithms. Privacy-preserv-\ning data mining. Springer: US; 2008. p. 11–52.\n\n 22. Jiang R, Lu R, Choo KK. Achieving high performance and privacy-preserving query over encrypted multidimensional \nbig metering data. Future Gen Comput Syst. 2018;78:392–401.\n\n 23. Wang K, Yu PS, Chakraborty S. Bottom-up generalization: A data mining solution to privacy protection. In: Fourth \nIEEE international conference on data mining, 2004 (ICDM’04). Piscataway: IEEE; 2004.\n\n 24. Fung BCM, Wang K, Yu PS. Top-down specialization for information and privacy preservation. In: Proceedings 21st \ninternational conference on data engineering, 2005 (ICDE 2005). Piscataway: IEEE; 2005.\n\n 25. Zhang X et al. A MapReduce based approach of scalable multidimensional anonymization for big data privacy \npreservation on cloud. In: Third international conference on cloud and green computing (CGC), 2013. Piscataway: \nIEEE; 2013.\n\n\n\nPage 12 of 12Ram Mohan Rao et al. J Big Data (2018) 5:33 \n\n 26. Zhang X, et al. A scalable two-phase top-down specialization approach for data anonymization using mapreduce \non cloud. IEEE Trans Parallel Distrib Syst. 2014;25(2):363–73.\n\n 27. Al-Zobbi M, Shahrestani S, Ruan C. Improving MapReduce privacy by implementing multi-dimensional sensitivity-\nbased anonymization. J Big Data. 2017;4(1):45.\n\n 28. Al-Zobbi M, Shahrestani S, Ruan C. Implementing a framework for big data anonymity and analytics access control. \nIn: Trustcom/BigDataSE/ICESS, 2017 IEEE. Piscataway: IEEE; 2017.\n\n 29. Schneider C. IBM Blogs; 2016. https ://www.ibm.com/blogs /watso n/2016/05/bigge st-data-chall enges -might \n-not-even-know/.\n\n 30. TCS. Emphasizing the need for government regulations on data privacy; 2016. https ://www.tcs.com/conte nt/dam/\ntcs/pdf/techn ologi es/Cyber -Secur ity/Abstr act/Stren gthen ing-Priva cy-Prote ction -with-the-Europ ean-Gener al-Data-\nProte ction -Regul ation .pdf.\n\n 31. He X, et al. Qoe-driven big data architecture for smart city. IEEE Commun Mag. 2018;56(2):88–93.\n 32. Ramakrishnan R et al. Azure data lake store: a hyperscale distributed file service for big data analytics. In: Proceed-\n\nings of the 2017 ACM international conference on management of data. New York: ACM; 2017.\n 33. Beheshti A et al. Coredb: a data lake service. In: Proceedings of the 2017 ACM on conference on information and \n\nknowledge management. New York: ACM; 2017.\n 34. Shang T et al. A DP Canopy K-means algorithm for privacy preservation of Hadoop platform. In: International sym-\n\nposium on cyberspace safety and security. Cham: Springer; 2017.\n 35. Jia Q et al. Preserving model privacy for machine learning in distributed systems. IEEE Trans Parallel Distrib Syst. \n\n2018;29(8).\n 36. Psychoula I et al. A deep learning approach for privacy preservation in assisted living. arXiv preprint arXiv \n\n:1802.09359 . 2018.\n 37. Guller M. Big data analytics with spark: a practitioner’s guide to using spark for large scale data analysis. New York: \n\nApress; 2015.\n 38. Fung BCM, Wang K, Philip SY. Anonymzing classification data for privacy preservation. IEEE Trans Knowl Data Eng. \n\n2007;19(5):711–25.\n\nhttps://www.ibm.com/blogs/watson/2016/05/biggest-data-challenges-might-not-even-know/\nhttps://www.ibm.com/blogs/watson/2016/05/biggest-data-challenges-might-not-even-know/\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttps://www.tcs.com/content/dam/tcs/pdf/technologies/Cyber-Security/Abstract/Strengthening-Privacy-Protection-with-the-European-General-Data-Protection-Regulation.pdf\nhttp://arxiv.org/abs/1802.09359\nhttp://arxiv.org/abs/1802.09359\n\n\tPrivacy preservation techniques in big data analytics: a survey\n\tAbstract \n\tIntroduction\n\tPrivacy threats in data analytics\n\tSurveillance\n\tDisclosure\n\tDiscrimination\n\tPersonal embracement and abuse\n\n\tPrivacy preservation methods\n\tK anonymity [10]\n\tL diversity\n\tT closeness\n\tRandomization technique\n\tData distribution technique\n\tCryptographic techniques\n\tMultidimensional Sensitivity Based Anonymization (MDSBA)\n\n\tAnalysis\n\tResults and discussions\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNTM3LTAxOC0wMTQxLTgucGRm0", "metadata_author": "P. Ram Mohan Rao ", "metadata_title": "Privacy preservation techniques in big data analytics: a survey", "metadata_creation_date": "2018-09-20T05:58:23Z", "keyphrases": [ "Privacy preservation techniques", "big data analytics", "survey" ] }, { "@search.score": 1, "content": "\nRawnaque et al. Brain Inf. (2020) 7:10 \nhttps://doi.org/10.1186/s40708-020-00109-x\n\nREVIEW\n\nTechnological advancements \nand opportunities in Neuromarketing: \na systematic review\nFerdousi Sabera Rawnaque1*, Khandoker Mahmudur Rahman2, Syed Ferhat Anwar3, Ravi Vaidyanathan4, \nTom Chau5, Farhana Sarker6 and Khondaker Abdullah Al Mamun1,7\n\nAbstract \n\nNeuromarketing has become an academic and commercial area of interest, as the advancements in neural record-\ning techniques and interpreting algorithms have made it an effective tool for recognizing the unspoken response \nof consumers to the marketing stimuli. This article presents the very first systematic review of the technological \nadvancements in Neuromarketing field over the last 5 years. For this purpose, authors have selected and reviewed a \ntotal of 57 relevant literatures from valid databases which directly contribute to the Neuromarketing field with basic \nor empirical research findings. This review finds consumer goods as the prevalent marketing stimuli used in both \nproduct and promotion forms in these selected literatures. A trend of analyzing frontal and prefrontal alpha band sig-\nnals is observed among the consumer emotion recognition-based experiments, which corresponds to frontal alpha \nasymmetry theory. The use of electroencephalogram (EEG) is found favorable by many researchers over functional \nmagnetic resonance imaging (fMRI) in video advertisement-based Neuromarketing experiments, apparently due to \nits low cost and high time resolution advantages. Physiological response measuring techniques such as eye tracking, \nskin conductance recording, heart rate monitoring, and facial mapping have also been found in these empirical stud-\nies exclusively or in parallel with brain recordings. Alongside traditional filtering methods, independent component \nanalysis (ICA) was found most commonly in artifact removal from neural signal. In consumer response prediction and \nclassification, Artificial Neural Network (ANN), Support Vector Machine (SVM) and Linear Discriminant Analysis (LDA) \nhave performed with the highest average accuracy among other machine learning algorithms used in these litera-\ntures. The authors hope, this review will assist the future researchers with vital information in the field of Neuromarket-\ning for making novel contributions.\n\nKeywords: Neuromarketing, Neural recording, Machine learning algorithm, Brain computer interface, Marketing\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\n1 Introduction\nNeuromarketing, an application of the non-invasive \nbrain–computer interface (BCI) technology, has emerged \nas an interdisciplinary bridge between neuroscience and \nmarketing that has changed the perception of market-\ning research. Marketing is the channel between prod-\nuct and consumers which determines the ultimate sale. \n\nWithout effective marketing, a good product fails to \ninform, engage and sustain its targeted audiences [1]. \nThe expanding economy with new businesses is continu-\nously evolving with changing consumer preferences. It \nis hard for the businesses to grow and sustain without \nhaving quantitative or qualitative assessment from their \nconsumers. Newly launched products need even more \neffective marketing to successfully enter into a com-\npetitive market. However, traditional marketing renders \nonly by posteriori analysis of consumer response. Con-\nventional market research depends on surveys, focus \n\nOpen Access\n\nBrain Informatics\n\n*Correspondence: frawnaque@umassd.edu\n1 Advanced Intelligent Multidisciplinary Systems Lab, Institute \nof Advanced Research, United International University, Dhaka, Bangladesh\nFull list of author information is available at the end of the article\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40708-020-00109-x&domain=pdf\n\n\nPage 2 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\ngroup discussion, personal interviews, field trials and \nobservations for collecting consumer feedback [2]. These \napproaches have the limitations of time requirement, \nhigh cost and unreliable information, which can often \nproduce inaccurate results. In contrast to the traditional \nmarketing research techniques, Neuromarketing allows \ncapturing consumers’ unspoken cognitive and emotional \nresponse to various marketing stimuli and can forecast \nconsumers’ purchase decisions.\n\nNeuromarketing uses non-invasive brain signal record-\ning techniques to directly measure the response of a \ncustomer’s brain to the marketing stimuli, supersed-\ning the traditional survey methods [3]. Functional mag-\nnetic resonance (fMRI), electroencephalography (EEG), \nmagnetoencephalography (MEG), transcranial mag-\nnetic stimulator (TMS), positron emission tomography \n(PET), functional near-infrared spectroscopy (fNIRS) etc. \nare some examples of neural recording devices used in \nNeuromarketing research. By obtaining neuronal activ-\nity from the brain using these devices, one can explore \nthe cognitive and emotional responses (i.e., like/dislike, \napproach/withdrawal) of a customer. Different stimuli \ntrigger associated response in a human brain and the \nresponse can be tracked by monitoring the change in \nneuronal signals or brainwaves [4]. Further, the signal \nand image processing techniques and machine learning \nalgorithms have enabled the researchers to measure, ana-\nlyze and interpret the possible meanings of brainwaves. \nThis opens a new door to detect, analyze and predict \nthe buying behavior of customers in marketing research. \nNow with the help of brain–computer interface, the men-\ntal states of a customer, i.e., excitement, engagement, \nwithdrawal, stress, etc., while experiencing a market-\ning stimuli can be captured [5]. Besides these brain sig-\nnal recording techniques, Neuromarketing also utilizes \nphysiological signals, i.e., eye tracking, heart rate and \nskin conductance measurements to gather the insight of \naudience’s physiological responses due to encountering \nstimuli. These neurophysiological signals with advanced \nspectral analysis and machine learning algorithms can \nnow provide nearly accurate depiction of consumers’ \npreferences and likes/dislikes [6–8].\n\nEarly years of Neuromarketing generated a contro-\nversy between the academician and the marketers due \nto its high promises and lack of groundwork. From \nthe claim of peeping into the consumer mind to find-\ning the buy buttons of human brain, Neuromarketing \nhas long been under the scrutiny of the academicians \nand researchers [9, 10]. However, academic research in \nthis field has started to pile up and the scope of Neuro-\nmarketing to reveal and predict consumer behavior is \ngradually becoming evident. Neuromarketing Science \nand Business Association (NMSBA) was established \n\nin 2012 to bridge the gap between academicians and \nNeuromarketers, and it is promoting Neuromarket-\ning research across the world with its annual event of \nNeuromarketing World Forum [11, 12]. It may be pro-\nposed that further dialogue may continue under such a \nplatform for further industry–academia collaboration. \nEvidently, more than 150 consumer neuroscience com-\npanies are commercially operating across the globe and \nbig brands (Google, Microsoft, Unilever, etc.) are using \ntheir insights to impact their consumers in a tailored and \nefficient way. Academic research, especially the high ana-\nlytical accuracy from the engineering part of Neuromar-\nketing has garnered this breakthrough and acceptance \nover the world. Hence, reviewing the building blocks of \nNeuromarketing is essential to evaluate its scopes and \ncapacities, and to contribute new perspective in this \nfield. Numerous literature reviews have been published \nfocusing the theoretical aspect of consumer neurosci-\nence, such as marketing, business ethics, management, \npsychology, consumer behavior, etc. [13–15]. However, \nsystematic literature review from the engineering per-\nspective with a focus on neural recording tools and inter-\npretational methodologies used in this field is absent. In \nthis regard, our article sets its premises to answer the fol-\nlowing questions:\n\n– What are the types of marketing stimuli currently \nbeing used in Neuromarketing?\n\n– What are the brain regions activated by these mar-\nketing stimuli?\n\n– What is the best brain signal recording tool currently \nbeing used in Neuromarketing research?\n\n– How are these brain signals preprocessed for further \nanalysis?\n\n– And what are the current methods or techniques \nused to interpret these brain signals?\n\nThese questions will allow us to gain a comprehensive \nknowledge on the up-to-date research scopes and tech-\nniques in consumer neuroscience. After this brief intro-\nduction, our methodology of conducting this systematic \nreview will be presented, followed by the state-of-the-art \nfindings corresponding to the aforementioned questions \nand synthesis of the important results. We concluded this \nreview with relevant inference from synthesized result \nand a recommendation for future researchers.\n\n2 Methodology\nThe systematic literature review is a process in which \na body of literature is collected, screened, selected, \nreviewed and assessed with a pre-specified objective for \nthe purpose of unbiased evidence collection and to reach \nan impartial conclusion [16]. Systematic review has the \n\n\n\nPage 3 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nobligation to explicitly define its research question and to \naddress inclusion–exclusion criteria for setting the scope \nof the investigation. After exhaustive search of existing \nliteratures, articles should be selected based on their rel-\nevance, and the results of the selected studies must be \nsynthesized and assessed critically to achieve clear con-\nclusions [16].\n\nIn this systematic review, we would like to explore \nthe marketing stimuli used in Neuromarketing research \narticles over the last 5 years with their triggered brain \nregions. We would also like to focus on the technologi-\ncal tools used to capture brain signals from these regions, \nand finally deliberate on signal processing and analytical \nmethodologies used in these experiments.\n\nTherefore, the inclusion criteria defined here are  as \nfollows:\n\n– Literatures must be published in the field of Neuro-\nmarketing from 2015 to 2019.\n\n– Studies must use brain–computer interface and/or \nother physiological signal recording device in their \nNeuromarketing experiments.\n\n– Studies must have experimental findings from neu-\nral and/or biometric data used in Neuromarketing \nresearch.\n\nThe exclusion criteria for this review are set as:\n\n– Any other literature review on Neuromarketing are \nexcluded from this review.\n\n– Book chapters are excluded from this review. Since \nNeuromarketing is comparatively a new research \nfield, alongside relevant academic journal articles, \nbook chapters conducting empirical experiments \nusing BCI can only be included.\n\n– Literatures written/published in any language other \nthan English are excluded from this article.\n\nTo serve the purpose of this systematic literature \nreview, a total of 931 articles were found across the \n\ninternet by using the search item “Neuromarketing” \nand “Neuro-marketing” in valid databases. Among the \nscreened publications, Table  1 presents the database \nsource of selected 57 research articles including book \nchapters, which directly contribute to the Neuromarket-\ning field with basic or empirical research findings.\n\nAs for the aggregation of relevant existing literatures, \nthe researchers defined that the search for articles would \nbe performed in six databases—Science Direct, Emer-\nald Insight, Sage, IEEE Xplore, Wiley Online Library, \nand Taylor Francis Online. After the initial article accu-\nmulation, the articles were exhaustively screened by \nthe authors by reviewing their title, abstract, keywords \nand scope to match the objective of this research. Once \nthe studies met our aforementioned inclusion criteria, \nthey were selected for further review and critical analy-\nsis. Table 2 classifies the selected articles in terms of the \naforementioned dimensions.\n\nBy exploring the articles selected to develop this sys-\ntematic review, it was possible to successfully categorize \nthe trends and advancements in Neuromarketing field in \nfollowing dimensions:\n\n i. Marketing stimuli used in Neuromarketing \nresearch\n\n ii. Activation of the brain regions due to marketing \nstimuli\n\n iii. Neural response recording techniques\n iv. Brain signal processing in Neuromarketing\n v. Machine learning applications in Neuromarketing.\n\nSome of these Neuromarketing studies have used \neye tracking, heart rate, galvanic skin response, facial \naction coding, etc., with or without brain signal \nrecording techniques to gauge the consumer’s hidden \nresponse. As they are the response from autonomous \nnervous system (ANS), they have proven themselves \nas successful means of exploring consumer’s focus, \narousal, attention and withdrawal actions. Hence, this \nstudy includes articles those empirically used these \n\nTable 1 Number of articles found and selected\n\nName of the database Results: search “Neuromarketing” Results: search “Neuro-marketing” Articles selected\n\nScience direct 281 55 12\n\nWiley online 111 11 7\n\nEmerald insight 115 8 14\n\nIEEE 34 0 14\n\nSage 12 15 6\n\nTaylor Francis online 106 36 4\n\nTotal found: 806 Total found: 125 Total selected: 57\n\n\n\nPage 4 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\ntools to answer Neuromarketing questions, since this \nstudy mainly focuses on the engineering perspective. \nInterpreting the neural data with only statistical analy-\nsis has been out of scope of this paper.\n\n3 Systematic review on the advancements \nof Neuromarketing\n\nNeuromarketing research utilizes marketing strategies in \nthe form of stimuli, and aims to invoke, capture and ana-\nlyze activities occurring in different brain regions while \n\nTable 2 Studies selected on the dimensions of this review\n\nDimensions Published articles\n\ni. Marketing stimuli used in Neuromarketing Product Chew et al. [17], Yadava et al. [18], Rojas et al. [19], Pozharliev [20], Touchette \nand Lee [21], Marques et al. [22], Shen et al. [23], Çakir et al. [24], Hubert \net al. [25], Hsu and Chen et al. [26], Hoefer et al. [27], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Wolfe et al. [31], Bosshard et al. [32], \nFehse et al. [33].\n\nPrice Çakar et al. [34], Marques et al. [22], Çakir et al. [24], Gong et al. [35], Pilelienė \nand Grigaliūnaitė [36], Hsu and Chen [26], Boccia et al. [37], Venkatraman \net al. [38], Baldo et al. [39].\n\nPromotion Soria Morillo et al. [40], Yang et al. [41], Cherubino et al. [42], Soria Morillo \net al. [43], Vasiljević et al. [44], Yang et al. [45], Pilelienė and Grigaliūnaitė \n[36], Daugherty et al. [46], Royo et al. [47], Etzold et al. [48], Chen et al. \n[49], Casado-Aranda et al. [50], Randolph and Pierquet [51], Nomura and \nMitsukura [52], Ungureanu et al. [53], Goyal and Singh [54], Oon et al. [55], \nSingh et al. [56].\n\nii. Activation of brain region due to marketing stimuli Soria Morillo et al. [40], Chew et al. [17], Cherubino et al. [42], Soria Morillo \net al. [43], Çakar et al. [34], Boksem and Smitds [57], Bhardwaj et al. [58], Ven-\nkatraman et al. [38], Touchette and Lee [21], Yang et al. [45], Marques et al. \n[22], Gong et al. [35], Gordon et al. [59], Krampe et al. [60], Hubert et al. [25], \nÇakir et al. [24], Holst and Henseler [61], Hsu and Cheng [62], Hoefer et al. \n[27], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Jain et al. \n[63], Wolfe et al. [31], Bosshard et al. [32], Fehse et al. [33].\n\niii. Neural response recording techniques EEG Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Cherubino et al. [42], \nSoria Morillo et al. [43], Yadava et al. [18], Doborjeh et al. [64], Çakar et al. \n[34], Kaur et al. [65], Baldo et al. [19], Boksem and Smitds [57], Pozharliev \net al. [20], Venkatraman [38], Touchette and Lee [21], Yang et al. [45], Pilelienė \nand Grigaliūnaitė [36], Shen et al. [23], Daugherty et al. [46], Royo et al. [47], \nGong et al. [35], Gordon et al. [59], Hsu and Chen et al. [26], Hoefer et al. [27], \nRandolph and Pierquet [51], Nomura and Mitsukura [52], Bhardwaj et al. \n[58], Fan and Touyama [66], Rakshit and Lahiri [67], Jain et al. [63],Ogino and \nMitsukura [68], Oon et al. [55], Bosshard et al. [32].\n\nfMRI Venkatraman et al. [38], Marques et al. [22], Hubert et al. [25], Hsu and Cheng \n[62], Chen et al. [49], Casado-Aranda et al. [50], Wang et al. [30], Wolfe et al. \n[31], Fehse et al. [33].\n\nfNIRS Çakir et al. [24], Krampe et al. [60].\n\nEMG Missagila et al. [69]\n\nEye tracking Venkatraman [38], Rojas et al. [19], Pilelienė and Grigaliūnaitė [36], Çakar et al. \n[34], Ceravolo et al. [70], Ungureanu et al. [53]\n\nGalvanic skin \nresponse, \nheart rate\n\nCherubino et al. [42], Çakar et al. [34], Magdin et al. [71], Goyal and Singh [54], \nSingh et al. [56].\n\niv. Brain signal processing in Neuromarketing Cherubino et al. [42], Bhardwaj et al. [53], Venkatraman [38], Pozharliev et al. \n[20], Boksem and Smitds [57], Wriessnegger et al. [29], Fan and Touyama \n[66], Pilelienė and Grigaliūnaitė [36], Yadava et al. [18], Baldo et al. [19], \nClerico et al. [72], Chen et al. [49], Casado-Aranda et al. [50], Hsu and Cheng \n[62], Taqwa et al. [73], Bhardwaj et al. [58],Wang et al. [30], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Oon et al. [55], Fehse et al. [33],\n\nv. Machine learning applications in Neuromarketing Soria Morillo et al. [40], Yang et al. [41], Chew et al. [17], Soria Morillo et al. [43], \nYadava et al. [18], Doborjeh et al. [64], Gordon [59], Gurbuj and Toga [28], \nWriessnegger et al. [29], Wang et al. [30], Taqwa et al. [73], Bhardwaj et al. \n[58], Randolph and Pierquet [51], Fan and Touyama [66], Rakshit and Lahiri \n[67], Goyal and Singh [54], Jain et al. [63], Ogino and Mitsukura [68], Oon \net al. [55], Singh et al. [56].\n\n\n\nPage 5 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nsubjects experience these stimuli. To conduct a system-\natic review on this matter, it is important to recall the \ninterconnection between brain functions with human \nbehavior and actions triggered by the  external stimuli. \nThe knowledge of brain anatomy and the physiologi-\ncal functions of brain areas as well as the physiological \nresponse due to external stimuli along with it, makes \nit possible to model brain activity and predict hidden \nresponse. For this purpose, current neural imaging sys-\ntems and neural recording systems have contributed \nmuch to capture the true essence of consumer prefer-\nences. This section will discuss the marketing stimuli, \ntheir targeted brain regions, neural and physiological \nsignal capturing technologies used over the last 5 years \nin Neuromarketing research. Comparing these signals \nwith their associated anatomical functionality some stud-\nies have already reached high accuracy. A number of the \nselected studies have used machine learning techniques \nto predict like/dislike and possible preference from the \ntest subjects.\n\nFor the purpose of Neuromarketing experiments, the \nfollowing literatures selected right-handed participants, \nwith normal or corrected-to-normal vision, free of cen-\ntral nervous system influencing medications and with no \nhistory of neuropathology.\n\n3.1 Marketing stimuli used in Neuromarketing\nAs Neuromarketing is a focus of marketers and consumer \nbehavior researchers, different strategies from market-\ning have been applied in Neuromarketing and they are \nbeing investigated for quantitative assessment from neu-\nrological data. Nemorin et al. asserts that Neuromarket-\ning differentiates from any other marketing models as \nit bypasses the thinking procedures of consumers and \ndirectly enters their brain [74]. Over the last 5  years, \nNeuromarketing stimuli has been mainly in two forms—\nproducts with/without price, and promotions. Product \ncan be defined as physical object or service that meets \nthe consumer demand. In Neuromarketing, product can \nbe physical such as tasting a beverage to conceptual like \na 3D (three dimensional) image of the product. Price in \nNeuromarketing experiments is mostly seen as a stimuli \nis most of the time intermingled with product or pro-\nmotion. However, it plays an important role that deter-\nmines the decision of test subjects to buy or not to buy \nthe product [75].\n\nConsumer response to a product has been recognized \nby either physically experiencing the product or by visu-\nalizing the image of  it. To understand the user esthetics \nof 3D shapes, Chew et  al. [17], used virtual 3D bracelet \nshapes in motion and recorded the brain response of \ntest subjects with EEG with motion. As 3D visualiza-\ntion of objects for preference recognition is a new area \n\nof research, the authors used mathematical model (Gie-\nlis superformula) to create 3D bracelet-like objects. \nTheir study displayed 3D shapes appear like bracelets as \nthe product to subjects. Using the 3D shapes gave the \nauthors an advantage to produce as many of 60 bracelet \nshapes to conduct the research on. Another new prod-\nuct was the E-commerce products presented to the test \nsubjects by Yadava et al. and Çakar et al. [18, 34]. Yadava \net  al. proposed a predictive modeling framework to \nunderstand consumer choice towards E-commerce prod-\nucts in terms of “likes” and “dislikes” by analyzing EEG \nsignals. In showing E-commerce product, they showed a \ntotal of 42 product images to the test participants. These \nproduct images were mainly of apparels and accessory \nitems such as shirts, sweaters, shoes, school bags, wrist \nwatches, etc. The test participants were asked to disclose \ntheir preference in terms of likes and dislikes after view-\ning the items  [18]. Çakar et  al. used both product and \nprice to explore the experience during product search of \nfirst-time buyers in E-commerce. To motivate the partici-\npants, this research provided each participants around \n73 USD as a gift card to use during the experiment. The \ntest participants were asked to search and select three \nproducts of their interest from an e-commerce website \nand reach the maximum of their gift card limit to acti-\nvate. Test subjects often experienced negative emotion \nwhile being unable to find necessary buttons such as “add \nto cart” or “sorting options” [34]. These Neuromarketing \nexperiments on E-commerce products may help develop-\ners to build better user experience. Retail businesses lose \nlarge amount of money when they invest in the wrong \nproduct. Among retail products, shoes have thousands \nof blueprints for manufacturing. Producing thousands \nof shoes of different designs to satisfy consumers can be \nlaborious and unprofitable since a large number of the \ndesigns turn out to be failures. Baldo et al. directly used \n30 existing image of shoe designs to show the test sub-\njects to and to choose from a mock shop showing on the \nscreen [39]. EEG signals were recorded during the whole \nshoe selection time and then subjects were asked to rate \nthe shoes in a rank of 1 to 5 of Likert scale. This experi-\nment helped realize brain response-based prediction can \nsupersede self-report-based methods, as the simulation \non sales data showed 12.1% profit growth for survey-\nbased prediction, and 36.4% profit growth for the brain \nresponse-based prediction.\n\nSimilar to the shoe experiment, Touchette and Lee [21] \nexperimented on the choice of apparel products among \nyoung adults, based on Davidson’s frontal asymmetry \ntheory. EEG signals were recorded while 34 college stu-\ndents viewed three attractive and three unattractive \napparel products on a high-resolution computer screen \nin a random order. Pozharliev et  al. [20] experimented \n\n\n\nPage 6 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\non the emotion associated with visualizing luxury brand \nproducts vs. regular brand products. The experiment dis-\nplayed 60 luxury items and 60 basic brand items to 40 \nfemale undergraduate students to recognize the brain \nresponse of seeing high emotional value (luxury) prod-\nucts in social vs. alone atmosphere. The study found \nthat, luxury brand products invoked a higher emotional \nvalue in social atmosphere which could be utilized by the \nmarketers. Bosshard et al. and Fehse et al. experimented \non brand images and the comparison between the brain \nresponses associated with preferred and not preferred \nbrands [32, 33]. In the study performed by Bosshard et al., \nconsumer attitude towards established brand names were \nmeasured via electroencephalography. Subjects were \nshown 120 brand names in capital white letter in Tahoma \nfont on black background and without any logo while \ntheir brain responses were recorded. On the other hand, \nFehse et al. compared the brain response of test subjects \nwhile they visualized blocks of popular vs. organic food \nbrand logos. These experiments on brand image may help \nmarketers to recognize the implicit response of consum-\ners on different types of branding.\n\nAs price is mentioned as an important factor that \ndetermines the user’s interest on purchasing a product, \na number of Neuromarketing studies have used price \nalongside the products. In the aforementioned study \nby Çakar et  al. [34] price was displayed while recording \nbrain response during first-time e-commerce user expe-\nrience. Marques et al. [22], Çakir et al. [24], Gong et al. \n[35], Pilelienė and Grigaliūnaitė [36], Hsu and Chen [26], \nBoccia et al. [37], Venkatraman et al. [38], and Baldo et al. \n[39] have included price as a marketing stimuli with the \nproduct or promotional.\n\nAn interesting concept was tried by Boccia et  al. to \nrecognize the relation between corporate social respon-\nsibilities and consumer behavior. The author attempted \nto identify if consumers were willing to pay more for the \nproducts from socially or environmentally responsible \ncompany. Consumers were found to prefer the conven-\ntional companies over the socially responsible companies \ndue to lesser price. Marques et  al. [22] investigated the \ninfluence of price to compare national brand vs. own-\nlabeled branded products. In the experiment of Çakir \net  al, product then product and price were shown to \nthe subjects before decision-making time and the brain \nresponses were recorded through fNIRS [24]. Sometimes \nprice can play a passive role in the form of discounts or \ngifts in a promotional. Gong et al. innovatively designed \nan experiment to compare consumer brain response \nassociated with promotional using discount (25% off) vs. \ngift-giving (gift value equivalent to the discount) mar-\nketing strategies. Their study found that lower degree of \nambiguity (e.g., discounts) better motivates consumer \n\ndecision-making [35]. Hsu and Chen used price as a con-\ntrol variable in their wine tasting experiment. As price \nplays a pivotal role in purchase decision, two wines were \nselected of approximately equal price $15. Then the EEG \nsignals of test subjects were recorded during the wine \ntasting session [26].\n\nPromotion is the communication from the marketers’ \nend to influence the purchase decision of consumers [75]. \nIn Neuromarketing research, promotion is usually found \nas the TV commercials and short movies for advertise-\nment. One of the key focus of Neuromarketers is to \nevaluate the consumer engagement of advertisements. \nPredicting the engagement of advertisements before \nbroadcasting them on air, ensures higher rate of success-\nful promotions.\n\nIn 2015, Yang et al. used six smartphone commercials \nof different brands to compare among them in terms \nof extract cognitive neurophysiological indices such as \nhappiness, surprise, and attention as well as behavio-\nral indices (memory rate, preference, etc.) [41]. A com-\nmon experimental design procedure is found among the \npromotion-based Neuromarketing experiments, that is \nsubjects are first made comfortable in the experimental \nsetting, consecutive advertisements were placed at a time \ndistance no shorter than 10 s and consecutive advertise-\nments used neutral stimuli such as white screen, green \nscenario, blank in between them to stabilize the test \nparticipants.\n\nThe Neuromarketing experiments of Soria Morillo \net  al. [40, 43] tried to find out the electrical activity of \naudience brain while viewing advertisement relevant to \naudiences’ taste. They display used 14 TV commercials \ndisplayed to their 10 test subjects for their experiment \nand predicted like or dislike response from audience \nwith the help of advanced algorithms. Cherubino et  al. \n[42] investigated cognitive and emotional changes of \ncerebral activity during the observation of TV commer-\ncials among different aged population. Among seven TV \ncommercials displayed during the experiment, one com-\nmercial with strong images was analyzed for the adults’ \nand older adults’ reaction. Other than them, Vasiljević \net  al. [44] used Nestle advertisement to measure con-\nsumer attention though pulse analysis; Daugherty et  al. \n[46] replicated an experiment of Krugman (1971) using \nboth TV advertisements and print media advertise-\nments to recognize how consumers look and think; Royo \net  al. [47] focused on consumer response while viewing \nadvertisements of sustainable product designs. For their \nexperiment, an animated commercial was made contain-\ning verbal narrative of sustainable product and an exist-\ning commercial was used to convey the visual narrative \nof conventional product. Venkatraman  et al. focused \non measuring the success of TV advertisements using \n\n\n\nPage 7 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nneuroimaging and biometric data  [38]. Randolph and \nPierquet [51] showed super bowl commercials to under-\ngraduate students to compare the class rank of the com-\nmercials and the neural response from the test subjects. \nNomura and Mitsukura [52] identified emotional states \nof audiences while watching favorable vs. unfavorable TV \ncommercials. They selected 100 TV commercials among \nwhich 50 commercials were award winning which were \nlabeled as favorable advertisements. Singh et al. [56] used \npromotion in the form of static vs. video advertisements \nto predict the success of omnichannel marketing strate-\ngies. Ungureanu et al. [53] measured user attention and \narousal by eye tracking while surfing through web page \ncontaining static advertisements, while Goyal and Singh \n[54] utilized facial biometric sensors to model an auto-\nmated review systems for video advertisements. Oon \net al. [55] used merchandise product advertisement clips \nto recognize user preference. Singh et al. [56] used video \nadvertisements to measure visual attentions of audiences.\n\nMost of the TVC (television commercials) in these lit-\neratures had a standard time of 30 s. In Neuromarketing, \nthese TVCs were displayed in between other videos such \nas documentary film, gaming video, drama, etc., to cap-\nture the true response of consumers.\n\nSometimes Neuromarketing  is observed dealing with \nadvertisement of different purposes, such as social adver-\ntisements or gender-related advertisements. The appli-\ncation of Neuromarketing in social advertisement is to \npredict the success of these ads to reach its messages to \nthe targeted social groups [45, 49, 69]. Chen et  al. [49] \nexperimented on the neural response of adolescent audi-\nences while they are exposed to e-cigarette commercials. \nAnother social advertisement stimuli of smoking cessa-\ntion frames was used by Yang [45], to understand what \ntypes of frames (positive/negative) achieve better atten-\ntion from smokers and non-smokers. Gender plays a \nsubstantial role in advertisement industry from celebrity \nendorsement to gender-targeted marketing. Missaglia \net  al. [69] conducted a research on fast marketed con-\nsumer goods (FMCG) advertisements with celebrity vs. \nnon-celebrity female spokesperson. Casado-Aranda et al. \n[50] worked on gender-targeted advertisements using \ncongruent vs. incongruent product–voice combination. \nThese studies show us the diversity of marketing stimuli \nfor future Neuromarketing applications.\n\n3.2 Activation of brain regions due to marketing stimuli\nHuman brain is a matter of profound astonishment. \nThe anatomical development of our brain resulted in \nthe complex web of cognitive and emotional process we \nexperience every day. The evolution of vertebrate brain \nwas initially proposed by Paul D. MacLean  in his Tri-\nune Brain model  [76]. In his hypothesis, evolution of \n\nvertebrate brain is formed through three phases. First \nthe reptilian complex, which indicates the association \nof instincts with the anatomical structure basal gan-\nglia. The paleomammalian complex consists of sep-\ntum, amygdalae, hypothalamus, hippocampal complex, \nand cingulate cortex as the limbic system. These orga-\nnelles were associated with motivation and emotional \nresponse of mammalian brain. Finally, neomammalian \ncomplex consists of cerebral neocortex or the outer \nlayer of advanced mammalian brain, which is particu-\nlarly a unique feature of human brain. In the cerebral \nneocortex, we find four lobes which control our sen-\nsory, motor, emotional and cognitive processes [76]. \nThe triune brain model has been rejected by new neu-\nroscientists due to the interconnectivity of human brain \nstructures and their function. However, the anatomical \nstructure of human brain explained by this theory plays \na vital role in recognizing cognitive, emotional and \nbehavioral process.\n\nUnderstanding the anatomy of human brain has \nshowed itself indispensable in Neuromarketing \nresearch, as its functionality is deeply associated with \nthe interpretation of neural response. The outer layer of \nthe human brain is a complex system organized in four \nlobes, namely (frontal, parietal, temporal and occipital \nlobes), each having distinct functionalities for cogni-\ntive, emotional, and motor responses. The frontal lobe \nis the region where most of our thoughts and conscious \ndecisions are made  [77]. Cognitive decision-making \nmainly takes part in the prefrontal region of this lobe, \nand movement-related decisions are made in the end \npart of frontal lobe. Information about taste, touch and \nmovement is processed by the parietal lobe. The occipi-\ntal lobe is the primary center for visual processing, \nand the temporal lobe is responsible for visual memo-\nries, auditory recognition and integrating new sensory \ninformation with memories  [78]. Besides the primary \nlobes, cerebral cortex brain anatomy has gyri and sulci \nwhich create the folded appearance of the brain. The \ngyri functions on increasing surface area for informa-\ntion processing. Alongside the primary lobes, gyri of \nthese lobes can be considered as the region of interest \n(ROI) in neural imaging techniques [79].\n\nDeeper structures of the human brain consist-\ning thalamus, amygdalae, etc., produces sensory and \ninstinctual responses which are later transported to \nthe cerebral cortex. Hypothalamus works as the master \ncontrol of our autonomic system. Sleep, hunger, thirst, \nblood pressure, body temperature, sexual arousal are \ncontrolled and regulated by hypothalamus. Thalamus \non the other hand regulates sensory information, atten-\ntion and memory. Amygdalae originate our emotional \n\n\n\nPage 8 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nresponse and hippocampus is the mainframe of our \nmemory [77].\n\nRetrieving information from brain requires diverse \ntypes of methodology. In Neuromarketing experiments, \ndifferent parts of brain are selected for retrieving dis-\ntinct information. An experiment which solely focuses \non attention might only look at the signals from frontal \nlobe, whereas experiments focusing on buyer’s motiva-\ntion might want to look at deeper structures [38].\n\nAccording to Soria Morillo et al., brain signal acquisi-\ntion may capture neural signals either from cerebral cor-\ntex or from the deeper layer of the brain [40, 43]. Their \nexperiment on TV advertisement liking recognition ini-\ntially uses information only from prefrontal cortex using \na single electrode EEG device. Their experiment showed, \nit is possible to classify like/dislike with information col-\nlected solely from frontal lobe.\n\nSimilarly, Cherubino et  al. emphasized on the signifi-\ncance of frontal cortex (FC) and prefrontal cortex (PFC) \nin Neuromarketing studies. PFC processes the conscious \nand unconscious cognitive and emotional information. \nHence, devices using only a single sensor select PFC as \ntheir signal acquisition region  [42]. Also, ventromedial \nprefrontal cortex corresponds to motivational behaviors, \nimaging of which by fMRI or MEG can reveal purchase \nmotivations [22].\n\nNeural communication in the brain is conducted \nthrough the action potentials, or the firing of neu-\nrons  [80]. A neuronal signal is the electrochemical \ninformation that neurons send to each other. These \ninformation are acquired as signals of non-linear pat-\ntern called the brainwaves  [80]. These brainwaves are \nfurther associated with the neural signature of brain \nstates. The neural signature is divided into frequency \nbands known as rhythms, such as the delta (0.1–4  Hz), \ntheta (4–8  Hz), alpha (8–12  Hz), beta (12–30  Hz), and \ngamma (30–90  Hz). These frequency bands are related \nto different brain states, regions, functions or patholo-\ngies. Delta (δ) waves are characteristic of deep sleep and \nhave not been explored for BCI applications  [81]. Theta \n(θ) waves are enhanced during sleep in adults and often \nrelated to various brain disorders. During wakefulness \nunder relaxed conditions alpha (α) waves with moder-\nate amplitude appear spontaneously. Beta (β) waves have \nless amplitude and are strongly related to motor control \nand engagement or decision-making procedure. Gamma \n(γ) waves are associated with movement-related activ-\nity of the brain and intensely observed in invasive neural \nrecording [81].\n\nIn Neuromarketing, beta wave amplitudes are associ-\nated with reward processing which can further predict \nsuccess of a product or TVC (Boksem and Smitds) [57].\n\nFrontal alpha asymmetry is a key concept of hem-\nisphere-based like–dislike classification approach. \nWhen the brain is considered as two hemispheres, left \nand right frontal cortices show hemispheric asymme-\ntry in their activation during processing positive and \nnegative emotion. Another term for emotional engage-\nment, Approach–Withdrawal Index refers to the emo-\ntional response from Frontal Alpha Asymmetry theory \n[34]. Frontal Asymmetry Index is a marker of approach \nand avoidance. “Emotional Engagement” in Neuromar-\nketing is expressed as the power of specific frequency \nbands from left and right frontal regions. The F3/F4 and \nF7/F8 electrodes are the best candidates for these EEG \npower reception as they are positioned at the most sen-\nsitive places (International 10–20 System). The alpha \nfrequency band (8–12  Hz) is commonly used in the \nfrontal alpha asymmetry theory. However, as the alpha \nactivity corresponds with relaxation and meditation, it \nis negatively correlated with cognitive engagement.\n\nFrontal Asymmetry Index is measured from the \nequation:\n\nHigher the Frontal Asymmetry Index value, the more \napproach response is obtained from the test subjects \nand vice versa. This high or positive asymmetry score \ncan determine pleasant feeling of a test subject and vice \nversa, which was explored in the study conducted by \nTouchette and Lee [21].\n\nNeuroimaging and neural signal recording devices \nuse these locations and brain states to map the mind of \na consumer. A standard 10–20 system has been estab-\nlished, which is an internationally recognized method \nto apply the EEG sensors or electrodes on a human \nscalp. EEG electrodes under 10–20 system have let-\nters to express their location on skull such as prefron-\ntal (Fp), frontal (F), temporal (T), parietal (P), occipital \n(O), and central (C). Even number of electrodes are \nplaced on the right side of the head.\n\nOn the other hand, a test subject is placed inside an \nfMRI machine where the activities of the cortices can \nbe recorded from the hemodynamic response or blood \noxygen level-dependent (BOLD) imaging process. \nfMRI can look deeper within the spatial range from \nmillimeters to centimeters. This enables Neuromar-\nketing researchers using fMRI imaging to examine the \nresponse at putamen, thalamus, amygdalae and even in \nthe hippocampus.\n\nFunctional near-infrared spectroscopy (fNIRS) \nis another new brain imaging tool which uses the \n\nFrontal Asymmetry Index\n\n= ln\nAlpha Power of Right F4 or F8\n\nAlpha Power of Left F3 or F7\n.\n\n\n\nPage 9 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nhemodynamic responses associated with neuronal \nactivities [24, 60]. However, fNIRS has a lower spatial \nresolution than fMRI and cannot look deeper than 4 cm.\n\nAlongside brain regions associated with neural \nresponse, the human has a peripheral system which \ncorresponds to cognitive and emotional processes. Eye \nmovement, skin conductance, heart rate, facial expres-\nsion all are result of neural processes. Eye tracking is \nprimarily considered as the physiological response in \nconsumer neuroscience, however studies have suggested \neye tracking as a result of activation of the visual cortex \nor a secondary neural response [34, 36, 38, 53, 70].\n\nNeuromarketing experiments focused on the affect–\ncircumflex coordinate or valance–arousal coordinate use \nautonomic nervous system (ANS) response from sweat \nglands of hands or galvanic skin response (GSR), and car-\ndiovascular measure or heart rate (HR). GSR is viewed as \na sensitive and convenient measure for indexing changes \nin sympathetic arousal associated with emotion, cogni-\ntion and attention. On the other hand, HR correlates with \nthe emotional valence of a stimulus, e.g., the positive or \nnegative component of the emotion [34].\n\nConsidering the available regions to capture signals \nfrom, it is highly likely that Neuromarketing will expo-\nnentially improve its recognition and predictions in user \nresponse and preferences.\n\n3.3 Neural response recording techniques\nThe groundwork in Neuromarketing field is evidently \nindebted to the advancement of neuroimaging and neu-\nral recording tools. Neurophysiological tools, such as \nelectroencephalography (EEG), functional magnetic \nresonance imaging (fMRI), eye tracking, skin conduct-\nance, heart rate, etc., made it feasible to conduct the aca-\ndemic and commercial Neuromarketing research. Many \nresearch-grade neurophysiological and biometric signal \ncapturing devices are now available in the market. How-\never, some devices have cost and mobility advantages \nover the others and therefore replacing the expensive and \nimmobile devices for Neuromarketing purpose.\n\nAmong all neuroimaging devices, functional mag-\nnetic resonance imaging (fMRI) has been the most \nwidely used neuroimaging technique in Neuromar-\nketing research during the initial time of consumer \nneuroscience. The reason behind the wide acceptance \nof fMRI is that it offers the identification of cerebral \nregions associated with cognitive and emotional pro-\ncess. Combining magnetic field and radio waves, fMRI \nproduces a sequence of images of the cerebral activ-\nity by measuring the blood flow of the cerebral cor-\ntex [38]. The signal imaged in fMRI is called BOLD \n(blood oxygen level dependent) signal. This technol-\nogy also allows 3D views of the coordinates that denote \n\ncertain location, making possible to investigate deeper \nbrain structures [57]. The primary disadvantages of \nthis method are that it is very expensive and till now \nhas a poor temporal resolution. The computer screen \nused in fMRI refreshes the image every 2 to 5  s. This \nlow temporal resolution to the order of seconds due to \nthe time requirement of the cerebral blood flow’s incre-\nment after being exposed to the stimuli, makes fMRI \nunsuitable for tracking brain activities to the order of \nmilliseconds, which is required in video advertisement \nanalysis. Other disadvantage is the head of the subject \nmust remain static during the whole image recording \nprocess [62]. This restriction causes complex preproc-\nessing and movement-related artifact removal from the \nfMRI signals. A number of studies, i.e., Venkatraman \net  al. [38], Marques et  al. [22], Hubert et  al. [25], Hsu \nand Cheng [26], Chen et al. [49], Casado-Aranda et al. \n[50], Wang et al. [30], Wolfe et al. [31], Fehse et al. [33], \netc., have used fMRI as the neuroimaging technique \nin their Neuromarketing studies. fMRI in all studies \nrequired the test subjects to remain static and displayed \nthe subjects the images and commercials of products \nfor 3–5 s. Later the subjects had to make purchase deci-\nsion within 5 s after their exposure to the stimuli [50]. \nResearchers over the last 5  years  are found using 3-T \nfMRI scanner 3.0-T Siemens Magnetom Trio system \nMRI Scanner equipped with a 32-channel bridge head \ncoil (Hubert and Hsu and Cheng) [25, 62] and 3 Tesla \nSiemens Verio scanner (Wang et  al. [30]). Cost of an \nfMRI machine can be from $500,000 to $3 million vary-\ning on its spatiotemporal resolution.\n\nAlongside fMRI, electroencephalography (EEG) is \nanother popular tool used in Neuromarketing research. \nNumber of research in Neuromarketing using EEG \ndevices is increasing due to EEG’s cost efficiency high \ntemporal resolution and mobility advantages. The EEG \nmeasures electrical activity in the cerebral cortex, the \nouter layer of the brain. EEG devices are placed follow-\ning the 10–20 system. According to the 10–20 system, \nthe 10 and 20 refer to the actual percentage of distances \nbetween adjacent electrodes which are either 10% or 20% \nof the total front–back or right–left distance of the skull \n[82]. As EEG is portable and allows capturing signal from \ncerebral cortex with high temporal resolution, it is mainly \nused in TV commercial engagement or success analy-\nsis. EEG signal of interest in Neuromarketing are mainly \nevent-related potential (ERP), and late positive potential \n(LPP). ERP and LPP are used by Pozharliev et al. [20] to \nmeasure the emotional value of luxury products. Çakar \net al. [34] used ERP to explore the experience of first-time \nuser of E-commerce product. Pilelienė and Grigaliūnaitė \n[36]) used ERP along with eye tracking signal to measure \nthe impact of celebrity spokesman in TVC. Shen et  al. \n\n\n\nPage 10 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\n[23] used ERP and LPP to explore the influence of rating \nreviews on online products.\n\nResearch-grade EEG devices are vastly used in Neuro-\nmarketing. Emotiv Epoc and Emotive Epoc+ were found \nas the mostly commonly used EEG devices in the review. \nThese devices were used in the studies of Yang et al. [45], \nChew et  al. [17], Soria Morillo et  al. [40], Yadava et  al. \n[18], Royo et al. [47], Jain et al. [63], and Singh et al. [56]. \nEmotive Epoc+ is a moveable, cost-effective EEG head-\nset having 14 electrodes those cover the frontal, tempo-\nral, parietal and occipital lobes with channels AF3, F7, \nF3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4. The \nacquired brain signals from Emotiv Epoc+ are highly \ndependable and have already been used in these scientific \nresearches. Another popular EEG device in Neuromar-\nketing, NeuroSky Mindwave, has only one sensor placed \non the prefrontal cortex of the head or the forehead. \nUnlike EEG devices with wet electrodes, Neurosky Mind-\nwave employs a biosensor which does not require any \nconductive medium to be applied on the test subject’s \nscalp [40]. With the help of NeroSkyLab, the provided \nscientific research tool, data viewing and analysis can be \nconducted easily by non-engineer population. In 2015, \nSoria Morillo et  al. and Ogino and Mitsukura in 2018 \nconducted Neuromarketing experiment with NeuroSky \ndevice and with the help of machine learning algorithm, \ntheir choice prediction accuracy was over 70% [40, 68]. A \n10-channel EEG device BrainAmp, from BrainProducts \nGmBh was used in the Neuromarketing experiment con-\nducted by Cherubino et  al. [42]. Another device EEGO \nSports from ANT Neuro (32 channels) was used to ana-\nlyze non-linear features of EEG signals by Oon et al. [55]. \nB-alert X10 headset from ABM consisting 9 electrode \nchannels is found in use by the experiment of Chew et al. \n[17]. 8-channel E-Prime from Neuroscan is another EEG \ndevice is used in the sales strategy experiment by Gong \net al. and Touchette et al. conducted their apparel liking \nexperiment with NeXus-10 biofeedback system. EEG \ndevices have different sampling rates starting from 128 \nto 512  Hz. This sampling rate determines the highest \nfrequency recordable by the EEG device. In general EEG \nhas a lower frequency spectrum, having Gamma band up \nto 90 Hz. This gives researchers advantage to choose the \nright EEG device from numerous manufacturers. Price \nof EEG devices depends mainly on the number of elec-\ntrode channels and performance. Cost of EEG device \nstarts from $99 and can go beyond $25,000, which gives \nresearchers buying flexibility.\n\nMagnetoencephalography (MEG) uses magnetic \npotentials to record brain activity at the scalp level, using \nmagnetic field sensitive detectors in the helmet placed \non the subject’s head. Magnetic field is not influenced \nby the type of tissue (blood, brain matter, bones), unlike \n\nelectrical field-based EEG, and can indicate the depth of \nthe location in the brain with high spatial and temporal \nresolution [3]. Similar to MEG, transcranial magnetic \nstimulation (TMS) uses varying magnetic field [83] gen-\nerated by electromagnetic induction using an iron core. \nTMS can stimulate targeted part of the brain, which \nenables it to conduct social or behavioral experiments. \nTMS and MEG are also used frequently in Neuromar-\nketing experiments. However, the selected databases for \nthis review did not contain any Neuromarketing research \narticles using these technologies over the last 5 years.\n\nThe electromyography (EMG) measures electrical \nactivity produced by skeletal muscles when the mus-\ncles contracts and expands in order to move the body \n[70]. Also EMG is generated from the autonomic nerv-\nous activity related to emotional or mental activity. In \nNeuromarketing research, facial EMG is the best meas-\nure of the valence of the emotional reaction as it records \nfacial muscle movement from two different muscles, i.e., \nzygomaticus muscle and corrugator muscle. Zygomatic \nmuscle is found to react more while exposed to positive \nstimuli [70].\n\nBesides these brain signal recordings, eye tracking \nis the most popular method for analyzing consumer \nresponse. Eye tracking offers to measure visualization \ntime and gaze path across a screen in Neuromarket-\ning experiments. Besides tracking eye movement, pupil \ndilation measurement allows one to associate audi-\nence’s focus and arousal to the marketing stimuli. In the \nreviewed literatures, Tobii Pro X2-30 system from Tobii \nTechnology was found as the most popular eye track-\ning device. In 2019, Etzold et  al. used this eye tracking \ndevice to explore attention research on online book-\ning [48]. Tobii Pro can also cooperate with fMRI-based \nNeuromarketing experiment (Venkatraman [38]). Other \nthan Tobii, Eye Tribe is found in use by Çakar et al. [34]. \nUngureanu et al. used eye tracking to measure the atten-\ntion level of consumers while displaying static advertise-\nments of cars and clothing products [53]. Figure 1 depicts \nthe most popular methods of neural response recording \ni.e. EEG, fMRI and eye tracking used in the Neuromar-\nketing experiments.\n\nSome of the Neuromarketing studies used heart rate, as \none of the metrics to measure arousal and focus of the \nconsumer while they encounter TV commercial stimuli. \nHeart rate is the speed of the heartbeat and it is typically \nmeasured by electrocardiogram (EKG). An EKG meas-\nures the electrical activity of the heart using external skin \nelectrodes. Heart rate is controlled by two antagonistic \nnervous systems, i.e., the sympathetic nervous system \n(SNS) and the parasympathetic nervous system (PNS). \nAutomatic response to external stimuli is determined by \nthe sympathetic system of the body. Activation of this \n\n\n\nPage 11 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nsystem increases heart rate, causing fight or flight mode, \nwhich is an independent measure of arousal [38]. In con-\ntrast, the calm and relaxed state characterized by slower \nheart rate is controlled by the parasympathetic system. \nSlower heart rate in response to an advertisement implies \nthe increased focus on the ad, hence provides an inde-\npendent measure of attention [38]. Another physiologi-\ncal parameter, skin conductance (SC), or galvanic skin \nresponse (GSR), develops when the skin acts as an elec-\ntrical conductor due to the increased activity of the sweat \nglands from exposure to stimulus [38]. Skin conductance \n\namplitude and response latency provide direct measures \nof arousal when watching TV commercials, unlike self-\nreported measures that are often based on later memory \nrecall. Although GSR cannot independently relate to \nemotional valence, some of the Neuromarketing studies, \ni.e., Cherubino et  al. [42], Çakar et  al. [34], Ungureanu \net al. [53], Magdin et al. [71], Goyal and Singh [54], and \nSingh et al. [56] have used skin conductance along with \nheart rate to measure the consumer attention and focus \non the TVC.\n\nFig. 1 Neural recording in Neuromarketing experiments: a multichannel EEG [43], b fMRI imaging [50], and c eye tracking for online booking \nappointment [48]\n\n\n\nPage 12 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\n3.4 Brain signal processing in Neuromarketing\nSince neural signals and images are highly vulnerable \nto noise and artifacts, before performing any analysis \nor interpretation it is imperative to preprocess the neu-\nral signals to increase the signal-to-noise ratio (SNR). \nNoises that commonly accompany the EEG signals \nare cardiac signals (ECG), power line interference, eye \nmovement artifact (EOG) and muscle movement arti-\nfacts (EMG). Preprocessing in Neuromarketing consists \nof filtering the signals to the frequency bands of inter-\nest, re-referencing the filtered signal to a common aver-\nage, detecting and interpolating bad channels, noise \nand artifact removal, and framing or segmentation for \nfurther machine learning process.\n\nEEG signals usually spread across its energy from \n0.5  Hz to around 90  Hz. For classification purpose, it \nis required to have energies only from the relevant fre-\nquency bands, hence EEG preprocessing commonly \nuses band pass filtering techniques. Band pass filter \nrequires two cutoff frequencies, one upper and one \nlower to pass the energy between them and blocks \nenergies from all other frequencies. Band pass filter \nused in these  Neuromarketing experiments  are basi-\ncally the digital version of the filter mostly applied by \nMATLAB and EEGLAB (a toolbox designated for EEG \nsignal processing in MATLAB). Re-referencing to a \ncommon average reference is also found common after \nband pass filtering in the studies of Yang et al. [41], Fan \nand Touyama [66] to reduce possible shifts from exter-\nnal artifacts. Power line interference is usually found \nremoved by using a notch filter at 60 Hz or 50 Hz.\n\nThe reviewed literatures had some common \napproaches in noise removal techniques. Since the noise \naccompanied with EEG signals are random in nature, \nsignal averaging is a common approach to reduce these \nnoises. Fan and Touyama [66] averaged the ERP signals \nfor noise removal. Chew et  al. [17] used ABM software \ndevelopment kit (SDK) in MATLAB to remove 5 types \nof artifacts, namely EMG, eye blinking artifact, excur-\nsions, saturations and spike. Excursion, saturation and \nspike artifacts in the EEG signals are replaced by zero val-\nues. Then they applied nearest neighbor interpolation to \nreplace those zero values. Another type of filter Savitzky–\nGolay is found in use by Yadava et  al. [18] for signal \nsmoothing. For noise and artifact removal, the 4th-order \nButterworth filter was used in the studies of Ogino and \nMitsukura [68] and Oon et al. [55].\n\nIndependent component analysis (ICA) is an approach \nto separate the statistical subcomponents of EEG sig-\nnals. ICA is found as the most sought after technique \nfor removing artifacts and noise from EEG signals in \nthese articles. Studies of Cherubino et al. [42], Bhardwaj \net al. [53], Venkatraman et al. [38], Pozharliev et al. [20], \n\nBoksem and Smitds[57], Wriessnegger et al. [29], Fan and \nTouyama [66], Pilelienė and Grigaliūnaitė [36] all used \nindependent component analysis mostly for eye blink \nand eye movement artifact, and muscular movement \nnoise removal.\n\nNeuromarketing with fMRI studies have a different \nmethod for image preprocessing. Since the fMRI provides \na 3D image of the brain region with time information, it \nis basically a 4D signal. A 4D dataset is motion corrected \nfor any head movement, slice time corrected, spatially \nnormalized and finally smoothed to recover a denoised \nfMRI image. Wang et al. [30] used statistical parametric \nmapping (SPM) software to preprocess their fMRI data. \nTheir raw fMRI signal was subjected to standard preproc-\nessing involving correction for head motion, slice timing \ncorrection, temporal and spatial denoising and normali-\nzation into standardized Montreal Neurological Institute \n(MNI) space. The mean fMRI signal from each region of \ninterest was extracted from voxels in a sphere of 6-mm \nradius centered at the activation point in the regional \nactivation map.\n\nfMRI scan was also used by Hubert et al. [25] in their \nexperiment on hedonic vs. prudent shopper based on \nconsumer impulsiveness. Decision-making process with \ncognitive deliberation and the consideration of long-\nterm consequences are associated with processing in \nbrain areas such as the ventromedial prefrontal cortex \n(vmPFC) and the dorsolateral prefrontal cortex (dlPFC). \nHence, these vmPFC and dlPFC were the region of inter-\nests to capture the BOLD activation imaging [62]. Brain \nactivation through BOLD signals was used by Hsu and \nCheng [26] to investigate negative emotion after prod-\nuct harm crisis. fMRI region of interest in this study \nincluded amygdala, left calcarine, striatum, ventral teg-\nmental area (VTA) and right insula. The amygdala is \nassociated with memory and subjective evaluation, left \ncalcarine relates to human visual processing, the striatum \nis associated with goal-oriented evaluation, and reward \nevaluation, VTA relates to decision-making process and \nmotive functions, and the insula regions are involved in \nconsumer decision-making related to negative reinforce-\nment. Acquiring activation within these regions affirms \nthe relation between stimuli and cognitive response.\n\nSignal detection and segmentation is the process by \nwhich the signal of interest is detected from the origi-\nnal signal and then separated for further procedures. \nThe energy of the signal may be used as a threshold \nfor detection of the signal. Often the Neuromarketing \nexperiments contain multiple types of stimuli shown \nto the test subjects. In such cases segmentation sepa-\nrates the event-based time signals for further pro-\ncessing, example Bhardwaj et  al. [58]. Segmentation \nor framing the EEG signals to a shorter time window \n\n\n\nPage 13 of 19Rawnaque et al. Brain Inf. (2020) 7:10 \n\nis mostly required to process the signal in time–fre-\nquency domain [58]. Cherubino et  al. [42] segmented \ntheir acquired and filtered EEG traces to extract the \ncerebral activity during the exposure to the market-\ning stimuli. Oon et al. [55] used 1-s segmentation time \nto extract non-linear detrended fluctuation analysis \nfeatures.\n\nThe goal of feature extraction is to find the set of \nfeature that minimizes intra-class variability and maxi-\nmizes inter-class variability. So we need to extract use-\nful information from the preprocessed signal, which \ncan be spatial, spectral or temporal [45]. As the EEG \nsignal is non-stationary, the feature extraction pro-\ncedure is quite often complicated. Discrete wavelet \ntransformation (DWT) is a viable way to extract fea-\ntures from EEG signals.\n\nYadava et  al. [18] performed DWT-based four-level \nwavelet analysis to extract features from their EEG sig-\nnals and decomposed the EEG signal into delta, theta, \nalpha, beta and gamma frequency bands. Another \nfeature extraction approach, principal component \nanalysis (PCA) was used by Venkatraman et  al. [38] \nfor extracting fMRI features in their Neuromarketing \nexperiment. In 2016, Fan and Touyama applied spatial \nand temporal principal component analysis (STPCA) \nfor feature extraction from ERP P300 signal. Rakshit \nand Lahiri [67] used a different approach to extract \nfeatures from EEG signals. They used Welch method \nfor one-sided power spectral density estimate and then \napplied a 256-point DFT algorithm on hamming win-\ndow of length 50 to extract features. Chew et  al. [17] \nadopted Hadjidimitriou and Hadjileontiadis methods \nin feature extraction where the feature estimation is \nbased on the event-related synchronization and desyn-\nchronization theory.\n\nFeature selection is also popularly known as dimen-\nsionality reduction or subset selection. This is a well-\nknown concept in machine learning which is about \nselecting an optimal set of features that decreases \ndimensionality, but has the most contribution to the \nclassification accuracy. In the past few years, feature \nselection has caught the attention of most research-\ners because of the nature of high dimensionality of \nbio-signals and the low number of sample data. Selec-\ntion of the optimal feature subset is always relative to \nan evaluation function. In most cases it is the evalua-\ntion function that measures the classification accu-\nracy. Feature selection techniques can be divided into \nthree categories, namely: filter, wrapper and embed-\nded approach. Wang et al. [30] used Recursive Cluster \nElimination (RCE) algorithm in spatiotemporal fMRI \nfeature selection. Soria Morillo et al. [40] used PCA for \nfeature reduction from their dataset. One-way analyses \n\nof variance (ANOVA) then cross-validation were also \nfound in use to identify the optimal feature set for cog-\nnitive or affective state classification by Yang et al. [41].\n\n3.5 Machine learning application in Neuromarketing\nUsing advanced neural recording method and signal pro-\ncessing tools, one can analyze EEG signals and interpret \ntheir correspondence with marketing stimuli. Frontal \nalpha asymmetry theory helped the researcher classify \nemotional approach/withdrawal response of the test sub-\njects using sub-band power of left and right hemispheric \nfrontal electrode [21]. However, classifying approach/\nwithdrawal or like/dislike without the FAA is possible, \neven possible from single electrode EEG signals. This \nrequires advanced Machine Learning algorithm appli-\ncation in Neuromarketing. Both supervised and unsu-\npervised learning methods were used in the following \nNeuromarketing experiments. Supervised learning in \nNeuromarketing uses a priori ground truth, usually the \ninterviewed response (like/dislike) from the test subjects \nas the labels. The labels help the classifier know the sig-\nnal pattern of like and dislike EEGs in the training data-\nsets. During the testing phase, like/dislike is predicted \nfrom a dataset without the labels. Researcher can hide \nthe training dataset labels from the classifier, and later \nuse it for accuracy calculation. On the other hand, unsu-\npervised learning approach used in Neuromarketing does \nnot require prior knowledge of the like/dislike labels. \nIt analyzes the signals with an aim to infer the existing \nstructures for different classes. Supervised learning usu-\nally solves either classification problem or a regression \nproblem. Support Vector Machines (SVM), Naive Bayes, \nArtificial Neural Networks (ANN), and Random Forests \n(RF) are the most common supervised learning classifi-\ners in Neuromarketing. In parallel, unsupervised learning \nin Neuromarketing has prominently the clustering type \nclassifiers, such as K-NN (k-nearest neighbors), principal \ncomponent analysis, singular value decomposition, and \nindependent component analysis (ICA).\n\nNeuromarketing researches over the last 5 years mainly \ndealt with like/dislike classification problem and pre-\ndicting ", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNzA4LTAyMC0wMDEwOS14LnBkZg2", "metadata_author": "Ferdousi Sabera Rawnaque ", "metadata_title": "Technological advancements and opportunities in Neuromarketing: a systematic review", "metadata_creation_date": "2020-09-18T02:02:41Z", "keyphrases": [ "Technological advancements", "systematic review", "opportunities", "Neuromarketing" ] }, { "@search.score": 1, "content": "\nDetecting problematic transactions \nin a consumer‑to‑consumer e‑commerce \nnetwork\nShun Kodate1,2, Ryusuke Chiba3, Shunya Kimura3 and Naoki Masuda2,4,5* \n\nIntroduction\nIn tandem with the rapid growth of online and electronic transactions and communi-\ncations, fraud is expanding at a dramatic speed and penetrates our daily lives. Fraud \nincluding cybercrimes costs billions of dollars per year and threatens the security of our \nsociety (UK Parliament 2017; McAfee 2019). In particular, in the recent era where online \nactivity dominates, attacking a system is not too costly, whereas defending the system \nagainst fraud is costly (Anderson et al. 2013). The dimension of fraud is vast and ranges \nfrom credit card fraud, money laundering, computer intrusion, to plagiarism, to name a \nfew.\n\nAbstract \n\nProviders of online marketplaces are constantly combatting against problematic \ntransactions, such as selling illegal items and posting fictive items, exercised by some \nof their users. A typical approach to detect fraud activity has been to analyze registered \nuser profiles, user’s behavior, and texts attached to individual transactions and the user. \nHowever, this traditional approach may be limited because malicious users can easily \nconceal their information. Given this background, network indices have been exploited \nfor detecting frauds in various online transaction platforms. In the present study, we \nanalyzed networks of users of an online consumer-to-consumer marketplace in which \na seller and the corresponding buyer of a transaction are connected by a directed \nedge. We constructed egocentric networks of each of several hundreds of fraudulent \nusers and those of a similar number of normal users. We calculated eight local network \nindices based on up to connectivity between the neighbors of the focal node. Based \non the present descriptive analysis of these network indices, we fed twelve features \nthat we constructed from the eight network indices to random forest classifiers with \nthe aim of distinguishing between normal users and fraudulent users engaged in each \none of the four types of problematic transactions. We found that the classifier accu-\nrately distinguished the fraudulent users from normal users and that the classification \nperformance did not depend on the type of problematic transaction.\n\nKeywords: Network analysis, Machine learning, Fraud detection, Computational social \nscience\n\nOpen Access\n\n© The Author(s) 2020. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://\ncreat iveco mmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nKodate et al. Appl Netw Sci (2020) 5:90 \nhttps://doi.org/10.1007/s41109‑020‑00330‑x Applied Network Science\n\n*Correspondence: \nnaokimas@buffalo.edu \n4 Department \nof Mathematics, University \nat Buffalo, Buffalo, NY \n14260-2900, USA\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0003-1567-801X\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1007/s41109-020-00330-x&domain=pdf\n\n\nPage 2 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nComputational and statistical methods for detecting and preventing fraud have been \ndeveloped and implemented for decades (Bolton and Hand 2002; Phua et  al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Standard practice for fraud detec-\ntion is to employ statistical methods including the case of machine learning algorithms. \nIn particular, when both fraudulent and non-fraudulent samples are available, one can \nconstruct a classifier via supervised learning (Bolton and Hand 2002; Phua et al. 2010; \nAbdallah et al. 2016; West and Bhattacharya 2016). Exemplar features to be fed to such a \nstatistical classifier include the transaction amount, day of the week, item category, and \nuser’s address for detecting frauds in credit card systems, number of calls, call duration, \ncall type, and user’s age, gender, and geographical region in the case of telecommunica-\ntion, and user profiles and transaction history in the case of online auctions (Abdallah \net al. 2016).\n\nHowever, many of these features can be easily faked by advanced fraudsters (Akoglu \net al. 2015; Google LLC 2018). Furthermore, fraudulent users are adept at escaping the \neyes of the administrators or authorities that would detect the usage of particular words \nas a signature of anomalous behavior (Pu and Webb 2006; Hayes 2007; Bhowmick and \nHazarika 2016). For example, if the authority discovers that one jargon means a drug, \nthen fraudulent users may easily switch to another jargon to confuse the authority.\n\nNetwork analysis is an alternative way to construct features and is not new to fraud \ndetection techniques (Savage et al. 2014; Akoglu et al. 2015). The idea is to use connec-\ntivity between nodes, which are usually users or goods, in the given data and calculate \ngraph-theoretic quantities or scores that characterize nodes. These methods stand on \nthe expectation that anomalous users show connectivity patterns that are distinct from \nthose of normal users (Akoglu et al. 2015). Network analysis has been deployed for fraud \ndetection in insurance (Šubelj et  al. 2011), money laundering (Dreżewski et  al. 2015; \nColladon and Remondi 2017; Savage et al. 2017), health-care data (Liu et al. 2016), car-\nbooking (Shchur et al. 2018), a social security system (Van Vlasselaer et al. 2016), mobile \nadvertising (Hu et al. 2017), a mobile phone network (Ferrara et al. 2014), online social \nnetworks (Bhat and Abulaish 2013; Jiang et  al. 2014; Hooi et  al. 2016; Rasheed et  al. \n2018), online review forums (Akoglu et al. 2013; Liu et al. 2017; Wang et al. 2018), online \nauction or marketplaces (Chau et  al. 2006; Pandit et  al. 2007; Wang and Chiu 2008; \nBangcharoensap et  al. 2015; Yanchun et  al. 2011), credit card transactions (Van Vlas-\nselaer et al. 2015; Li et al. 2017), cryptocurrency transaction (Monamo et al. 2016), and \nvarious other fields (Akoglu et al. 2010). For example, fraudulent users and their accom-\nplices were shown to form approximately bipartite cores in a network of users to inflate \ntheir reputations in an online auction system (Chau et al. 2006). Then, the authors pro-\nposed an algorithm based on a belief propagation to detect such suspicious connectivity \npatterns. This method has been proven to be also effective on empirical data obtained \nfrom eBay (Pandit et al. 2007).\n\nIn the present study, we analyze a data set obtained from a large online consumer-to-\nconsumer (C2C) marketplace, Mercari, operating in Japan and the US. They are the larg-\nest C2C marketplace in Japan, in which, as of 2019, there are 13 million monthly active \nusers and 133 billion yen (approximately 1.2 billion USD) transactions per quarter year \n(Mercari 2019). Note that we analyze transaction frauds based on transaction networks \nof users, which contrasts with previous studies of online C2C marketplaces that looked \n\n\n\nPage 3 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nat reputation frauds (Chau et al. 2006; Pandit et al. 2007; Wang and Chiu 2008; Yanchun \net  al. 2011). Many prior network-based fraud detection algorithms used global infor-\nmation about networks, such as connected components, communities, betweenness, \nk-cores, and that determined by belief propagation (Chau et al. 2006; Pandit et al. 2007; \nWang and Chiu 2008; Šubelj et  al. 2011; Akoglu et  al. 2013; Bhat and Abulaish 2013; \nFerrara et al. 2014; Jiang et al. 2014; Bangcharoensap et al. 2015; Dreżewski et al. 2015; \nVan Vlasselaer et al. 2015; Hooi et al. 2016; Liu et al. 2016; Van Vlasselaer et al. 2016; \nColladon and Remondi 2017; Hu et al. 2017; Li et al. 2017; Liu et al. 2017; Savage et al. \n2017; Shchur et al. 2018; Rasheed et al. 2018; Wang et al. 2018). Others used local infor-\nmation about the users’ network, such as the degree, the number of triangles, and the \nlocal clustering coefficient (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yan-\nchun et al. 2011; Bhat and Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et al. \n2015; Monamo et al. 2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017). We \nwill focus on local features of users, i.e., features of a node that can be calculated from \nthe connectivity of the user and the connectivity between neighbors of the user. This is \nbecause local features are easier and faster to calculate and thus practical for commercial \nimplementations.\n\nMaterials and methods\nData\n\nMercari is an online C2C marketplace service, where users trade various items among \nthemselves. The service is operating in Japan and the United States. In the present study, \nwe used the data obtained from the Japanese market between July 2013 and January \n2019. In addition to normal transactions, we focused on the following types of prob-\nlematic transactions: fictive, underwear, medicine, and weapon. Fictive transactions are \ndefined as selling non-existing items. Underwear refers to transactions of used under-\nwear; they are prohibited by the service from the perspective of morality and hygiene. \nMedicine refers to transactions of medicinal supplies, which are prohibited by the law. \nWeapon refers to transactions of weapons, which are prohibited by the service because \nthey may lead to crime. The number of sampled users of each type is shown in Table 1.\n\nNetwork analysis\n\nWe examine a directed and weighted network of users in which a user corresponds to a \nnode and a transaction between two users represents a directed edge. The weight of the \nedge is equal to the number of transactions between the seller and the buyer. We con-\nstructed egocentric networks of each of several hundreds of normal users and those of \nfraudulent users, i.e., those engaged in at least one problematic sell. Figure 1 shows the \negocentric networks of two normal users (Fig. 1a, b) and those of two fraudulent users \ninvolved in selling a fictive item (Fig. 1c, d). The egocentric network of either a normal or \nfraudulent user contained the nodes neighboring the focal user, edges between the focal \nuser and these neighbors, and edges between the pairs of these neighbors.\n\nWe calculated eight indices for each focal node. They are local indices in the mean-\ning that they require the information up to the connectivity among the neighbors of the \nfocal node.\n\n\n\nPage 4 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nFive out of the eight indices use only the information about the connectivity of the focal \nnode. The degree ki of node vi is the number of its neighbors. The node strength  (Barrat \net al. 2004) (i.e., weighted degree) of node vi , denoted by si , is the number of transactions in \nwhich vi is involved. Using these two indices, we also considered the mean number of trans-\nactions per neighbor, i.e., si/ki , as a separate index. These three indices do not use informa-\ntion about the direction of edges.\n\nThe sell probability of node vi , denoted by SPi , uses the information about the direction of \nedges and defined as the proportion of the vi ’s neighbors for which vi acts as seller. Precisely, \nthe sell probability is given by\n\n(1)SPi =\nkouti\n\nk ini + kouti\n\n,\n\nFig. 1 Examples of egocentric networks. a, b Egocentric networks of arbitrarily selected two normal users. c, \nd Egocentric networks of arbitrarily selected two fraudulent users involved in selling a fictive item\n\n\n\nPage 5 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nwhere k ini is vi ’s in-degree (i.e., the number of neighbors from whom vi bought at least \none item) and kouti is vi ’s out-degree (i.e., the number of neighbors to whom vi sold at \nleast one item). It should be noted that, if vi acted as both seller and buyer towards vj , the \ncontribution of vj to both in- and out-degree of vi is equal to one. Therefore, k ini + kouti is \nnot equal to ki in general.\n\nThe weighted version of the sell probability, denoted by WSPi , is defined as\n\nwhere sini is node vi ’s weighted in-degree (i.e., the number of buys) and souti is vi ’s weighted \nout-degree (i.e., the number of sells).\n\nThe other three indices are based on triangles that involve the focal node. The local \nclustering coefficient Ci quantifies the abundance of undirected and unweighted triangles \naround vi (Newman 2010). It is defined as the number of undirected and unweighted trian-\ngles including vi divided by ki(ki − 1)/2 . The local clustering coefficient Ci ranges between \n0 and 1.\n\nWe hypothesized that triangles contributing to an increase in the local clustering coef-\nficient are localized around particular neighbors of node vi . Such neighbors together with vi \nmay form an overlapping set of triangles, which may be regarded as a community (Radicchi \net al. 2004; Palla et al. 2005). Therefore, our hypothesis implies that the extent to which the \nfocal node is involved in communities should be different between normal and fraudulent \nusers. To quantify this concept, we introduce the so-called triangle congregation, denoted \nby mi . It is defined as the extent to which two triangles involving vi share another node and \nis given by\n\nwhere Tri = Ciki(ki − 1)/2 is the number of triangles involving vi . Note that mi ranges \nbetween 0 and 1.\n\nFrequencies of different directed three-node subnetworks, conventionally known as net-\nwork motifs (Milo et al. 2002), may distinguish between normal and fraudulent users. In \nparticular, among triangles composed of directed edges, we hypothesized that feedforward \ntriangles (Fig. 2a) should be natural and that cyclic triangles (Fig. 2b) are not. We hypoth-\nesized so because a natural interpretation of a feedforward triangle is that a node with out-\ndegree two tends to serve as seller while that with out-degree zero tends to serve as buyer \nand there are many such nodes that use the marketplace mostly as buyer or seller but not \nboth. In contrast, an abundance of cyclic triangles may imply that relatively many users use \nthe marketplace as both buyer and seller. We used the index called the cycle probability, \ndenoted by CYPi , which is defined by\n\nwhere FFi and CYi are the numbers of feedforward triangles and cyclic triangles to which \nnode vi belongs. The definition of FFi and CYi , and hence CYPi , is valid even when the \n\n(2)WSPi =\nsouti\n\nsini + souti\n\n,\n\n(3)mi =\n(Number of pairs of triangles involving vi that share another node)\n\nTri(Tri − 1)/2\n,\n\n(4)CYPi =\nCYi\n\nFFi + CYi\n,\n\n\n\nPage 6 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntriangles involving vi have bidirectional edges. In the case of Fig. 2c, for example, any of \nthe three nodes contains one feedforward triangle and one cyclic triangle. The other four \ncases in which bidirectional edges are involved in triangles are shown in Fig. 2d–g. In the \ncalculation of CYPi , we ignored the weights of edges.\n\nRandom forest classifier\n\nTo classify users into normal and fraudulent users based on their local network proper-\nties, we employed a random forest classifier (Breiman 2001; Breiman et al. 1984; Hastie \net  al. 2009) implemented in scikit-learn (Pedregosa et  al. 2011). It uses an ensemble \nlearning method that combines multiple classifiers, each of which is a decision tree, \nbuilt from training data and classifies test data avoiding overfitting. We combined 300 \ndecision-tree classifiers to construct a random forest classifier. Each decision tree is con-\nstructed on the basis of training samples that are randomly subsampled with replace-\nment from the set of all the training samples. To compute the best split of each node \nin a tree, one randomly samples the candidate features from the set of all the features. \nThe probability that a test sample is positive in a tree is estimated as follows. Consider \nthe terminal node in the tree that a test sample eventually reaches. The fraction of posi-\ntive training samples at the terminal node gives the probability that the test sample is \nclassified as positive. One minus the positive probability gives the negative probability \nestimated for the same test sample. The positive or negative probability for the random \nforest classifier is obtained as the average of single-tree positive or negative probability \nover all the 300 trees. A sample is classified as positive by the random forest classifier if \nthe positive probability is larger than 0.5, otherwise classified as negative.\n\nWe split samples of each type into two sets such that 75% and 25% of the samples of \neach type are assigned to the training and test samples, respectively. There were more \n\ncyclicfeedforward feedforward: 1\ncyclic: 1\n\nfeedforward: 2\ncyclic: 0\n\nfeedforward: 3\ncyclic: 1\n\nfeedforward: 6\ncyclic: 2\n\na b c d\n\nf g\n\nfeedforward: 2\ncyclic: 0\n\ne\n\nFig. 2 Directed triangle patterns and their count. a Feedforward triangle. b Cyclic triangle. c– g Five \nthree-node patterns that contain directed triangles and reciprocal edges. The numbers shown in the figure \nrepresent the number of feedforward or cyclic triangles to which each three-node pattern contributes\n\n\n\nPage 7 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nnormal users than any type of fraudulent user. Therefore, to balance the number of \nthe negative (i.e., normal) and positive (i.e., fraudulent) samples, we uniformly ran-\ndomly subsampled the negative samples (i.e., under-sampling) such that the number \nof the samples is the same between the normal and fraudulent types in the training \nset. Based on the training sample constructed in this manner, we built each of the 300 \ndecision trees and hence a random forest classifier. Then, we examined the classifica-\ntion performance of the random forest classifier on the set of test samples.\n\nThe true positive rate, also called the recall, is defined as the proportion of the posi-\ntive samples (i.e., fraudulent users) that the random forest classifier correctly classifies \nas positive. The false positive rate is defined as the proportion of the negative samples \n(i.e., normal users) that are incorrectly classified as positive. The precision is defined \nas the proportion of the truly positive samples among those that are classified as posi-\ntive. The true positive rate, false positive rate, and precision range between 0 and 1.\n\nWe used the following two performance measures for the random forest classifier. \nTo draw the receiver operating characteristic (ROC) curve for a random forest clas-\nsifier, one first arranges the test samples in descending order of the estimated prob-\nability that they are positive. Then, one plots each test sample, with its false positive \nrate on the horizontal axis and the true positive rate on the vertical axis. By connect-\ning the test samples in a piecewise linear manner, one obtains the ROC curve. The \nprecision–recall (PR) curve is generated by plotting the samples in the same order in \n[0, 1]2 , with the recall on the horizontal axis and the precision on the vertical axis. For \nan accurate binary classifier, both ROC and PR curves visit near (x, y) = (0, 1) . There-\nfore, we quantify the performance of the classifier by the area under the curve (AUC) \nof each curve. The AUC ranges between 0 and 1, and a large value indicates a good \nperformance of the random forest classifier.\n\nTo calculate the importance of each feature in the random forest classifier, we \nused the permutation importance (Strobl et al. 2007; Altmann et al. 2010). With this \nmethod, the importance of a feature is given by the decrease in the performance of \nthe trained classifier when the feature is randomly permuted among the test samples. \nA large value indicates that the feature considerably contributes to the performance \nof the classifier. To calculate the permutation importance, we used the AUC value of \nthe ROC curve as the performance measure of a random forest classifier. We com-\nputed the permutation importance of each feature with ten different permutations \nand adopted the average over the ten permutations as the importance of the feature.\n\nWe optimized the parameters of the random forest classifier by a grid search with \n10-fold cross-validation on the training set. For the maximum depth of each tree (i.e., \nthe max_depth parameter in scikit-learn), we explored the integers between 3 and 10. \nFor the number of candidate features for each split (i.e., max_features), we explored \nthe integers between 3 and 6. For the minimum number of samples required at termi-\nnal nodes (i.e., min_samples_leaf ), we explored 1, 3, and 5. As mentioned above, the \nnumber of trees (i.e., n_estimators) was set to 300. The seed number for the random \nnumber generator (i.e., random_state) was set to 0. For the other hyperparameters, \nwe used the default values in scikit-learn version 0.22. In the parameter optimization, \nwe evaluated the performance of the random forest classifier with the AUC value of \nthe ROC curve measured on a single set of training and test samples.\n\n\n\nPage 8 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nTo avoid sampling bias, we built 100 random forest classifiers, trained each classifier, \nand tested its performance on a randomly drawn set of train and test samples, whose \nsampling scheme was described above.\n\nResults\nDescriptive statistics\n\nThe survival probability of the degree (i.e., a fraction of nodes whose degree is larger \nthan a specified value) is shown in Fig. 3a for each user type. Approximately 60% of the \nnormal users have degree ki = 1 , whereas the fraction of the users with ki = 1 is approxi-\nmately equal to 2% or less for any type of fraudulent user (Table 1). Therefore, we expect \nthat whether ki = 1 or ki ≥ 2 gives useful information for distinguishing between normal \nand fraudulent users. The degree distribution at ki ≥ 2 may provide further information \nuseful for the classification. The survival probability of the degree distribution condi-\ntioned on ki ≥ 2 for the different types of users is shown in Fig. 3b. The figure suggests \nthat the degree distribution is systematically different between the normal and fraudu-\nlent users. However, we consider that the difference is not as clear-cut as that in the frac-\ntion of users having ki = 1 (Table 1).\n\nThe survival probability of the node strength (i.e., weighted degree) is shown in Fig. 3c \nfor each user type. As in the case for the unweighted degree, we found that many nor-\nmal users, but not fraudulent users, have si = 1 . In fact, the number of the normal users \nwith si = 1 is equal to those with ki = 1 (Table 1), implying that all normal users with \nki = 1 participated in just one transaction. In contrast, no user had si = 1 for any type \nof fraudulent user. The survival probability of the node strength conditioned on si ≥ 2 \napparently does not show a clear distinction between the normal and fraudulent users \n(Fig. 3d, Table 1).\n\na b\n\nc d\n\nFig. 3 Survival probability of the degree for each user type. a Degree (i.e., ki ) for all nodes. b Degree for the \nnodes with ki ≥ 2 . c Strength (i.e., si ) for all nodes. d Strength for the nodes with si ≥ 2\n\n\n\nPage 9 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nThe distribution of the average number of transactions per edge, i.e., si/ki , is shown \nin Fig. 4a. We found that a majority of normal users have si/ki = 1 . This result indicates \nthat a large fraction of normal users is engaged in just one transaction per neighbor \n(Table 1). This result is consistent with the fact that approximately 60% of the normal \nusers have ki = si = 1 . In contrast, many of any type of fraudulent users have si/ki > 1 . \nHowever, they tend to have a smaller value of si/ki than the normal users. This differ-\nence is more noticeable when we discraded the users with si/ki = 1 (Fig. 4b, Table 1). \nTherefore, less frequent transactions with a specific neighbor seem to be a characteristic \nbehavior of fraudulent users.\n\nThe distribution of the unweighted sell probability for the different user types is \nshown in Fig.  5a. The distribution for the normal users is peaked around 0 and 1, \n\nTable 1 Properties of different types of users\n\nIn the first column, Mean ( A | B ), for example, represents the mean of A conditioned on B. Unless the first column mentions \nthe conditional mean, median, or the number of transactions, the numbers reported in the table represent the number of \nusers\n\nSeed user type Normal Fictive Underwear Medicine Weapon\n\nNumber of seed users 999 440 468 469 416\n\nNumber of transactions \ninvolving the seed user\n\n151,021 66,215 151,278 92,497 81,970\n\nTotal number of transactions 27,683,860 850,739 2,325,898 925,361 533,963\n\nki = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( ki | ki ≥ 2) 195.0 138.3 297.8 184.2 179.7\n\nMedian ( ki | ki ≥ 2) 77.5 61.0 170.0 97.0 86.0\n\nsi = 1 587 (58.8%) 8 (1.8%) 3 (0.6%) 2 (0.4%) 5 (1.2%)\n\nMean ( si | si ≥ 2) 365.1 153.3 325.3 198.1 199.4\n\nMedian ( si | si ≥ 2) 89.0 66.5 175.0 100.0 90.0\n\nsi ≥ 2 412 432 465 467 411\n\nsi/ki = 1 97 (23.5%) 97 (22.5%) 86 (18.5%) 156 (33.4%) 121 (29.4%)\n\nMean ( si/ki | si/ki > 1) 1.413 1.135 1.055 1.066 1.092\n\nMedian ( si/ki | si/ki > 1) 1.124 1.059 1.03 1.031 1.055\n\nki ≥ 2 412 432 465 467 411\n\nSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\nk\nout\ni\n\n= 1 118 (28.6%) 21 (4.9%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nsi ≥ 2 412 432 465 467 411\n\nWSPi = 1 157 (38.1%) 15 (3.5%) 21 (4.5%) 16 (3.4%) 17 (4.1%)\n\ns\nout\ni\n\n= 1 118 (28.6%) 14 (3.2%) 2 (0.4%) 2 (0.4%) 9 (2.2%)\n\nki ≥ 2 412 432 465 467 411\n\nCi = 0 118 (28.6%) 152 (35.2%) 108 (23.2%) 154 (33.0%) 128 (31.1%)\n\nMean ( Ci | Ci > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( Ci | Ci > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nTri ≥ 2 262 241 317 251 244\n\nmi = 0 17 (6.5%) 27 (11.2%) 54 (17.0%) 44 (17.5%) 32 (13.1%)\n\nmi = 1 12 (4.6%) 9 (3.7%) 4 (1.3%) 6 (2.4%) 11 (4.5%)\n\nMean ( mi | mi > 0) 8.554× 10\n−3\n\n8.348× 10\n−3 9.500× 10\n\n−4\n2.231× 10\n\n−3\n3.810× 10\n\n−3\n\nMedian ( mi | mi > 0) 2.411× 10\n−3\n\n2.039× 10\n−3 5.288× 10\n\n−4\n6.494× 10\n\n−4\n1.337× 10\n\n−3\n\nFFi + CYi ≥ 1 294 280 357 313 283\n\nCYPi = 0 234 (79.6%) 188 (67.1%) 222 (62.2%) 227 (72.5%) 202 (71.4%)\n\nMean ( CYPi | CYPi > 0) 1.987× 10\n−2\n\n7.367× 10\n−2\n\n6.739× 10\n−2\n\n8.551× 10\n−2\n\n5.544× 10\n−2\n\nMedian ( CYPi | CYPi > 0) 1.521× 10\n−2\n\n4.481× 10\n−2\n\n3.396× 10\n−2\n\n3.822× 10\n−2\n\n3.618× 10\n−2\n\n\n\nPage 10 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nindicating that a relatively large fraction of normal users is almost exclusive buyer or \nseller. Note that, by definition, the sell probability is at least 1/(k ini + kouti ) because our \nsamples are sellers. Therefore, a peak around the sell probability of zero implies that \nthe users probably have no or few sell transactions apart from the one sell transaction \nbased on which the users have been sampled as seller. In contrast, the distribution \nfor any fraudulent type is relatively flat. Figure  5b shows the relationships between \nthe unweighted sell probability and the degree. On the dashed line in Fig. 5b, the sell \nprobability is equal to 1/(k ini + kouti ) , indicating that the node has kouti = 1 , which is \nthe smallest possible out-degree. The users on this line were buyers in all but one \n\na b\n\nFig. 4 Survival probability of the average number of transactions per neighbor. a si/ki for all nodes. b si/ki for \nthe nodes with si/ki > 1\n\na b\n\nc d\n\nFig. 5 Sell probability for each user type. a Distribution of the unweighted sell probability. b Relationship \nbetween the degree and the unweighted sell probability. c Distribution of the weighted sell probability. d \nRelationship between the node strength and the weighted sell probability. The dashed lines in b, d indicate \n1/(k in\n\ni\n+ k\n\nout\ni\n\n) and 1/(sin\ni\n+ s\n\nout\ni\n\n) , respectively\n\n\n\nPage 11 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ntransaction. Figure 5b indicates that a majority of such users are normal as opposed \nto fraudulent users, which is quantitatively confirmed in Table 1. We also found that \nmost of the normal users were either on the horizontal line with the sell probability \nof one (38.1% of the normal users with ki ≥ 2 ; see Table 1 for the corresponding frac-\ntions of normal users with ki = 1 ) or on the dashed line (28.6%). This is not the case \nfor any type of fraudulent user (Table 1).\n\nThe distribution of the weighted sell probability for the different user types and the \nrelationships between the weighted sell probability and the node strength are shown \nin Fig.  5c, d, respectively. The results are similar to the case of the unweighted sell \nprobability in two aspects. First, the normal users and the fraudulent users form dis-\ntinct frequency distributions (Fig. 5c). Second, most of the normal users are either on \nthe horizontal line with the weighted sell probability of one or on the dashed line with \nthe smallest possible weighted sell probability, i.e., 1/si (Fig. 5d, Table 1).\n\nThe survival probability of the local clustering coefficient is shown in Fig.  6a. It \nshould be noted that, in this analysis, we confined ourselves to the users with ki ≥ 2 \nbecause Ci is undefined when ki = 1 . We found that the number of users with Ci = 0 is \nnot considerably different between the normal and fraudulent users (also see Table 1). \nFigure  6b shows the survival probability of Ci conditioned on Ci > 0 . The normal \nusers tend to have a larger value of Ci than fraudulent users, whereas this tendency is \nnot strong (Table 1).\n\nThe survival probability of the triangle congregation is shown in Fig. 7a. Contrary to \nour hypothesis, there is no clear difference between the distribution of the normal and \nfraudulent users. The triangle congregation tends to be large when the node strength \nis small (Fig. 7b) and the local clustering coefficient is large (Fig. 7d). It depends little \non the weighted sell probability (Fig. 7c). However, we did not find clear differences in \nthe triangle congregation between the normal and fraudulent users (also see Table 1).\n\nThe survival probability of the cycle probability is shown in Fig. 8a. A large fraction \nof any type of users has CYPi = 0 (Table 1). When the users with CYPi = 0 are dis-\ncarded, the normal users tend to have a smaller value of CYPi than any type of fraudu-\nlent users (Fig. 8b, Table 1).\n\na b\n\nFig. 6 Local clustering coefficient for each user type. a Survival probability. b Survival probability conditioned \non Ci > 0\n\n\n\nPage 12 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nClassification of users\n\nBased on the eight indices whose descriptive statistics were analyzed in the previ-\nous section, we defined 12 features and fed them to the random forest classifier. The \naim of the classifier is to distinguish between normal and fraudulent users. The first \nfeature is binary and whether the degree ki = 1 or ki ≥ 2 . The second feature is also \nbinary and whether the node strength si = 1 or si ≥ 2 . The third feature is si/ki , which \nis a real number greater than or equal to 1. The fourth feature is binary and whether the \nunweighted sell probability SPi = 1 or SPi < 1 . The fifth feature is binary and whether \n\na b\n\nc d\n\nFig. 7 Triangle congregation for each user type. a Survival probability. b Relationship between the triangle \ncongregation, mi , and the node strength. c Relationship between mi and the weighted sell probability. d \nRelationship between mi and the local clustering coefficient\n\na b\n\nFig. 8 Cycle probability for each user type. a Survival probability. b Survival probability conditioned on \nCYPi > 0\n\n\n\nPage 13 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nSPi = 1/(k ini + kouti ) or SPi > 1/(k ini + kouti ) , i.e., whether kouti = 1 or kouti > 1 . The sixth \nfeature is SPi , which ranges between 0 and 1. The seventh feature is binary and whether \nthe weighted sell probability WSPi = 1 or WSPi < 1 . The eighth feature is binary and \nwhether WSPi = 1/(sini + souti ) or WSPi > 1/(sini + souti ) , i.e., whether souti = 1 or \nsouti > 1 . The ninth feature is WSPi , which ranges between 0 and 1. The tenth feature is \nthe local clustering coefficient Ci , which ranges between 0 and 1. When ki = 1 , the local \nclustering coefficient is undefined. In this case, we set Ci = − 1 . The eleventh feature is \nthe triangle congregation mi , which ranges between 0 and 1. When there is no triangle \nor only one triangle involving vi , one cannot calculate mi . In this case, we set mi = − 1 . \nFinally, the twelfth feature is the cycle probability CYPi , which ranges between 0 and 1. \nWhen there is neither feedforward nor cyclic triangle involving vi , CYPi is undefined. In \nthis case, we set CYPi = − 1.\n\nThe ROC and PR curves when all the 12 features of users are used and the fraudu-\nlent type is fictive transactions are shown in Fig. 9a, b, respectively. Each thin line cor-\nresponds to one of the 100 classifiers. The thick lines correspond to the average of the \n100 lines. The dashed lines correspond to the uniformly random classification. Figure 9 \nindicates that the classification performance seems to be high. Quantitatively, for this \nand the other types of fraudulent users, the AUC values always exceeded 0.91 (Table 2).\n\nThe importance of each feature in the classifier is shown in Fig.  10a, separately for \nthe different fraud types. The importance of each feature is similar across the different \ntypes of fraud. Figure 10a indicates that the average number of transactions per neighbor \n(i.e., si/ki ), whether or not kouti = 1 (i.e., SPi = 1/(k ini + kouti ) ), whether or not souti = 1 \n(i.e., WSPi = 1/(sini + souti ) ), and the weighted sell probability (i.e., WSPi ) are the four \nfeatures of the highest importance. Given the results of the descriptive statistics in the \nprevious section, a small value of si/ki , kouti = 1 , souti = 1 , and a moderate WSPi value \nstrongly suggest that the user may be fraudulent.\n\nFigure 10a also suggests that the features based on the triangles, i.e., Ci , mi , and CYPi , \nare not strong contributors to the classifier’s performance. Because these features are the \nonly ones that require the information about the connectivity between pairs of neigh-\nbors of the focal node, it is practically beneficial if one can realize a similar classification \n\na b\n\nFig. 9 ROC and PR curves when the normal users and those involved in fictive transactions are classified. a \nROC curves. b PR curves. Each thin line corresponds to one of the 100 classifiers. The thick lines correspond to \nthe average of the 100 lines. The dashed lines correspond to the uniformly random classification\n\n\n\nPage 14 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nperformance without using these features; then only the information on the connectivity \nof the focal users is required. To explore this possibility, we constructed the random for-\nest classifier using the nine out of the twelve features that do not require the connectivity \nbetween neighbors of the focal node. The mean AUC values for the ROC and PR curves \nare shown in Table 2. We find that, despite some reduction in the performance scores \nrelative to the case of the classifier using all the 12 features, the AUC values with the \nnine features are still large, all exceeding 0.88. The permutation importance of the nine \nfeatures is shown in Fig. 10b. The results are similar to those when all the 12 features are \nused, although the importance of WSPi considerably increased in the case of the nine \nfeatures (Fig. 10a).\n\nMore than half of the normal users have ki = 1 , and there are few fraudulent users \nwith ki = 1 in each fraud category (Table 1). The classification between the normal and \nfraudulent users may be an easy problem for this reason, leading to the large AUC val-\nues. To exclude this possibility, we carried out a classification test for the subdata in \nwhich the normal and fraudulent users with ki = 1 were excluded, leaving 412 normal \nusers and a similar number of fraudulent users in each category (Table 1). We did not \n\na b\n\nFig. 10 Permutation importance of the features in the random forest classifier. a 12 features. b 9 features. The \nbars indicate the average over the 100 classifiers. The error bars indicate standard deviation\n\nTable 2 AUC values for the random forest classifiers\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n12 features\n\nROC 0.962 ± 0.003 0.981 ± 0.001 0.979 ± 0.003 0.969 ± 0.004\n\nPR 0.916 ± 0.009 0.948 ± 0.006 0.947 ± 0.005 0.916 ± 0.015\n\n9 features\n\nROC 0.951 ± 0.003 0.973 ± 0.003 0.971 ± 0.003 0.961 ± 0.004\n\nPR 0.889 ± 0.009 0.923 ± 0.010 0.930 ± 0.009 0.888 ± 0.025\n\n\n\nPage 15 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ncarry out subsampling because the number of the negative and positive samples were \nsimilar. Instead, we generated 100 different sets of train and test samples and built a clas-\nsifier based on each set of train and test samples. The AUC values when either 10 or 7 \nfeatures (i.e., the features excluding whether or not ki = 1 and whether or not si = 1 ) are \nused are shown in Table 3. The table indicates that the AUC values are still competitively \nlarge while they are smaller than those when whether or not ki = 1 and whether or not \nsi = 1 are used as features (Table 2).\n\nDiscussion\nWe showed that a random forest classifier using network features of users distinguished \ndifferent types of fraudulent users from normal users with approximately 0.91–0.98 in \nterms of the AUC. We only used the information about local transaction networks cen-\ntered around focal users to synthesize their features. We did so because it is better in \npractice not to demand the information about global transaction networks due to the \nlarge number of users. It should be noted that AUC values of ≈ 0.88–0.97 was also real-\nized when we only used the information about the connectivity of the focal user, not the \nconnectivity between the neighbors of the focal user. This result has a practical advan-\ntage when the present fraud-detection method is implemented online because it allows \none to classify users with a smaller amount of data per user.\n\nThe random forest classifier is an arbitrary choice. One can alternatively use a different \nlinear or nonlinear classifier to pursue a higher classification performance. This is left as \nfuture work. Other future tasks include the generalizability of the present results to dif-\nferent types of fraudulent transactions, such as resale tickets, pornography, and stolen \nitems, and to different platforms. In particular, if a classifier trained with test samples \nfrom fraudulent users of a particular type and normal users is effective at detecting dif-\nferent types of fraud, the classifier will also be potentially useful for detecting unknown \ntypes of fraudulent transactions. It is also a potentially relevant question to assess the \nclassification performance when one pools different types of fraud as a single positive \ncategory to train a classifier.\n\nPrior network-based fraud detection has employed either global or local network \nproperties to characterize nodes. Global network properties refer to those that require \nthe structure of the entire network for calculating a quantity for individual nodes, such \nas the connected component (Šubelj et al. 2011; Savage et al. 2017; Wang et al. 2018), \nbetweenness centrality (Šubelj et al. 2011; Dreżewski et al. 2015; Colladon and Remondi \n2017), user’s suspiciousness determined by belief propagation (Chau et al. 2006; Pandit \n\nTable 3 AUC values for the random forest classifiers excluding users with ki = 1\n\nThe average and standard deviation were calculated based on the 100 classifiers\n\nFictive Underwear Medicine Weapon\n\n10 features\n\nROC 0.925 ± 0.016 0.950 ± 0.013 0.954 ± 0.012 0.916 ± 0.019\n\nPR 0.923 ± 0.019 0.950 ± 0.018 0.954 ± 0.016 0.911 ± 0.023\n\n7 features\n\nROC 0.886 ± 0.020 0.921 ± 0.015 0.933 ± 0.014 0.899 ± 0.020\n\nPR 0.874 ± 0.027 0.901 ± 0.021 0.928 ± 0.019 0.880 ± 0.028\n\n\n\nPage 16 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\net al. 2007; Akoglu et al. 2013; Bangcharoensap et al. 2015; Van Vlasselaer et al. 2015, \n2016; Li et al. 2017; Hu et al. 2017), dense subgraphs including the case of communities \n(Šubelj et al. 2011; Bhat and Abulaish 2013; Ferrara et al. 2014; Jiang et al. 2014; Hooi \net al. 2016; Liu et al. 2016; Shchur et al. 2018), and k-core (Wang and Chiu 2008; Rasheed \net  al. 2018). Although many of these methods have accrued a high classification per-\nformance, they require the information about the entire network. Obtaining such data \nmay be difficult when the network is large or rapidly evolving over time, thus potentially \ncompromising the computation speed, memory requirement, and the accuracy of the \ninformation on the nodes and edges. Alternatively, other methods employed local net-\nwork properties such as the degree including the case of directed and/or weighted net-\nworks (Chau et al. 2006; Akoglu et al. 2010; Šubelj et al. 2011; Yanchun et al. 2011; Bhat \nand Abulaish 2013; Bangcharoensap et al. 2015; Dreżewski et  al. 2015; Monamo et al. \n2016; Van Vlasselaer et al. 2016; Colladon and Remondi 2017) and the abundance of tri-\nangles and quadrangles (Monamo et al. 2016; Van Vlasselaer et al. 2016). The use of local \nnetwork properties may be advantageous in industrial contexts, particularly to test sam-\npled users, because local quantities can be rapidly calculated given a seed node. Another \nreason for which we focused on local properties was that we could not obtain the global \nnetwork structure for computational reasons. It should be noted that, while the use of \nglobal network properties in addition to local ones may improve the classification accu-\nracy (Bhat and Abulaish 2013), the present local method attained a similar classification \nperformance to those based on global network properties, i.e., 0.880–0.986 in terms of \nthe ROC AUC (Šubelj et al. 2011; Van Vlasselaer et al. 2015; Van Vlasselaer et al. 2016; \nHu et al. 2017; Li et al. 2017; Savage et al. 2017).\n\nA prior study using data from the same marketplace, Mercari, aimed to distinguish \nbetween desirable non-professional frequent sellers and undesirable professional sellers \n(Yamamoto et al. 2019). The authors used information about user profiles, item descrip-\ntions, and other behavioral data such as the number of purchases per day. In contrast, \nwe focused on local network features of the users (while a quantity similar to WSPi was \nused as a feature in Yamamoto et al. (2019)). In addition, we used specific types of fraud-\nulent transactions, whereas Yamamoto et al. (2019) focused on problematic transactions \nas a single broad category. How the present results generalize to different categoriza-\ntions of fraudulent transactions, the platform’s different data such as their US market \ndata, and similar data obtained from other online marketplaces is unknown. Combining \nnetwork and non-network features may realize a better classification performance . Fur-\nthermore, using the information about the time of the transactions may also yield better \nclassification. Using the time information allows us to ask new questions such as predic-\ntion of users’ behavior. These topics warrant future work.\n\nAbbreviations\nC2C: Consumer-to-consumer; ROC: Receiver operating characteristic; PR: Precision–recall; AUC : Area under the curve.\n\nAcknowledgements\nThis work was carried out using the computational facilities of the Advanced Computing Research Centre, University of \nBristol.\n\nAuthors’ contributions\nShun Kodate analyzed data, developed methodology, visualized the results, and drafted the manuscript; RC curated data \nand critically revised the manuscript; Shunya Kimura coordinated the study, acquired funding, and critically revised the \nmanuscript; NM coordinated the study, acquired funding, developed methodology, drafted the manuscript. All authors \n\n\n\nPage 17 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\ngave final approval for publication and agreed to be held accountable for the work performed therein. All authors read \nand approved the final manuscript.\n\nFunding\nThe authors acknowledge financial support by Mercari, Inc. S. Kodate was supported in part by the Top Global University \nProject from the Ministry of Education, Culture, Sports, Science and Technology (MEXT) of Japan.\n\nAvailability of data and materials\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors \nare employees of the company). The figures and tables of the present paper are summary statistics of the data and not \nsufficient on their own for others to replicate the results of the present study. Although the data have been hashed, the \ncompany cannot share the data with the public. This is because, if anybody traces the transaction data on the Mercari’s \nweb platform and checks them against the hashed data, that person would be able to identify individual users including \ntheir private information. Therefore, hashing/anonymizing does not help to guarantee the users’ privacy. Any bona fide \nresearcher could approach the company (Shunya Kimura: kimuras@mercari.com and Ryusuke Chiba: metalunk@mercari.\ncom) to seek access to the complete dataset. However, for the aforementioned reasons, such an attempt is unlikely to be \nsuccessful. The users were made aware that their data may be used for the present research because the Mercari’s terms \nof use (in Japanese only: https ://www.merca ri.com/jp/tos/), Article 20, Term 2, states that their data can be used for \nresearch by the company and by those who the company permits.\n\nEthics approval and consent to participate\nMercari, Inc. approved the use of the data for the present study under the condition that the data were hashed and \nonly released to the collaborators of the project (i.e., the first and last authors, because the second and third authors are \nemployees of the company).\n\nCompeting interests\nThe second and third authors are employees of the company that provided the data analysed in the present manuscript. \nHowever, this fact does not cause any conflict of interest because the analyses, results and their interpretation are free of \nany bias towards the merit of the company.\n\nAuthor details\n1 Graduate School of Information Sciences, Tohoku University, Sendai 980-8579, Japan. 2 Department of Engineering \nMathematics, University of Bristol, Bristol BS8 1UB, UK. 3 Mercari, Inc., Tokyo 106-6118, Japan. 4 Department of Mathemat-\nics, University at Buffalo, Buffalo, NY 14260-2900, USA. 5 Computational and Data-Enabled Science and Engineering \nProgram, University at Buffalo, Buffalo, NY 14260-5030, USA. \n\nReceived: 12 August 2020 Accepted: 23 October 2020\n\nReferences\nAbdallah A, Maarof MA, Zainal A (2016) Fraud detection system: a survey. J Netw Comput Appl 68:90–113\nAkoglu L, McGlohon M, Faloutsos C (2010) Oddball: spotting anomalies in weighted graphs. In: Pacific-Asia conference on \n\nknowledge discovery and data mining, pp 410–421\nAkoglu L, Chandy R, Faloutsos C (2013) Opinion fraud detection in online reviews by network effects. In: 7th international \n\nAAAI conference on weblogs and social media, pp 2–11\nAkoglu L, Tong H, Koutra D (2015) Graph based anomaly detection and description: a survey. Data Min Knowl Discov \n\n29:626–688\nAltmann A, Toloşi L, Sander O, Lengauer T (2010) Permutation importance: a corrected feature importance measure. Bioinfo \n\n26:1340–1347\nAnderson R, Barton C, Böhme R, Clayton R, Van Eeten MJ, Levi M, Moore T, Savage S (2013) Measuring the cost of cybercrime. \n\nIn: The economics of information security and privacy. Springer, Berlin, pp 265–300\nBangcharoensap P, Kobayashi H, Shimizu N, Yamauchi S, Murata T (2015) Two step graph-based semi-supervised learning \n\nfor online auction fraud detection. In: Joint European conference on machine learning and knowledge discovery in \ndatabases, pp 165–179\n\nBarrat A, Barthelemy M, Pastor-Satorras R, Vespignani A (2004) The architecture of complex weighted networks. Proc Natl \nAcad Sci USA 101:3747–3752\n\nBhat SY, Abulaish M (2013) Community-based features for identifying spammers in online social networks. In: 2013 IEEE/ACM \ninternational conference on advances in social networks analysis and mining (ASONAM 2013), pp 100–107\n\nBhowmick A, Hazarika SM (2016) Machine learning for e-mail spam filtering: review, techniques and trends. Preprint arXiv \n:1606.01042 \n\nBolton RJ, Hand DJ (2002) Statistical fraud detection: a review. Stat Sci 17:235–249\nBreiman L (2001) Random forests. Mach Learn 45:5–32\nBreiman L, Friedman JH, Olshen RA, Stone CJ (1984) Classification and regression trees. Chapman & Hall, Boca Raton\nChau DH, Pandit S, Faloutsos C (2006) Detecting fraudulent personalities in networks of online auctioneers. In: European \n\nconference on principles of data mining and knowledge discovery, pp 103–114\nColladon AF, Remondi E (2017) Using social network analysis to prevent money laundering. Expert Syst Appl 67:49–58\nDreżewski R, Sepielak J, Filipkowski W (2015) The application of social network analysis algorithms in a system supporting \n\nmoney laundering detection. Inf Sci 295:18–32\nFerrara E, De Meo P, Catanese S, Fiumara G (2014) Detecting criminal organizations in mobile phone networks. Expert Syst \n\nAppl 41:5733–5750\nGoogle LLC and White Ops, Inc (2018) The Hunt for 3ve. https ://servi ces.googl e.com/fh/files /blogs /3ve_googl e_white \n\nops_white paper _final _nov_2018.pdf. Accessed: 10 May 2019\n\nhttps://www.mercari.com/jp/tos/\nhttp://arxiv.org/abs/1606.01042\nhttp://arxiv.org/abs/1606.01042\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\nhttps://services.google.com/fh/files/blogs/3ve_google_whiteops_whitepaper_final_nov_2018.pdf\n\n\nPage 18 of 18Kodate et al. Appl Netw Sci (2020) 5:90 \n\nHastie T, Tibshirani R, Friedman J (2009) The elements of statistical learning: data mining, inference, and prediction. Springer, \nNew York\n\nHayes B (2007) How many ways can you spell v1@gra? Am Sci 95:298–302\nHooi B, Song HA, Beutel A, Shah N, Shin K, Faloutsos C (2016) Fraudar: bounding graph fraud in the face of camouflage. In: \n\nProceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp 895–904\nHu J, Liang J, Dong S (2017) ibgp: a bipartite graph propagation approach for mobile advertising fraud detection. Mobile Inf \n\nSyst 2017:1–12\nJiang M, Cui P, Beutel A, Faloutsos C, Yang S (2014) Inferring strange behavior from connectivity pattern in social networks. In: \n\nPacific-Asia conference on knowledge discovery and data mining, pp 126–138\nLi Y, Sun Y, Contractor N (2017) Graph mining assisted semi-supervised learning for fraudulent cash-out detection. In: Pro-\n\nceedings of the 2017 IEEE/ACM international conference on advances in social networks analysis and mining 2017, pp \n546–553\n\nLiu J, Bier E, Wilson A, Guerra-Gomez JA, Honda T, Sricharan K, Gilpin L, Davies D (2016) Graph analysis for detecting fraud, \nwaste, and abuse in healthcare data. AI Mag 37:33–46\n\nLiu S, Hooi B, Faloutsos C (2017) Holoscope: topology-and-spike aware fraud detection. In: Proceedings of the 2017 ACM on \nconference on information and knowledge management, pp 1539–1548\n\nMcAfee LLC (2019) Economic impact of cybercrime report. https ://www.mcafe e.com/enter prise /en-us/solut ions/lp/econo \nmics-cyber crime .html. Accessed: 25 Apr 2018\n\nMercari Inc (2019) FY2019.6 Q3 Presentation Material. https ://about .merca ri.com/en/ir/libra ry/resul ts/. Accessed 1 Nov 2020\nMilo R, Shen-Orr S, Itzkovitz S, Kashtan N, Chklovskii D, Alon U (2002) Network motifs: simple building blocks of complex \n\nnetworks. Science 298:824–827\nMonamo P, Marivate V, Twala B (2016) Unsupervised learning for robust Bitcoin fraud detection. In: 2016 information security \n\nfor South Africa (ISSA), pp 129–134\nNewman M (2010) Networks: an introduction. Oxford University Press, Oxford\nPalla G, Derényi I, Farkas I, Vicsek T (2005) Uncovering the overlapping community structure of complex networks in nature \n\nand society. Nature 435:814–818\nPandit S, Chau DH, Wang S, Faloutsos C (2007) Netprobe: a fast and scalable system for fraud detection in online auction \n\nnetworks. In: Proceedings of the 16th international conference on world wide web, pp 201–210\nPedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M, Prettenhofer P, Weiss R, Dubourg V et al (2011) \n\nScikit-learn: machine learning in Python. J Mach Learn Res 12:2825–2830\nPhua C, Lee V, Smith K, Gayler R (2010) A comprehensive survey of data mining-based fraud detection research. Preprint arXiv \n\n:1009.6119\nPu C, Webb S (2006) Observed trends in spam construction techniques: a case study of spam evolution. In: CEAS, pp 104–112\nRadicchi F, Castellano C, Cecconi F, Loreto V, Parisi D (2004) Defining and identifying communities in networks. Proc Natl Acad \n\nSci USA 101:2658–2663\nRasheed J, Akram U, Malik AK (2018) Terrorist network analysis and identification of main actors using machine learning tech-\n\nniques. In: Proceedings of the 6th international conference on information technology: IoT and smart city, pp 7–12\nSavage D, Zhang X, Yu X, Chou P, Wang Q (2014) Anomaly detection in online social networks. Soc Netw 39:62–70\nSavage D, Wang Q, Zhang X, Chou P, Yu X (2017) Detection of money laundering groups: supervised learning on small net-\n\nworks. In: Workshops at the 31st AAAI conference on artificial intelligence, pp 43–49\nShchur O, Bojchevski A, Farghal M, Günnemann S, Saber Y (2018) Anomaly detection in car-booking graphs. In: 2018 IEEE \n\ninternational conference on data mining workshops (ICDMW), pp 604–607\nStrobl C, Boulesteix A-L, Zeileis A, Hothorn T (2007) Bias in random forest variable importance measures: illustrations, sources \n\nand a solution. BMC Bioinform 8:25\nŠubelj L, Furlan Š, Bajec M (2011) An expert system for detecting automobile insurance fraud using social network analysis. \n\nExpert Syst Appl 38:1039–1052\nUK Parliament: The Growing Threat of Online Fraud (2017). https ://old.parli ament .uk/busin ess/commi ttees /commi ttees -a-z/\n\ncommo ns-selec t/publi c-accou nts-commi ttee/inqui ries/parli ament -2017/growi ng-threa t-onlin e-fraud -17-19/publi catio \nns/. Accessed 1 Nov 2020\n\nVan Vlasselaer V, Bravo C, Caelen O, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2015) Apate: a novel approach for automated \ncredit card transaction fraud detection using network-based extensions. Decis Support Syst 75:38–48\n\nVan Vlasselaer V, Eliassi-Rad T, Akoglu L, Snoeck M, Baesens B (2016) Gotcha! network-based fraud detection for social security \nfraud. Manag Sci 63:3090–3110\n\nWang J-C, Chiu C-C (2008) Recommending trusted online auction sellers using social network analysis. Expert Syst Appl \n34:1666–1679\n\nWang Z, Gu S, Zhao X, Xu X (2018) Graph-based review spammer group detection. Knowl Inf Syst 55:571–597\nWest J, Bhattacharya M (2016) Intelligent financial fraud detection: a comprehensive review. Comput Secur 57:47–66\nYamamoto H, Sugiyama N, Toriumi F, Kashida H, Yamaguchi T (2019) Angels or demons? Classifying desirable heavy users and \n\nundesirable power sellers in online C2C marketplace. J Comput Soc Sci 2:315–329\nYanchun Z, Wei Z, Changhai Y (2011) Detection of feedback reputation fraud in Taobao using social network theory. In: 2011 \n\ninternational joint conference on service sciences, pp 188–192\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://www.mcafee.com/enterprise/en-us/solutions/lp/economics-cybercrime.html\nhttps://about.mercari.com/en/ir/library/results/\nhttp://arxiv.org/abs/1009.6119\nhttp://arxiv.org/abs/1009.6119\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\nhttps://old.parliament.uk/business/committees/committees-a-z/commons-select/public-accounts-committee/inquiries/parliament-2017/growing-threat-online-fraud-17-19/publications/\n\n\tDetecting problematic transactions in a consumer-to-consumer e-commerce network\n\tAbstract \n\tIntroduction\n\tMaterials and methods\n\tData\n\tNetwork analysis\n\tRandom forest classifier\n\n\tResults\n\tDescriptive statistics\n\tClassification of users\n\n\tDiscussion\n\tAcknowledgements\n\tReferences\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQxMTA5LTAyMC0wMDMzMC14LnBkZg2", "metadata_author": " Shun Kodate ", "metadata_title": "Detecting problematic transactions in a consumer-to-consumer e-commerce network", "metadata_creation_date": "2020-11-12T15:20:34Z", "keyphrases": [ "problematic transactions", "commerce network", "consumer" ] }, { "@search.score": 1, "content": "\nMulti technique amalgamation \nfor enhanced information identification \nwith content based image data\nRik Das1*, Sudeep Thepade2 and Saurav Ghosh3\n\nBackground\nRecent years have witnessed the digital photo-capture devices as a ubiquity for the com-\nmon mass (Raventós et al. 2015). The low cost storage, increasing computer power and \never accessible internet have kindled the popularity of digital image acquisition. Efficient \nindexing and identification of image data from these huge image repositories has nur-\ntured new research challenges in computer vision and machine learning (Madireddy \net  al. 2014). Automatic derivation of sematically-meaningful information from image \ncontent has become imperative as the traditional text based annotation technique has \nrevealed severe limitations to fetch information from the gigantic image datasets (Walia \net al. 2014). Conventional techniques of image recognition were based on text or key-\nwords based mapping of images which had limited image information. It was dependent \non the perception and vocabulary of the person performing the annotation. The manual \nprocess was highly time consuming and slow in nature. The aforesaid limitations have \n\nAbstract \n\nImage data has emerged as a resourceful foundation for information with proliferation \nof image capturing devices and social media. Diverse applications of images in areas \nincluding biomedicine, military, commerce, education have resulted in huge image \nrepositories. Semantically analogous images can be fruitfully recognized by means of \ncontent based image identification. However, the success of the technique has been \nlargely dependent on extraction of robust feature vectors from the image content. The \npaper has introduced three different techniques of content based feature extraction \nbased on image binarization, image transform and morphological operator respec-\ntively. The techniques were tested with four public datasets namely, Wang Dataset, \nOliva Torralba (OT Scene) Dataset, Corel Dataset and Caltech Dataset. The multi tech-\nnique feature extraction process was further integrated for decision fusion of image \nidentification to boost up the recognition rate. Classification result with the proposed \ntechnique has shown an average increase of 14.5 % in Precision compared to the exist-\ning techniques and the retrieval result with the introduced technique has shown an \naverage increase of 6.54 % in Precision over state-of-the art techniques.\n\nKeywords: Image classification, Image retrieval, Otsu’s threshold, Slant transform, \nMorphological operator, Fusion, t test\n\nOpen Access\n\n© 2015 Das et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nRESEARCH\n\nDas et al. SpringerPlus (2015) 4:749 \nDOI 10.1186/s40064-015-1515-4\n\n*Correspondence: rikdas78@\ngmail.com \n1 Department of Information \nTechnology, Xavier Institute \nof Social Service, Dr. Camil \nBulcke Path (Purulia Road), \nP.O. Box 7, Ranchi 834001, \nJharkhand, India\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40064-015-1515-4&domain=pdf\n\n\nPage 2 of 26Das et al. SpringerPlus (2015) 4:749 \n\nbeen effectively handled with content based image identification which has been exer-\ncised as an effective alternative to the customary text based process (Wang et al. 2013). \nThe competence of the content based image identification technique has been depend-\nent on the extraction of robust feature vectors. Diverse low level features namely, color, \nshape, texture etc. have constituted the process of feature extraction. However, an image \ncomprises of number of features which can hardly be defined by a single feature extrac-\ntion technique (Walia et al. 2014). Therefore, three different techniques of feature extrac-\ntion namely, feature extraction with image transform, feature extraction with image \nmorphology and feature extraction with image binarization have been proposed in this \npaper to leverage fusion of multi-technique feature extraction. The recognition decision \nof three different techniques was further integrated by means of Z score normalization \nto create hybrid architecture for content based image identification. The main contribu-\ntion of the paper has been to propose fusion architecture for content based image recog-\nnition with novel techniques of feature extraction for enhanced recognition rate.\n\nThe research objectives have been enlisted as follows:\n\n • Reducing the dimension of feature vectors.\n • Successfully implementing fusion based method of content based image identifica-\n\ntion.\n • Statistical validation of research results.\n • Comparison of research results with state-of-the art techniques.\n\nThree different techniques of feature extraction using image binarization, image trans-\nforms and morphological operators have been combined to develop fusion based archi-\ntecture for content based image classification and retrieval. Hence, it is in correlation with \nresearch on binarization based feature extraction, transform based feature extraction and \nmorphology based feature extraction from images. It is also in connection with research \non multi technique fusion for content based image identification. Therefore, the following \nfour subsections have reviewed some contemporary and earlier works on these four topics.\n\nFeature extraction using image transform\n\nChange of domain of the image elements has been carried out by using image trans-\nformation to represent the image by a set of energy spectrum. An image can be repre-\nsented as series of basis images which can be formed by extrapolating the image into a \nseries of basis functions (Annadurai and Shanmugalakshmi 2011). The basis images have \nbeen populated by using orthogonal unitary matrices as image transformation opera-\ntor. This image transformation from one representation to another has advantages in \ntwo aspects. An image can be expanded in the form of a series of waveforms with the \nuse of image transforms. The transformation process has been helpful to differentiate \nthe critical components of image patterns and in making them directly accessible for \nanalysis. Moreover, the transformed image data has a compact structure useful for effi-\ncient storage and transmission. The aforesaid properties of image transforms facilitate \nradical reduction of feature vector dimension to be extracted from the images. Diverse \ntechniques of feature extraction has been proposed by exploiting the properties of image \ntransforms to extract features from images using fractional energy coefficient (Kekre and \n\n\n\nPage 3 of 26Das et al. SpringerPlus (2015) 4:749 \n\nThepade 2009; Kekre et  al. 2010). The techniques have considered seven image trans-\nforms and fifteen fractional coefficients sets for efficient feature extraction. Original \nimages were divided into subbands by using multiple scales Biorthogonal wavelet trans-\nform and the subband coefficients were used as features for image classification (Prakash \net al. 2013). The feature spaces were reduced by applying Isomap-Hysime random aniso-\ntropic transform for classification of high dimensional data (Luo et al. 2013).\n\nImage binarization techniques for feature extraction\n\nFeature extraction from images has been largely carried out by means of image binariza-\ntion. Appropriate threshold selection has been imperative for execution of efficient image \nbinarization. Nevertheless, various factors including uneven illumination, inadequate \ncontrast etc. can have adverse effect on threshold computation (Valizadeh et  al. 2009). \nContemporary literatures on image binarization techniques have categorized three dif-\nferent techniques for threshold selection namely, mean threshold selection, local thresh-\nold selection and global threshold selection to deal with the unfavourable influences on \nthreshold selection. Enhanced classification results have been comprehended by feature \nextraction from mean threshold and multilevel mean threshold based binarized images \n(Kekre et al. 2013; Thepade et al. 2013a, b). Eventually, it has been identified that selection \nof mean threshold has not dealt with the standard deviation of the gray values and has \nconcentrated only on the average which has prevented the feature extraction techniques \nto take advantage of the spread of data to distinguish distinct features. Therefore, image \nsignature extraction was carried out with local threshold selection and global thresh-\nold selection for binarization, as the techniques were based on calculation of both mean \nand standard deviation of the gray values (Liu 2013; Yanli and Zhenxing 2012; Ramírez-\nOrtegón and Rojas 2010; Otsu 1979; Shaikh et al. 2013; Thepade et al. 2014a).\n\nUse of morphological operators for feature extraction\n\nCommercial viability of shape feature extraction has been well highlighted by systems \nlike Image Content (Flickner et  al. 1995), PicToSeek (Gevers and Smeulders 2000). \nTwo different categorization of shape descriptors namely, contour-based and region-\nbased descriptors have been elaborated in the existing literatures (Mehtre et  al. 1997; \nZhang and Lu 2004). Emphasize of the contour based descriptors has been on bound-\nary lines. Popular contour-based descriptors have embraced Fourier descriptor (Zhang \nand Lu 2003), curvature scale space (Mokhtarian and Mackworth 1992), and chain codes \n(Dubois and Glanz 1986). Feature extraction from complex shapes has been well car-\nried out by means of region-based descriptors, since the feature extraction has been per-\nformed from whole area of object (Kim and Kim 2000).\n\nFusion methodologies and multi technique feature extraction\n\nInformation recognition with image data has utilized the features extracted by means \nof diverse extraction techniques to harmonize each other for enhanced identification \nrate. Recent studies in information fusion have categorized the methodologies typically \ninto four classes, namely, early fusion, late fusion, hybrid fusion and intermediate fusion. \nEarly fusion combines the features of different techniques and produces it as a single \ninput to the learner. The process inherently increases the size of feature vector as the \n\n\n\nPage 4 of 26Das et al. SpringerPlus (2015) 4:749 \n\nconcentrated features easily correspond to higher dimensions. Late fusion applies sepa-\nrate learner to each feature extraction technique and fuses the decision with a combiner. \nAlthough it offers scalability in comparison to early fusion, still, it cannot explore the \nfeature level correlations, since it has to make local decisions primarily. Hybrid fusion \nmakes a mix of the two above mentioned techniques. Intermediate fusion integrates \nmultiple features by considering a joint model for decision to yield superior prediction \naccuracy (Zhu and Shyu 2015). Color and texture features were extracted by means of \n3 D color histogram and Gabor filters for fusion based image identification. The space \ncomplexity of the feature was further reduced by using genetic algorithm which has also \nobtained the optimum boundaries of numerical intervals. The process has enhanced \nsemantic retrieval by introducing feature selection technique to reduce memory con-\nsumption and to decrease retrieval process complexity (ElAlami 2011). Local descriptors \nbased on color and texture was calculated from Color moments and moments on Gabor \nfilter responses. Gradient vector flow fields were calculated to capture shape information \nin terms of edge images. The shape features were finally depicted by invariant moments. \nThe retrieval decisions with the features were fused for enhanced retrieval performance \n(Hiremath and Pujari 2007). Feature vectors comprising of color histogram and tex-\nture features based on a co-occurrence matrix were extracted from HSV color space \nto facilitate image retrieval (Yue et al. 2011). Visually significant point features chosen \nfrom images by means of fuzzy set theoretic approach. Computation of some invariant \ncolor features from these points was performed to gauge the similarity between images \n(Banerjee et al. 2009). Recognition process was boosted up by combining color layout \ndescriptor and Gabor texture descriptor as image signatures (Jalab 2011). Multi view \nfeatures comprising of color, texture and spatial structure descriptors have contributed \nfor increased retrieval rate (Shen and Wu 2013). Wavelet packets and Eigen values of \nGabor filters were extracted as feature vectors by the authors in (Irtaza et al. 2013) for \nneural network architecture of image identification. The back propagation neural net-\nwork was trained on sub repository of images generated from the main image reposi-\ntory and utilizes the right neighbourhood of the query image. This kind of training was \naimed to insure correct semantic retrieval in response to query images. Higher retrieval \nresults have been apprehended with intra-class and inter-class feature extraction from \nimages (Rahimi and Moghaddam 2013). In (ElAlami 2014), extraction of color and tex-\nture features through color co-occurrence matrix (CCM) and difference between pixels \nof scan pattern (DBPSP) has been demonstrated and an artificial neural network (ANN) \nbased classifier was designed. In (Subrahmanyam et  al. 2013), content-based image \nretrieval was carried out by integrating the modified color motif co-occurrence matrix \n(MCMCM) and difference between the pixels of a scan pattern (DBPSP) features with \nequal weights. Fusion of semantic retrieval results obtained by capturing colour, shape \nand texture with the color moment (CMs), angular radial transform descriptor and edge \nhistogram descriptor (EHD) features respectively had outclassed the Precision values of \nindividual techniques (Walia et al. 2014). Six semantics of local edge bins for EHD were \nconsidered which included the vertical and the horizontal edge (0,0), 45° edge and 135° \nedge of sub-image (0,0), non directional edge of sub-image (0,0) and vertical edge of sub-\nimage at (0,1). Color histogram and spatial orientation tree has been used for unique \nfeature extraction from images for retrieval purpose (Subrahmanyam et al. 2012).\n\n\n\nPage 5 of 26Das et al. SpringerPlus (2015) 4:749 \n\nMethods\nThree different techniques of feature extraction have been introduced in this work namely, \nfeature extraction with image binarization, feature extraction with image transform and \nfeature extraction with morphological operator. However, there are popular feature extrac-\ntion techniques like GIST descriptor which has much greater feature dimension com-\npared to the proposed techniques in the work. GIST creates 32 feature maps of same \nsize by convolving the image with 32 Gabor filters at 4 scales, 8 orientations (Douze et al. \n2009). It averages the feature values of each region by dividing each feature map into 16 \nregions. Finally, it concatenates the 16 average value of all 32 feature maps resulting in \n16 × 32 = 512 GIST descriptor. On the other hand, our approach has generated a fea-\nture dimension of 6 from each of the binarization and morphological technique. Feature \nextraction by applying image transform has yielded a feature size of 36. On the whole, the \nfeature size for the fusion based classifier was (6 + 36 + 6 = 48) which is far less than GIST \nand has much lesser computational overhead. Furthermore, fusion based architecture for \nclassification and retrieval have been proposed for enhanced identification rate of image \ndata. Each of the techniques of feature extraction as well as the methods for fusion based \narchitecture of classification and retrieval has been discussed in the following four subsec-\ntions and the description of datasets has been given in the fifth subsection.\n\nFeature extraction with image binarization\n\nInitially, the three color components namely, Red (R), Green (G) and Blue (B) were sepa-\nrated in each of the test images. A popular global threshold selection method named \nOtsu’s method has been applied separately on each of the color components for binari-\nzation as in Fig. 1. The above mentioned thresholding method has been largely used for \ndocument image binarzation. Otsu’s technique has been operated directly on the gray \nlevel histogram which has made it fast executable. It has been efficient to remove redun-\ndant details from the image to bring out the necessary image information. The method \nhas been considered as a non-parametric method which has considered two classes of \npixels, namely, the foreground pixels and the background pixels. It has calculated the \noptimal threshold by using the within-class variance and between-class variance. The \nseparation was carried out in such a way so that their combined intra-class variance is \nminimal (Otsu 1979; Shaikh et al. 2013). Comprehensive investigation has been carried \nout for the threshold that minimizes the intra-class variance represented by the weighted \nsum of variances of the two classes of pixels for each of the three color components.\n\nThe weighted within-class variance has been given in Eq. 1.\n\nq1(t) = ∑ ti=0P(i) where the class probabilities of different gray level pixels were estimated \nas shown in Eqs. 2 and 3:\n\n(1)σ 2\nw(t) = q1(t)σ\n\n2\n1 (t)+ q2(t)σ\n\n2\n2 (t)\n\n(2)q1(t) =\n\nt\n∑\n\ni=0\n\np(i)\n\n(3)\nq2(t) =\n\n255\n∑\n\ni=t+1\n\nP(i)\n\n\n\nPage 6 of 26Das et al. SpringerPlus (2015) 4:749 \n\nThe class means were given as in Eqs. 4 and 5:\n\nTotal variance (σ2) = Within-class variance (σw\n2(t)) + Between-class Variance(σb\n\n2(t)).\nSince the total variance was constant and independent of t, the effect of changing \n\nthe threshold was purely to shift the contributions of the two terms back and forth. \nBetween-class variance has been given in Eq. 6\n\nThus, minimizing the within-class variance was the same as maximizing the between-\nclass variance.\n\nBinarization of the test images was carried out using the Otsu’s local threshold selec-\ntion method. The process has been repeated for all the three color components to gen-\nerate bag of words model (BoW) of features. Conventional BoW model has been based \non SIFT algorithm which has a descriptor dimension of 128 (Zhao et al. 2015). There-\nfore, for three color components the dimension of the descriptor would have been 128 \n× 3 = 384. The size for SIFT descriptor has been huge and it has predestined problem \nfor information losses and omissions as it has been found suitable only for the stability \n\n(4)µ1(t) =\n\nt\n∑\n\ni=0\n\ni ∗ P(i)\n\nq1(t)\n\n(5)µ2(t) =\n\n255\n∑\n\ni=t+1\n\ni ∗ P(i)\n\nq2(t)\n\n(6)σ 2\nb (t) = q1(t)[1− q1(t)][µ1(t)− µ2(t)]\n\n2\n\n \nRed Component Green Component Blue Component \n\n \nBinarization of \n\nRed Component \nBinarzation of \n\nGreen Component \nBinarization of \n\nBlue Component \nFig. 1 Binarization using Otsu’s Threshold selection\n\n\n\nPage 7 of 26Das et al. SpringerPlus (2015) 4:749 \n\nof image feature point extraction and description. Furthermore, the generated SIFT \ndescriptors has to be clustered by k means clustering which has been based on alloca-\ntion of cluster members by means of comparing squared Euclidian distance. The clus-\ntering process has been helpful to generate codewords for codebook generation which \nhas been the final step of BoW. Process of k means clustering has huge computational \noverhead for calculating the squared Euclidian distance which eventually slows down \nthe BoW generation. Hence, in our approach, the grey values higher than the threshold \nwas clustered in higher intensity group and the grey values lower than the cluster was \nclustered in the lower intensity group. The mean of the two groups were calculated to \nformulate the codewords of higher intensity feature vectors and the lower intensity fea-\nture vectors respectively. Thus, each color component of a test image has been mapped \nto two codewords of higher intensity and lower intensity respectively. This has generated \nof codebook of size (3 × 2 = 6) for each image.\n\nThe algorithm for feature extraction has been stated in Algorithm 1 as follows:\n\nAlgorithm 1 \n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Calculate the local threshold value Tx for \neach pixel in each color component R,G and \nB using Otsu's Method.\n\n3. Compute binary image maps for each pixel \nfor the given image.\n\nTxjixif >=),(....1\n\nTxjixif <),(....0\n\n/*x = R, G and B */\n\n4. Generate image features for the given \nimage for each color component.\n\n/*x = R, G and B */\n\nEnd\n\n=),( jiBitmapx\n\nTx\np q\n\nqpxmean\nmean\n\nxhi >== ∑∑ )),((\n\nTx\np q\n\nqpxmean\nmean\n\nxlo <= ∑∑ )),((\n\n\n\nPage 8 of 26Das et al. SpringerPlus (2015) 4:749 \n\nFeature extraction using image transform\n\nTransforms convert spatial information to frequency domain information, where cer-\ntain operations are easier to perform. Energy compaction property of transforms has \nthe capacity to pack large fraction of the average energy into a few components. This \nhas led to faster execution and efficient algorithm design. Image transforms has the \nproperty to convert the spatial domain information of an image to frequency domain \ninformation, where certain operations are easier to perform. For example, convolu-\ntion operation can be reduced to matrix multiplication in frequency domain. It has the \ncharacteristic of energy compaction which ensures that a large fraction of the average \nenergy of the image remains packed into a few components. This property has led to \nfaster execution and efficient algorithm design by drastic reduction of feature vector \nsize which is achieved by means of discarding insignificant transform coefficients as in \nFig. 2. The approach has been implemented by applying slant transform on each of the \nRed (R), Green (G) and Blue (B) color component of the image for extraction of fea-\nture vectors with smaller dimension. Slant transform has reduced the average coding \nof a monochrome image from 8 bits/pixel to 1 bit/pixel without seriously degrading the \nimage quality. It is an orthogonal transform which has also reduced the coding of color \nimages from 24–2 bits/pixel (Pratt et al. 1974). Slant transform matrices are orthogo-\nnal and it holds all real components. Hence, it has much less computational overhead \ncompared to discrete Fourier transform. Slant transform is an unitary transform and \nfollows energy conservation. It tends to pack a large fraction of signal energy into a few \ntransform coefficients which has a significant role in reducing the feature vector for the \nimage. Let [F] be an N × N matrix of pixel values of an image and let [fi] be an N × 1 \nvector representing the ith. column of [F]. One dimensional transform of the ith. image \nline can be given by\n\n [S] = N × N unitary slant matrix.\n\n[fi] = [S][fi]\n\n0.06 % of (N*N) feature vector\n\n0.012% of (N*N) feature vector\n\n50% of (N*N) feature vector\n\nN*N feature vector\n\nFeature Vector Dimension Reduction with Partial Coefficients\n\nFig. 2 Feature extraction by applying image transform\n\n\n\nPage 9 of 26Das et al. SpringerPlus (2015) 4:749 \n\nA two dimensional slant transform can be performed by sequential transformations \nof row and column of [F] and the forward and inverse transform can be expressed as in \nEqs. 7 and 8.\n\nA transform operation can be conveniently represented in a series. The two dimensional \nforward and inverse transform in series form can be represented as in Eqs. 9 and 10\n\nThe algorithm for feature extraction using slant transform has been given in Algo-\nrithm 2.\n\nAlgorithm 2 \n\n(7)[ℑ] = |S|[F ][S]T\n\n(8)[F ] = [S]T [ℑ][S]\n\n(9)ℑ(u, v) =\n\nN\n∑\n\nj=1\n\nN\n∑\n\nk=1\n\nF(j, k)S(u, j)S(k , v)\n\n(10)F\n(\n\nj, k\n)\n\n=\n\nN\n∑\n\nu=1\n\nN\n∑\n\nv=1\n\nℑ(u, v)S\n(\n\nj,u\n)\n\nS(v, k)\n\nBegin\n\n1. Red, Green and Blue color components were \nextracted from a given image.\n\n2. Slant Transform was applied on each of the \ncomponent to extract feature vectors.\n\n3. The extracted feature vectors from each of the \ncomponent were stored as complete set of feature \nvectors.\n\n4. Further, partial coefficients from the entire \nfeature vector set were extracted to form the \nfeature vector database.\n\n5. Feature vector database with 100% transformed \ncoefficients and partial coefficients ranging from \n50% of the complete set of feature vectors till \n0.06% of the complete set of feature vectors were \nconstructed\n\n6. The feature vectors of the query image for the \nwhole set of feature vectors and for partial \ncoefficient of feature vectors were compared with \nthe database images for classification results.\n\n7. The fractional coefficient of feature vector \nhaving the highest classification result was \nconsidered as the feature set extracted by applying \nimage transform\n\nEnd\n\n\n\nPage 10 of 26Das et al. SpringerPlus (2015) 4:749 \n\nHere the features were extracted in the form of visual words. Visual words have been \ndefined as a small patch of image which can carry significant image information. The \nenergy compaction property of Slant transform has condensed noteworthy image infor-\nmation in a block of 12 elements for an image of dimension (256 × 256). Thus, the \nfeature vector extracted with slant transform was of size 12 for each color component \nwhich has given the dimension of feature vector as 36 (12 ×  3 =  36) for three color \ncomponents in each test image.\n\nFeature extraction with morphological operator\n\nHuman perception has largely been governed by shape context. It has been helpful to \nrecover the point correspondences from an image which has considerable contribution \nin feature vector formation. A variant of gray scale opening and closing operations has \nbeen termed as the top-hat transformation that has been instrumental in producing only \nthe bright peaks of an image (Sridhar 2011). It has been termed as the peak detector and \nits working process has been given as follows:\n\n1. Apply the gray scale opening operation to an image.\n2. Peak = original image—opened image.\n3. Display the peak.\n4. Exit.\n\nThe top-hat transform technique was applied on each color component Red (R), \nGreen (G) and Blue (B) of the test images for feature extraction using morphologi-\ncal operator as in Fig. 3. After applying the tophat operator, the pixels designated as \nthe foreground pixels were grouped in one cluster and were calculated with mean and \nstandard deviation to formulate the higher intensity feature vector. Similar process \nwas followed with the pixels designated as the background pixels to calculate the lower \nintensity feature vector. The feature vector extraction process has followed the bag of \nwords (BoW) methodology which has generated codewords from the cluster of fore-\nground and background pixels by calculating the mean and the standard deviation of \nboth the clusters and adding the two. Hence, codebook size for each color component \nwas two which have yielded a dimension of 6 (3 × 2 = 6) on the whole for the code-\nbook generated for three color components for each test image.\n\nThe algorithm for feature extraction using morphological operator has been given in \nAlgorithm 3.\n\n\n\nPage 11 of 26Das et al. SpringerPlus (2015) 4:749 \n\nAlgorithm 3 \n\nSimilarity measures\n\nDetermination of image similarity measures was performed by evaluating distance \nbetween set of image features. Higher similarity has been characterized by shorter dis-\ntance (Dunham 2009). A fusion based classifier, an artificial neural network (ANN) clas-\nsifier and a support vector machine (SVM) classifier was used for the purpose. Each of \nthe classifier types has been discussed in the following sections:\n\nBegin\n\n1. Input an image I with three different color \ncomponents R, G and B respectively of size \nm*n each. \n\n2. Apply tophat transform on each color \ncomponent\n\n3. Cluster the foreground and background \npixels obtained after the morphological \noperation \n\n4. Generate image features xhiF.V. and xloF.V.\nfor the given image for each color \ncomponent.\n\n/*x = R, G and B */\n\nEnd\n\n∑∑=\np q\n\nqp\nforeground\n\nxmean\nmean\n\nxhi )),((\n\n∑∑=\np q\n\nqp\nforeground\n\nx\nstdev\n\nxhi )),((σ\n\n( )\nstdev\n\nxhi\nmean\n\nxhimeanxhi\nVF\n\nxhi += +\n..\n\n∑∑=\np q\n\nqp\nbackground\n\nxmean\nmean\n\nxlo )),((\n\n∑∑=\np q\n\nqp\nbackground\n\nx\nstdev\n\nxlo )),((σ\n\n( )\nstdev\n\nxlo\nmean\n\nxlomeanxlo\nVF\n\nxlo += +\n..\n\n\n\nPage 12 of 26Das et al. SpringerPlus (2015) 4:749 \n\nFusion based classifier\n\nThree different distance measures, namely, city block distance, Euclidian distance and \nmean squared error (MSE) distance metric was considered to compute the distance \nbetween query image Q and database image T as in Eqs. 11, 12 and 13\n\nwhere, Qi is the query image and Di is the database image.\nData standardization technique was followed to standardize the calculated distances \n\nfor the individual techniques with Z score normalization which was based on mean and \nstandard deviation of the computed values as in Eq. 14. The normalization process has \nbeen implemented to avoid dependence of the classification decision on a feature vec-\ntor with higher values of attributes which have the possibilities to have greater effect or \n“weight.” The process has normalized the data within a common range such as [−1, 1] or \n[0.0, 1.0].\n\nwhere, µ is the mean and σ is the standard deviation.\n\n(11)Dcityblock =\n\nn\n∑\n\ni−1\n\n|Qi − Di|\n\n(12)Deuclidian =\n\n√\n\n√\n\n√\n\n√\n\nn\n∑\n\ni=1\n\n(Qi − Di)2\n\n(13)DMSE =\n1\n\nn\n\nn\n∑\n\ni=1\n\n(Qi − Di)\n2\n\n(14)distn =\ndisti − µ\n\nσ\n\n \nRed Component Green Component Blue Component \n\n \nApplying Top-Hat \noperator on Red \n\nComponent \n\nApplying Top-Hat \noperator on Green \n\nComponent \n\nApplying Top-Hat \noperator on Blue \n\nComponent \nFig. 3 Effect of applying morphological operator\n\n\n\nPage 13 of 26Das et al. SpringerPlus (2015) 4:749 \n\nFurther, the final distance was calculated by adding the weighted sum of individual \ndistances. The weights were calculated from the precision values of corresponding tech-\nniques. Finally, the image was classified based on the class majority of k nearest neigh-\nbors [Sridhar 2011] where value of k was\n\nThe classified image was forwarded for retrieval purpose. The image was a classified \nquery and has searched for similar images only within the class of interest. Ranking of \nthe images was done with Canberra Distance measure as in Eq. 15 and top 20 images \nwere retrieved.\n\nwhere, Qi is the query image and Di is the database image.\nThe process of fusion based classification and then retrieval with classified query has \n\nbeen illustrated in Fig. 4.\n\nArtificial neural network (ANN) classifier\n\nThe set of input features from images were mapped to an appropriate output by a feed \nforward Neural Network Classifier known as Multilayer Perceptron (MLP) as shown in \nFig. 5 (Alsmadi et al. 2009).\n\nThe back propagation technique of multi layer perceptron has a significant role in \nsupervised learning procedure. The network has been trained for optimization of clas-\nsification performance by using the procedure of back propagation. For each training \ntuple, the weights were modified so as to minimize the mean squared error between the \nnetwork prediction and the target value. These modifications have been made in the \nbackward direction through each hidden layer down to the first hidden layer. The input \nfeature vectors have been fed to the input units which comprised the input layer. The \nnumber of input units has been dependent on the summation of the number of attrib-\nutes in the feature vector dataset and the bias node. The subsequent layer has been the \nhidden layer whose number of nodes has to be determined by considering the half of the \nsummation of the number of classes and the number of attributes per class. The inputs \nthat have passed the input layer have to be weighted and fed simultaneously to the hid-\nden layer for further processing. Weighted output of the hidden layer was used as input \nto the final layer which has been named as the output layer. The number of units in the \noutput layer has been denoted by the number of class labels. The feed forward property \nof this architecture does not allow the weights to cycle back to the input units.\n\nSupport vector machine (SVM) classifier\n\nSVM transforms original training data to higher dimension by using nonlinear mapping. \nOptimal separating hyperplane has to be searched by the algorithm within this new \ndimension. Data from two different classes can readily be separated by a hyperplane by \nmeans of an appropriate nonlinear mapping to a sufficiently high dimension as in Fig. 6.\n\nk ≤\n√\n\nnumber..of ..training ..ins tan ces.\n\n(15)Dcanberra =\n\nn\n∑\n\ni=1\n\n|Qi − Di|\n\n|Qi| + |Di|\n\n\n\nPage 14 of 26Das et al. SpringerPlus (2015) 4:749 \n\n No \n\nYes \n\nRetrieve top 20 images \n\nFeature \nExtraction \n\nwith \nBinarization \n\nFeature \nExtraction \nwith partial \nTransform \ncoefficients \n\nFeature \nExtraction \n\nwith \nMorphological \n\nOperator \n\nRank Images \nby City-Block \nDistance \n\nRank Images \nby Euclidian \nDistance \n\nRank Images \nby MSE \nDistance\n\nFuse the distances by Z score Normalization\n\nRank the images using fused distance\n\n Rank the images using fused distance\n\nClassify the query based on the class majority of k nearest neighbors\n\n Forward the classified query for retrieval from the class of interest\n\nRetrieve top 20 images\n\n Qi Di \n\nClassify \nquery? \n\nFig. 4 Fusion technique for image identification\n\nInput layer Hidden Layer Output Layer \n\nw1j\n\n w2j wjk\n\n Oj\nwij\n\n Ok\n\n wnj\n\nOj= Output value for hidden layer \nOk= Output Value for output layer \n\nx\n\nx\n\nx\n\nx\n\nFig. 5 Multilayer perceptron (MLP)\n\n\n\nPage 15 of 26Das et al. SpringerPlus (2015) 4:749 \n\nSVM has searched for the maximum separating hyperplane as shown in Fig.  6. The \nsupport vectors have been shown with thicker borders.\n\nThe algorithm was implemented using sequential minimal optimization (SMO) \n(Keerthi et al. 2001). The operating principle of SMO has been to select two Lagrange \nmultipliers as the multipliers must obey a linear equality constraint. The two selected \nLagrange multipliers jointly optimize to find the optimal value for these multipliers and \nupdates the SVM to reflect the new optimal values.\n\nDatasets used\n\nFour different datasets namely Wang dataset, Oliva and Torralba (OT-Scene) dataset, \nCorel dataset and Caltech Dataset was used for the content based image recognition \npurpose. Each of the datasets has been described in the following subsections.\n\nWang’s dataset\n\nIt consists of 10 different categories of 1000 images and was provided by Li and Wang \n(2003). Each image is of dimension 256 × 384 or 384 × 256 and each category com-\nprises of 100 images. The different classes in this dataset are Tribals, Sea Beaches, Gothic \nStructures, Buses, Dinosaur, Elephants, Flowers, Horses, Mountains and Food. A sample \ncollage for Wang’s dataset has been given in Fig. 7.\n\nOliva and torralba (OT‑Scene) dataset\n\nThis dataset comprises of 2688 images and is divided into eight different categories. The \ndataset is provided by MIT (Walia and Pal 2014). The different categories in the dataset \nare Coast and Beach (with 360 images), Open Country (with 328 images), Forest (with \n260 images), Mountain (with 308 images), Highway (with 324 images), Street (with 410 \nimages), City Centre (with 292 images) and Tall Building (with 306 images). A sample \ncollage for OT Scene dataset is given in Fig. 8.\n\nA2\n\nA1\n\nClass 1 Class 2\n\nFig. 6 Structure of hyperplane in SVM\n\n\n\nPage 16 of 26Das et al. SpringerPlus (2015) 4:749 \n\nCorel dataset\n\nThe dataset comprised of 10,800 images (Liu 2013). It has 80 different categories of \nimages of dimension 80 × 120 or 120 × 80. Some of the categories are art, antique, \ncyber, dinosaur, mural, castle, lights, modern, culture, drinks, feast, fitness, dolls, avia-\ntion, balloons, bob, bonsai, bus, car, cards, decoys, dish, door, easter eggs, faces etc. A \nsample collage of the Corel dataset is given in Fig. 9. The research work has used 2500 \nimages of different categories from this dataset.\n\nCaltech dataset\n\nThe dataset includes 8127 images divided into 100 different categories (Walia and Pal \n2014). Each of the categories has different number of images with a dimension of 300 x \n200. Some of the categories are accordion, airplanes, anchor, ant, Background google, \nbarrel, bass, beaver, binocular, bonsai, brain, brontosaurus, buddha, butterfly, camera, \ncannon, car side, ceiling fan, cellphone, chair etc. A sample collage for the Caltech data-\nset has been given in Fig. 10. The research work has used 2533 images of different cat-\negories from the dataset.\n\nFig. 7 Sample collage for wang dataset\n\nFig. 8 Sample collage for OT-scene dataset\n\n\n\nPage 17 of 26Das et al. SpringerPlus (2015) 4:749 \n\nResults and discussions\nThe experiments were executed with Matlab version 7.11.0 (R2010b) on Intel core i5 \nprocessor with 4 GB RAM under Microsoft Windows environment. Initially the misclas-\nsification rate (MR) and F1 Score for classification with fractional coefficients of slant \ntransform were compared to each other to identify the fractional coefficient with high-\nest classification value and lowest MR. Wang dataset was used for the purpose. Further, \nthe precision and recall values for classification were determined on four different pub-\nlic datasets namely, Wang dataset, OT scene dataset Caltech dataset and Corel dataset. \nHenceforth, precision and recall values of the fused architecture for classification were \ncompared against state-of-the art techniques. The precision, recall misclassification rate \n(MR) and F1 Score were represented by Eqs. 16, 17, 18 and 19.\n\n(16)Pr ecision =\nTP\n\nTP + FP\n\n(17)TPRate/Re call =\nTP\n\nTP + FN\n\n(18)MR =\nFP + FN\n\nTP + TN + FP + FN\n\nFig. 9 Sample collage for corel dataset\n\nFig. 10 Sample collage for caltech dataset\n\n\n\nPage 18 of 26Das et al. SpringerPlus (2015) 4:749 \n\nTrue Positive (TP) = Number of instances classified correctly. True Negative (TN) = Num-\nber of negative results created for negative instances False Positive (FP) = Number of erro-\nneous results as positive results for negative instances False Negative (FN) = Number of \nerroneous results as negative results for positive instances.\n\nComparison of MR and F1 Score for classification with different fractional coefficients \nof slant transform has been shown in Fig. 11.\n\nIt was observed that classification with 0.024  % of the transform coefficient has the \nhighest F1 Score and lowest MR compared to the rest. Hence, it was considered as the \nfeature vector with a dimension of 36.\n\nFurther, the precision and recall values of four public datasets have been shown in \nTable 1.\n\nHenceforth, Wang dataset was considered in order to carry out classification using \nfusion technique. The classification decision obtained for Wang dataset using three dif-\nferent feature extraction techniques were fused by means of Z score normalization and \nwere compared to classification results obtained by classifying individual techniques by \n\n(19)F1score =\n2 ∗ Pr ecision ∗ Re call\n\nPr ecision+ Re call\n\nF1 Score MR\n100% feature size 0.478 0.103\n50% of feature size 0.48 0.095\n25% of feature size 0.487 0.09\n12.5% of feature size 0.489 0.09\n6.25% of feature size 0.501 0.09\n3.125% of feature size 0.528 0.089\n1.5625% of feature\n\nsize 0.53 0.089\n\n0.7813% of feature\nsize 0.532 0.088\n\n0.39% of feature size 0.536 0.088\n0.195% of feature size 0.538 0.087\n0.097% of feature size 0.539 0.087\n0.048% of feature size 0.539 0.087\n0.024% of feature size 0.54 0.086\n0.012% of feature size 0.536 0.088\n0.006% of feature size 0.534 0.089\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\nV\nal\n\nue\ns \n\nComparison of MR and F1 Score for \nFractional Coeffiecnts of Slant Transform \n\nFig. 11 Comparison of MR and F1 score for partial coefficients of slant transform\n\n\n\nPage 19 of 26Das et al. SpringerPlus (2015) 4:749 \n\nmeans of artificial neural network (ANN) classifier and support vector machine (SVM) \nclassifier respectively. The comparisons have been shown in Fig. 12.\n\nThe comparison in Fig.  12 has clearly revealed that fusion based classification has \nshown an enhanced precision of 0.12, 0.13 and 0.067 compared to classification with \nANN classifier for feature extraction with image binarization, partial transform coef-\nficients and morphological operator respectively. The recall rate for classification with \nfusion based classification was also higher by 0.134, 0.141 and 0.08 in comparison to \nclassification with ANN classifier for feature extraction with three above mentioned \ntechniques.\n\nThe fusion based classifier has revealed an improved precision rate of 0.221, 0.204 \nand 0.118 in comparison to classification with SVM classifier for feature extraction with \nimage binarization, partial transform coefficient and morphological operator respec-\ntively as in Fig. 13. The recall value for classification with fusion based classifier was also \nhigher by 0.224, 0.21 and 0.136 compared to SVM classifier which is seen in Fig. 13.\n\nTable 1 Precision and recall values for four public datasets using three feature extraction \ntechniques\n\nFeature extraction with  \nbinarization\n\nFeature extraction with fractional \ncoefficients of slant transform\n\nFeature extraction with  \nmorphological operator\n\nWang OT scene Caltech Corel Wang OT scene Caltech Corel Wang OT scene Caltech Corel\n\nPrecision 0.609 0.41 0.49 0.534 0.555 0.449 0.454 0.527 0.728 0.607 0.523 0.711\n\nRecall 0.604 0.4 0.543 0.519 0.563 0.407 0.523 0.533 0.725 0.597 0.597 0.697\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nANN Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.628 0.631\n\nANN Classifier\n(Feature Extraction\n\nwith Partial Transform\nCoefficient)\n\n0.627 0.624\n\nANN Classifier\n(Feature Extraction\nwith morphological\n\noperator)\n\n0.681 0.685\n\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Classification Results of \nFusion Based Classifier and Artifical Neural \n\nNetwork (ANN) Classifier \n\nFig. 12 Comparison of classification with fusion based classifier and ANN classifier\n\n\n\nPage 20 of 26Das et al. SpringerPlus (2015) 4:749 \n\nFurther, the fusion based classification results were compared to existing techniques in \nFig. 14.\n\nIt was observed that the proposed method has outclassed the existing techniques. It \nhas an increased precision rate of 0.012, 0.108, 0.109, 0.178 and 0.228 and an enhanced \nrecall rate of 0.037, 0.125, 0.126, 0.195 and 0.245 compared to the existing techniques, \nnamely, (Thepade et  al. 2014b; Yanli and Zhenxing 2012; Ramírez-Ortegón and Rojas \n2010; Liu 2013; Shaikh et al. 2013) respectively as in Fig. 14. The proposed fusion tech-\nnique was observed to have the maximum precision and recall values compared to the \nrecent techniques cited in the literature.\n\nHenceforth, content based image retrieval was carried out with individual tech-\nniques of feature extraction and was compared to fusion based technique of retrieval in \nFig. 15. The fusion based retrieval technique comprised of classification as a precursor of \nretrieval. Comparison of fusion techniques with classified query and without classified \nquery has been shown in Fig. 16 by using a sample query.\n\nThe figure has clearly divulged that fusion technique of retrieval with classified query \nhas fetched all the images of the same category to that of the query image, whereas, \nretrieval with generic or unclassified query has three images from classes other than the \nclass of query in position 2, 15 and 19 respectively.\n\nA comparison of retrieval with individual techniques of feature extraction and fusion \nbased retrieval with classified query has been given in Fig. 15.\n\nPrecision Recall\nFusion Based\n\nClassifier 0.748 0.765\n\nSVM Classifier\n(Feature Extraction\n\nwith Image\nBinarization)\n\n0.527 0.541\n\nSVM Classifier\n(Feature Extraction\n\nwith Partial\nTransform Coefficient)\n\n0.544 0.555\n\nSVM Classifier\n(Feature Extraction\nwith morphological\n\noperator)\n\n0.63 0.629\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nVa\nlu\n\nes\n \n\nComparison of Classification Results of \nFusion Based Classifier and Support Vector \n\nMachine (SVM) Classifier \n\nFig. 13 Comparison of classification with fusion based classifier and SVM classifier\n\n\n\nPage 21 of 26Das et al. SpringerPlus (2015) 4:749 \n\nResults in Fig. 15 have shown an increase of 26.3, 34.5 and 19.5 % in precision values \nand enhancement of 5.26, 6.9 and 3.9  % in recall values for the fusion based retrieval \ntechnique with classified query in comparison to retrieval with individual feature extrac-\ntion techniques. It was clearly established that the fusion based technique has outper-\nformed the individual techniques.\n\nFurther, a paired t test was conducted to validate the statistical findings and a null \nhypothesis was formulated in Hypothesis 1 (Yıldız et al. 2011).\n\nHypothesis 1: There is no significant difference among the Precision values of fusion \nbased retrieval with classified query with respect to individual retrieval techniques\n\nThe p values for the paired t test have been enlisted in Table 2. The precision value \nof fusion based retrieval with classified query was compared to that of the individual \nretrieval techniques to obtain the computed values in Table 2.\n\nThe p values have clearly indicated significant difference in precision values of the fusion \nbased retrieval technique with classified query compared to the existing techniques of \nretrieval. Hence, the null hypothesis was rejected and the proposed fusion technique with \nclassified query has been found to boost the precision values with statistical significance.\n\nFinally, the precision and recall values of the proposed fusion technique were com-\npared to existing fusion based retrieval techniques. The results have been displayed in \nFig. 17.\n\nPrecision Recall\nProposed Fusion\n\nTechnique 0.748 0.765\n\nThepade et al. (2014b) 0.736 0.728\n(Yanli Y. and\n\nZhenxing Z., 2012) 0.64 0.64\n\n(Ramírez-Ortegón, \nM.A. And Rojas R., \n\n2010) \n0.63 0.63\n\n(Liu.C, 2013) 0.57 0.57\n(Shaikh, 2013) 0.52 0.52\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall values \nfor Classification \n\nFig. 14 Comparison of classification results of proposed technique with respect to existing techniques\n\n\n\nPage 22 of 26Das et al. SpringerPlus (2015) 4:749 \n\nPrecision Recall\nRetrieval with feature\n\nextraction with\nBinarization\n\n49.7 9.94\n\nRetrieval with feature\nextraction with\n\nFractional Coefficient\nof Slant Transform\n\n41.5 8.3\n\nRetrieval with feature\nextraction with\nMorphological\n\nOperator\n\n56.5 11.3\n\nFusion Based retrieval\nwith classified query 76 15.2\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Precision and Recall for \nFusion Based Retrieval and Individual \n\nRetrieval Technique \n\nFig. 15 Comparison of precision and recall with fusion based retrieval technique and individual retrieval \ntechnique\n\nRetrieval with \nclassified query \n\nRetrieval with generic \nquery \n\nFig. 16 Comparison of fusion based retrieval with classified and generic query\n\n\n\nPage 23 of 26Das et al. SpringerPlus (2015) 4:749 \n\nThe comparison in Fig.  17 has clearly established the superiority of the proposed \nfusion based retrieval technique with respect to existing fusion based technique of \nretrieval. The proposed retrieval technique has improved precision of 1.98, 3.2, 3.3, 3.49, \n17.8, 21.1 and 26.31 % and superior recall of 0.4, 0.64, 0.66, 0.7, 3.56, 4.22 and 5.26 % \ncompared to the existing fusion based techniques mentioned in Fig. 13.\n\nHenceforth, the proposed method was compared to the semantic retrieval techniques \nin Fig. 18.\n\nTable 2 Statistical validation with paired t test\n\np value Significance\n\nRetrieval by feature extraction with image transform 0.0013 Significant\n\nRetrieval by feature extraction with image binarization 0.0076 Significant\n\nRetrieval by feature extraction with morphological operator 0.0452 Significant\n\n \n \n\n \n\nPrecision Recall\nProposed 76 15.2\nSubrahmanyam et al.\n\n2013 74.02 14.80\n\nShen & Wu (2013) 72.8 14.56\nBanerjee et al. (2009) 72.7 14.54\nSubrahmanyam et al.\n\n2012 72.51 14.50\n\nJalab (2011) 58.2 11.64\nHiremath & Pujari\n\n(2007) 54.9 10.98\n\nRahimi & Moghaddam\n(2013) 49.69 9.94\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Retrieval Performance with \nFusion based Techniques \n\nFig. 17 Comparison of retrieval with the proposed technique compared to state-of-the art fusion tech-\nniques\n\n\n\nPage 24 of 26Das et al. SpringerPlus (2015) 4:749 \n\nThe comparison shown in Fig. 18 has revealed an enhanced precision rate of 0.2, 0.5 \nand 2.1 % and increased recall rate of 0.04, 0.1 and 0.6 % respectively for the proposed \nmethod with respect to the existing semantic retrieval techniques.\n\nTherefore, the research work has fulfilled the following objectives:\n\n • It has reduced the dimension of feature vectors.\n • It has successfully implemented fusion based method of content based image identi-\n\nfication.\n • The research results have shown statistical significance.\n • The research results have outperformed the results of state-of-the art techniques.\n\nConclusions\nIn depth analysis of feature extraction techniques have been exercised in this research \nwork. Three different techniques of feature extraction comprising of image binariza-\ntion, fractional coefficients of image transforms and morphological operations has been \nimplemented to extract features from the images. The extracted features with multiple \ntechniques were used for fusion based identification process. The proposed method of \nfusion has divulged statistical significance with respect to the individual techniques. \nThe retrieval technique was implemented with classification as a precursor. The classi-\nfication technique was used to classify the query image for retrieval. The method has \n\nPrecision Recall\nProposed 76 15.2\nWalia et al. (2014) 75.8 15.16\nIrtaza et al. (2013) 75.5 15.10\nAlami (2011) 73.9 14.78\n\n0\n\n10\n\n20\n\n30\n\n40\n\n50\n\n60\n\n70\n\n80\n\nV\nal\n\nue\ns \n\nComparison of Proposed Technique with \nSemantic Retrieval Technique \n\nFig. 18 Comparison of retrieval with the proposed technique to semantic retrieval techniques\n\n\n\nPage 25 of 26Das et al. SpringerPlus (2015) 4:749 \n\nshown better performance compared to generic query based method of retrieval. Thus, \nthe importance of classification was established in limiting the computational overhead \nfor content based image identification. Finally, image identification with the proposed \ntechnique has surpassed the state-of-the art methods for content based image recogni-\ntion. The work may be extended towards content based image recognition in the field of \nmilitary, media, medical science, journalism, e commerce and many more.\nAuthor’s contributions\nRD and ST have designed the feature extraction techniques and the classification and retrieval techniques. RD and SG \nhave planned the statistical test and conclusion. RD wrote the manuscript. All the authors have read and approved the \nfinal manuscript.\n\nAuthor details\n1 Department of Information Technology, Xavier Institute of Social Service, Dr. Camil Bulcke Path (Purulia Road), P.O. \nBox 7, Ranchi 834001, Jharkhand, India. 2 Pimpri Chinchwad College of Engineering, Akrudi, Sec-26,Pradhikaran, Nigdi, \nPune 411033, Maharashtra, India. 3 A.K. Choudhury School of Information Technology, University of Calcutta, 92, APC \nRoad, Kolkata 700009, West Bengal, India. \n\nAcknowledgements\nThe authors acknowledge Late Dr. H.B. Kekre for encouraging the experimental process. The authors also acknowledge \nDr. Rohit Vishal Kumar and Dr. Subhajit Bhattacharya for explaining the statistical techniques.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nReceived: 12 September 2015 Accepted: 5 November 2015\n\nReferences\nAlsmadi MK, Omar KB, Noah SA, Almarashdah I (2009) Performance comparison of multi-layer perceptron (Back Propaga-\n\ntion, Delta Rule and Perceptron) algorithms in neural networks. 2009 IEEE International Advance Computing Confer-\nence, IACC 2009, 7: pp 296–299\n\nAnnadurai S, Shanmugalakshmi R (2011) Image transforms, fundamentals of digital image processing. Dorling Kindersley \n(India) Pvt. Ltd., pp 31–66\n\nBanerjee M, Kundu MK, Maji P (2009) Content- based image retrieval using visually significant point features. Fuzzy Sets \nSyst 160:3323–3341\n\nDouze M, Jégou H, Singh H, Amsaleg L, Schmid C (2009) Evaluation of GIST descriptors for web-scale image search. In \nACM International Conference on Image and Video Retrieval, pp 0–7\n\nDubois SR, Glanz FH (1986) An autoregressive model approach to two-dimensional shape classification. IEEE Trans Pat-\ntern Anal Mach Intell 8(1):55–66\n\nDunham MH (2009) Data Mining Introductory and Advanced Topics: Pearson Education, p 127\nElAlami ME (2011) A novel image retrieval model based on the most relevant features. Knowl-Based Syst 24:23–32\nElAlami ME (2014) A new matching strategy for content based image retrieval system. Appl Soft Comput J 14:407–418\nFlickner M, Sawhney H, Niblack W, Ashley J, Huang Q, Dom B, Gorkani M et al (1995) Query by image and video content: \n\nthe QBIC system. Computer 28(9):23–32 IEEE\nGevers T, Smeulders AW (2000) PicToSeek: combining color and shape invariant features for image retrieval. IEEE Trans \n\nImage Proc Publ IEEE Signal Proc Soc 9(1):102–119\nHiremath PS, Pujari J (2007) Content based image retrieval based on color, texture and shape features using image and \n\nits complement. Int J Computer Sci Secur 1:25–35\nIrtaza A, Jaffar MA, Aleisa E, Choi TS (2013) Embedding neural networks for semantic association in content based image \n\nretrieval. Multimed Tool Appl 72(2):1911–1931\nJalab HA (2011) Image retrieval system based on color layout descriptor and Gabor filters. 2011 IEEE Conference on Open \n\nSystems. pp 32–36\nKeerthi SS, Shevade SK, Bhattacharyya C, Murthy KRK (2001) Improvements to Plattʼs SMO Algorithm for SVM classifier \n\ndesign. Neural Comput 13:637–649\nKekre HB, Thepade S (2009) Improving the performance of image retrieval using partial coefficients of transformed \n\nimage. Int J Inf Retr Ser Publ 2(1):72–79\nKekre HB, Thepade S, Maloo A (2010) Image Retrieval using Fractional Coefficients of Transformed Image using DCT and \n\nWalsh Transform‖. Int J Eng Sci Technol (IJEST) 2(4):362–371\nKekre HB, Thepade S, Das R, Ghosh S (2013) Multilevel block truncation coding with diverse colour spaces for image clas-\n\nsification. In: IEEE-International conference on Advances in Technology and Engineering (ICATE), pp 1–7\nKim WY, Kim YS (2000) Region-based shape descriptor using Zernike moments. Sig Process Image Commun 16:95–102\nLi J, Wang JZ (2003) Automatic linguistic indexing of pictures by a statistical modeling approach. IEEE Trans Pattern Anal \n\nMach Intell 25:1075–1088\nLiu C (2013) A new finger vein feature extraction algorithm, In: IEEE 6th. International Congress on Image and Signal \n\nProcessing (CISP), pp 395–399\n\n\n\nPage 26 of 26Das et al. SpringerPlus (2015) 4:749 \n\nLuo H, Lina Y, Haoliang Y, Yuan YT (2013) Dimension reduction with randomized anisotropic transform for hyperspectral \nimage classification. In: 2013 IEEE International Conference on Cybernetics, CYBCONF 2013, pp 156–161\n\nMadireddy RM, Gottumukkala PSV, Murthy PD, Chittipothula S (2014) A modified shape context method for shape based \nobject retrieval. SpringerPlus 3:674. doi:10.1186/2193-1801-3-674\n\nMehtre BM, Kankanhalli MS, Lee Wing Foon (1997) Shape measures for content based image retrieval: a comparison. Inf \nProcess Manage 33:319–337\n\nMokhtarian F, Mackworth AK (1992) A theory of multiscale, curvature-based shape representation for planar curves. IEEE \nTrans Pattern Anal Mach Intell 14:789–805\n\nOtsu N (1979) A threshold selection method from gray- level histogram IEEE transactions on systems. Man Cybern \n9:62–66\n\nPrakash O, Khare M, Srivastava RK, Khare A (2013) Multiclass image classification using multiscale biorthogonal wavelet \ntransform, In: IEEE Second International Conference on Information Processing (ICIIP), pp 131–135\n\nPratt W, Chen WH, Welch L (1974) Slant transform image coding. IEEE Transactions on Communications 22\nRahimi M and Moghaddam ME (2013) A content based image retrieval system based on color ton distributed descrip-\n\ntors. Signal Image Video Process 9(3):691–704. http://dx.doi.org/10.1007/s11760-013-0506-6\nRamírez-Ortegón MA and Rojas R (2010) Unsupervised evaluation methods based on local gray-intensity variances \n\nfor binarization of historical documents. Proceedings—International Conference on Pattern Recognition, pp \n2029–2032\n\nRaventós A, Quijada R, Torres L, Tarrés F (2015) Automatic summarization of soccer highlights using audio- visual descrip-\ntors. SpringerPlus 4:301. doi:10.1186/s40064-015-1065-9\n\nShaikh SH, Maiti AK, Chaki N (2013) A new image binarization method using iterative partitioning. Mach Vis Appl \n24(2):337–350\n\nShen GL and Wu XJ (2013) Content based image retrieval by combining color texture and CENTRIST, In: IEEE international \nworkshop on signal processing, vol 1, pp 1–4\n\nSridhar S (2011) Image features representation and description digital image processing. India Oxford University Press, \nNew Delhi, pp 483–486\n\nSubrahmanyam M, Maheshwari RP, Balasubramanian R (2012) Expert system design using wavelet and color vocabulary \ntrees for image retrieval. Expert Syst Appl 39:5104–5114\n\nSubrahmanyam M, Wu QMJ, Maheshwari RP, Balasubramanian R (2013) Modified color motif co- occurrence matrix for \nimage indexing and retrieval. Comput Electr Eng 39:762–774\n\nThepade S, Das R, Ghosh S (2013a) Advances in computing, communication and control. Image classification \nusing advanced block truncation coding with ternary image maps, vol 361. Springer, Berlin, pp 500–509. \ndoi:10.1007/978-3-642-36321-4_48\n\nThepade S, Das R, Ghosh S (2013b) Performance comparison of feature vector extraction techniques in RGB color space \nusing block truncation coding or content based image classification with discrete classifiers. In: India Conference \n(INDICON), IEEE, pp 1–6. doi: 10.1109/INDCON.2013.6726053\n\nThepade S, Das R, Ghosh S (2014a) A novel feature extraction technique using binarization of bit planes for content \nbased image classification. J Eng. doi:10.1155/2014/439218\n\nThepade S, Das R, Ghosh S (2014b) Feature extraction with ordered mean values for content based image classification. \nAdv Comput Eng 2014. doi:10.1155/2014/454876\n\nValizadeh M, Armanfard N, Komeili M, Kabir E (2009) A novel hybrid algorithm for binarization of badly illuminated docu-\nment images. 2009 14th International CSI Computer Conference, CSICC 2009, pp 121–126\n\nWalia E, Pal A (2014) Fusion framework for effective color image retrieval. J Vis Commun Image Represent \n25(6):1335–1348\n\nWalia E, Vesal S, Pal A (2014) An Effective and Fast Hybrid Framework for Color Image Retrieval. Sens Imaging 15:93. doi: \n10.1007/s11220-014-0093-9\n\nWang X, Bian W, Tao D (2013) Grassmannian regularized structured multi-view embedding for image classification. IEEE \nTrans Image Process 22(7):2646–2660\n\nYanli Y and Zhenxing Z (2012) A novel local threshold binarization method for QR image, In: IET International Conference \non Automatic Control and Artificial Intelligence (ACAI), pp 224–227\n\nYıldız OT, Aslan O, Alpaydın E (2011) Multivariate statistical tests for comparing classi-fication algorithms. Lect Notes \nComp Sci, vol 6683, Springer, Berlin, pp 1–15\n\nYue J, Li Z, Liu L, Fu Z (2011) Content-based image retrieval using color and texture fused features. Math Comput Model \n54:1121–1127\n\nZhang D, Lu G (2003) A comparative study of curvature scale space and Fourier descriptors for shape- based image \nretrieval. J Vis Commun Image Represent 14:39–57\n\nZhang D, Lu G (2004) Review of shape representation and description techniques. Pattern Recogn 37:1–19\nZhao C, Li X, Cang Y (2015) Bisecting k-means clustering based face recognition using block-based bag of words model. \n\nOptik Int J Light Electron Optics 126(19):1761–1766\nZhu Q, Shyu M-L (2015) sparse linear integration of content and context modalities for semantic concept retrieval. IEEE \n\nTrans Emerg Top Comput 3(2):152–160\n\nhttp://dx.doi.org/10.1186/2193-1801-3-674\nhttp://dx.doi.org/10.1007/s11760-013-0506-6\nhttp://dx.doi.org/10.1186/s40064-015-1065-9\nhttp://dx.doi.org/10.1007/978-3-642-36321-4_48\nhttp://dx.doi.org/10.1109/INDCON.2013.6726053\nhttp://dx.doi.org/10.1155/2014/439218\nhttp://dx.doi.org/10.1155/2014/454876\nhttp://dx.doi.org/10.1007/s11220-014-0093-9\n\n\tMulti technique amalgamation for enhanced information identification with content based image data\n\tAbstract \n\tBackground\n\tFeature extraction using image transform\n\tImage binarization techniques for feature extraction\n\tUse of morphological operators for feature extraction\n\tFusion methodologies and multi technique feature extraction\n\n\tMethods\n\tFeature extraction with image binarization\n\tFeature extraction using image transform\n\tFeature extraction with morphological operator\n\tSimilarity measures\n\tFusion based classifier\n\tArtificial neural network (ANN) classifier\n\tSupport vector machine (SVM) classifier\n\tDatasets used\n\tWang’s dataset\n\tOliva and torralba (OT-Scene) dataset\n\tCorel dataset\n\tCaltech dataset\n\n\tResults and discussions\n\tConclusions\n\tAuthor’s contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwMDY0LTAxNS0xNTE1LTQucGRm0", "metadata_author": "Rik Das", "metadata_title": "Multi technique amalgamation for enhanced information identification with content based image data", "metadata_creation_date": "2015-11-26T05:08:05Z", "keyphrases": [ "content based image data", "Multi technique amalgamation", "information identification", "enhanced" ] }, { "@search.score": 1, "content": "\nComputational Visual Media\nhttps://doi.org/10.1007/s41095-020-0189-1 Vol. 7, No. 2, June 2021, 159–167\n\nReview Article\n\nMachine learning for digital try-on: Challenges and progress\n\nJunbang Liang1 (�), Ming C. Lin1\n\nc© The Author(s) 2020.\n\nAbstract Digital try-on systems for e-commerce have\nthe potential to change people’s lives and provide notable\neconomic benefits. However, their development is limited\nby practical constraints, such as accurate sizing of the\nbody and realism of demonstrations. We enumerate three\nopen challenges remaining for a complete and easy-to-use\ntry-on system that recent advances in machine learning\nmake increasingly tractable. For each, we describe\nthe problem, introduce state-of-the-art approaches, and\nprovide future directions.\n\nKeywords machine learning; digital try-on; garment\nmodeling; human body estimation; material\nmodeling\n\n1 Introduction\nE-commerce has grown at a rapid pace in recent\nyears. Consumers today are more likely to shop\nonline than to visit a retail store. The situation is\nmuch more complicated, however, when it comes to\nbuying clothes. People need to know how a garment\nfits on them, how it looks, and how it feels. Digital\ntry-on systems can potentially satisfy these needs,\nproviding a direct visual impression, and possibly\ncustomized clothes sizing as well. Therefore, it has\ndrawn much attention as an attractive alternative to\nimprove the user experience and popularize online\nfashion shopping.\n\nHowever, the technology is still far from practical,\neasy-to-use, and adequate to replace physical try-on.\nCurrently, most try-on systems rely on either image-\nediting, copy-pasting, or template demonstrations,\n\n1 University of Maryland, College Park, MD 20785, USA.\nE-mail: J. Liang, liangjb@cs.umd.edu (�); M. C. Lin,\nlin@cs.umd.edu.\n\nManuscript received: 2020-06-24; accepted: 2020-07-21\n\nwhile the ultimate goal is a fast and realistic try-on\nsystem adaptive to each customer’s body. There is\nstill a substantial technological gap between modeling\nand demonstrating garment fitting in the digital and\nreal worlds, including fast and realistic demonstration,\naccurate modeling of human body and garments,\nfaithful modeling of garment material, and lossless\ntransformation of garments between virtual and\nphysical worlds.\n\nIn this paper, we present some open research issues\nthat contribute to this technological gap, including:\n1. accurate estimation of human shapes and sizes\n\nusing consumer devices,\n2. faithful recovery of garment materials via (online)\n\nimages, and\n3. ease of design and manipulation of sewing patterns\n\nand garment pieces by end-users.\nAlthough traditional methods have made important\n\nprogress on these under-constrained problems,\nlearning-based approaches have shown tremendous\npotential to make a notable impact. Compared to\ntraditional methods, machine-learning algorithms are\nusually much faster since training and optimization\nare performed offline. They are also good at\ngeneralizing to unseen images without the need for\ntedious data pre-processing. While extensive research\nexists on 2D image learning, machine learning of\nhighly variable 3D human body shapes is still far\nfrom mature, which is the reason why the open issues\ndescribed above remain elusive.\n\nFor each problem listed above, we motivate its\nimportance, provide a problem description, and\npresent state-of-the-art approaches with potential\nfor improvements. We believe that solutions to\nthese challenging problems will lead to significant\nadvances in digital try-on, as well as other areas of\ne-commerce.\n\n159\n\n\n\n160 J. Liang, M. C. Lin\n\n2 Open problems\nIn this section we first introduce three major\nchallenges that limit digital try-on technology from\nbeing widely adopted and accepted by shoppers.\nThere are several reasons why shoppers still prefer\nphysical try-on. Firstly, consumers are unsure if what\nthey buy online will fit them well. Although general\nsizing systems exist, their lack of consistency and\nstandardization across different brands and garment\nmaterials can often make it difficult to size clothes,\nespecially for persons with non-standard body shapes\nand proportion. Accurate estimation of human body\nshape is the key to successful digital try-on. Secondly,\nfabric is usually a key consideration when shopping\nfor clothes. Different fabric affects how garments\nlook and fit, how consumers would wear them, and\nwhether or not they would buy them. However, the\ncorrespondence between the actual material and its\ndigital representation are not well understood. It is\nalso challenging to acquire a full fabric digital model\nfrom real-world examples.\n\nFor the customers, appearance is as critical as other\nfactors. There are two approaches to displaying\ngarments: 2D image-based, and 3D mesh-based\nwith photo-realistic rendering. They have different\nadvantages and drawbacks, but both need a large\ngarment database for support. While creating a 3D\ngarment takes considerable effort, 2D images often\nsuffer from a lack of variation and are much more\ndifficult to customize. In either case, the try-on\nsystem needs a user-friendly design and manipulation\nbackend. Last, but not least, a fast and realistic\nanimation of the garments in motion along with\nbody movements greatly improves the user experience.\nAlthough it is not so critical as other factors, it\nwould effectively reduce the perceptual gap between\nthe real and digital worlds for online shopping.\nPrevious work has proposed using cloud computing\nto improve the animation speed, but there is still a\nnotable technology gap for high-quality, interactive\n3D animation of clothes.\n\n3 Human shape estimation\nAs noted, accurate human shape estimation is key to\nenabling digital try-on. Human body reconstruction,\nconsisting of pose and shape estimation, has been\nwidely studied in a variety of areas, including digital\n\nsurveillance, computer animation, special effects, and\nvirtual and augmented environments. Yet, it remains\na challenging and popular topic of interest. While\ndirect 3D body scanning can provide excellent and\naccurate results, its adoption is somewhat limited by\nthe required specialized hardware. RGB images are\nwidely available for input to digital try-on and can\nbe easily captured using commodity mobile devices.\nAlthough purely image-based try-on methods have\nbeen proposed [1], learning-based 3D body estimation\nis more widely applicable in that the 3D body can be\narticulated and so re-posed and re-targeted.\n\nWe define the human-body reconstruction problem\ninformally as, given one or more RGB images, to\nestimate the human body geometry and size, and\noutput (preferably) a 3D humanoid mesh. Traditional\nalgorithms often formulate it as an optimization\nproblem, in which the silhouette difference is a major\npart of the objective function [2]. Therefore, these\nmethods either require the human to wear tight\nclothes, or alternatively relax the target function\nto be unilateral on uncovered body parts [3], or\nto point correspondences [4]. The use of machine\nlearning methods in this problem has led to significant\nadvances. Firstly, it has moved the algorithm from\nonline to offline, significantly reducing response time.\nSecond, by using a parametric human model [5],\none can easily construct a regression network for\nthe parameters while the losses needed can also be\ninferred from them. While early works proposed\nnetwork models for only 2D/3D body skeletons [6–\n8], more recent works have introduced techniques to\nperform regression for the entire human body—either\nusing a parametric human model [9, 10] or a voxel-\nbased representation [11–13]. As annotations in most\nreal-world datasets contain only joint positions, the\nlearning process has been refined in various ways [14–\n17]. The current state of the art is the recent work\nby Ref. [18] �. It emphasizes shape learning, while\nmany other works often focus on body-joint losses,\nbut neglect the effect of body shapes.\n\nThe key contribution of Ref. [18] is a multi-view,\nmulti-stage framework to address ambiguity caused by\ncamera projection (see Fig. 1). Their model performs\nseveral stages of error correction. Each of the image\ninputs is passed on step by step; at each step, a shared-\n\n� Liang and Lin’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/humanmultiview\n\n\n\nMachine learning for digital try-on: Challenges and progress 161\n\nFig. 1 Network structure from Ref. [18]. By using an iterative value correction structure, visual information from different views is effectively\nintegrated to provide a unified human shape. Reproduced with permission from Ref. [18], c© The Author(s) 2019.\n\nparameter prediction block computes the correction\nbased on the image feature and the input guesses.\nThe camera and the human body parameters are\nestimated at the same time, projecting the predicted\n3D joints back to 2D for loss computation. The\nestimated pose and shape parameters are shared\namong all views, while each view maintains its own\ncamera calibration and global orientation. Their\nproposed framework uses a recurrent structure,\nmaking it a universal model applicable to any number\nof views. At the same time, it couples shareable\ninformation across different views so that the human\nbody pose and shape are optimized using image\nfeatures from all views. Unlike static multi-view\nCNNs which have a fixed number of inputs, they\nmake use of the RNN-like structure in a cyclic form to\naccept any number of views, and prevent the gradient\nvanishing by predicting corrective values instead of\nupdating parameters in each regression block.\n\nExperiments have shown that, after training, this\nmodel can form a single view image, provide equally\ngood pose estimation as the state of the art, and\nprovide considerably improved pose estimation when\nusing multi-view inputs, leading to better shape\nestimation across all datasets. An example is\ndemonstrated in Fig. 2. Moreover, a physically-based\nsynthetic data generation pipeline is introduced to\nenrich the training data, which is very helpful for\n\nshape estimation and regularization in cases that\ntraditional datasets do not capture. While synthetic\ndata improves the diversity of human bodies with\nground-truth parameters, a larger garment dataset\nand a more convenient registration process are needed\nto minimize the performance gap between real-world\nimages and synthetic data. In addition, other\nvariables such as hair, skin color, and 3D backgrounds\nare subtle elements that can influence the perceived\nrealism of the synthetic data at the higher expense of\na more complex data generation pipeline. With the\nrecent progress in image style transfer using GAN, a\n\nFig. 2 Prediction results using the state of the art [18]. The model\ncaptures the shape of the human body by learning from synthetic\ndata. The recovered legs and chest are close to those of the person\nin the image. Reproduced with permission from Ref. [18], c© The\nAuthor(s) 2019.\n\n\n\n162 J. Liang, M. C. Lin\n\npromising direction is to transfer the synthetic result\nto more realistic images to further improve the result.\n\n4 Garment material modeling\n4.1 Introduction\nGarment material plays an important role in digital\ntry-on systems. Physical recreation of the fabric not\nonly gives a compelling visual simulation of the cloth,\nbut also affects how the garment feels and fits on\nthe body. However, fabric modeling is a challenging\ntask: the appearance and physical properties of\nthe garment are determined not only by the type\nof materials the clothes are made of, but also by\nsewing and weave. Thus, researchers often focus on\nthe physical behaviour, rather than the underlying\nsemantic primitives.\n\nHence, we state the garment material modeling\nproblem as follows. Given a sufficient amount of data,\nmodel the material’s physical behavior and physical\nproperties, so that visual effects the same as or similar\nto those of the real material can be reproduced by a\ncomputer. This has two implications: firstly, we need\nto define a physical model of the material, and then\nwe must estimate the parameters in the model.\n\nThere are many ways to model clothes, including\nspring–mass systems and finite elements. The latter is\nthe most popular model since it can produce realistic\nresults. While one can use isotropic properties such\nas Young’s modulus and Poisson ratio, an anisotropic\nmodel is a better choice since it can support different\nbehaviors caused by the weave of the material.\n\n4.2 Learning-based estimation\nWhile traditional optimization methods [19] often\ntake a long time to compute material parameters,\nmachine-learning methods can make predictions in\nreal time by a simple feed-forward operation, which\nis more useful in applications that need fast feedback,\nsuch as garment prototyping. The state-of-the-art\nmodel from Yang et al. [20] � uses CNNs combined\nwith LSTM to recover material parameters from\nvideos. To constrain both the input and solution\nspace, they choose one of the materials as a basis;\nthe material sub-space is constructed by multiplying\nthis material basis with a positive coefficient. To\nconstruct an optimal material parameter sub-space, a\n\n� Yang et al.’s data and code are available at http://gamma.cs.unc.edu/\nVideoCloth\n\nmaterial parameter sensitivity analysis is conducted\nto examine the sensitivity of the material parameters\nκ with respect to the amount of deformation D(κ).\nPhysically based cloth simulations are used to\ngenerate a much larger number of data samples within\nthese sub-spaces, which would otherwise be difficult\nor time-consuming to capture. The cloth meshes are\ngenerated through physically-based simulation, and\nthen rendered as 2D images with a randomly assigned\ntexture. Using the data samples, they combine the\nimage signal feature extraction method, a CNN, with\nthe temporal sequence learning method, LSTM, to\nlearn the mapping from visual appearance to material.\nAs shown in Fig. 3, the CNN layer is used to extract\nboth low- and high-level visual features, while the\nLSTM layer focuses on learning the mapping between\nthe material properties of the cloth and its consequent\nmovement.\n\nThey demonstrated the proposed framework with\nthe application of “material cloning”. With the\ntrained deep neural network model being able to\ncapture the cloth motion (Fig. 4), the material type\ncan be inferred from a video recording of the motion\nof the cloth in a fairly small amount of time. The\nrecovered material type can be “cloned” onto another\npiece of cloth or garment as shown in Fig. 5.\n\nIn this work, the videos contain only a single piece\nof cloth which does not interact with any other object.\nWhile this is not applicable to all real-world scenarios,\nthis method provides new insights into addressing this\nchallenging problem. A natural extension would be to\nlearn from videos of clothing directly interacting with\nthe human body, under varying lighting conditions\nand partial occlusion.\n\n4.3 Optimization using differentiable physics\nAnother approach to modeling the fabric is to measure\ngeometric differences directly during parameter\n\nFig. 3 Network model from Ref. [20]. The material is modeled\nby learning motion patterns of image features given by CNNs.\nReproduced with permission from Ref. [20], c© The Author(s) 2017.\n\n\n\nMachine learning for digital try-on: Challenges and progress 163\n\nFig. 4 Learned CNN conv5-layer activation visualization from\nRef. [20]. Experiments show that the trained model is able to capture\nmoving parts of the cloth even in an unseen video. Reproduced with\npermission from Ref. [20], c© The Author(s) 2017.\n\nFig. 5 Yang et al. [20] modeled clothes materials in input videos (left),\nand applied those materials to a simulated skirt (right). Reproduced\nwith permission from Ref. [20], c© The Author(s) 2017.\n\noptimization. Assuming that the environment is\nknown to the system, computation of the estimated\nmotion and its gradient with respect to the material\nparameters can be achieved using differentiable\nsimulation. A typical usage of differentiable\nsimulation is motion control (see Fig. 6), where the\ndifference to the target is measured and the loss\nbackpropagated to the network. Similar processes\ncan be applied to material parameter estimation as\nwell. By measuring the distance to the target as the\nloss and computing corresponding gradients, either in\npixel space or in 3D space, the material parameters\n\ncan be learned or optimized to achieve the desired\ncloth motion or visual effect. Recent differentiable\nphysics work covers rigid bodies [22, 23], cloth [24],\nand particle-grid systems [25, 26]. The state-of-\nthe-art is Ref. [24] �, which proposes a method for\ndifferentiable cloth simulation. It is the first work\nto tackle a high dimensional simulation problem\nand to propose a general differentiable collision\nhandling algorithm. Later, a follow-up work [21]\nextended the algorithm to be applicable to coupled\ndynamics with rigid bodies. Overall, they follow\nthe computational flow of the common approach\nto cloth simulation: discretization using the finite\nelement method, integration using an implicit Euler\nmethod, and collision response on impact zones. They\nuse implicit differentiation in the linear solver and\noptimization in order to compute the gradient with\nrespect to the input parameters. The discontinuity\nintroduced by collision response is negligible because\nthe discontinuous states constitute a zero-measure\nset. During backpropagation in the optimization,\ngradient values can be directly computed after QR\ndecomposition of the constraint matrix. Their\npipeline contains several techniques that can be\nemployed in other differentiable simulations.\n4.3.1 Derivatives of the physical solution\nIn modern simulation algorithms, an implicit Euler\nmethod is often used for stable integration results.\nThus the mass matrix M often includes the Jacobian\nof the forces, and is denoted as M̂ to indicate this\ndifference. A linear solver is needed to compute the\nacceleration since it is time-consuming to compute\nM̂−1. Implicit differentiation is used to compute the\ngradients of the linear solution. Given an equation\nM̂a = f with a solution z and propagated gradient\n∂L/∂a|a=z, where L is the task-specific loss function,\nimplicit differentiation is used to derive the gradients.\nWe refer readers to the original paper [24] for more\ndetails.\n4.3.2 Derivatives of the collision response\nA general approach using LCP to integrate collision\nconstraints into physics simulations has been\nproposed, but constructing a static LCP is often\nimpractical in cloth simulation due to the high\ndimensionality. Collisions and contacts which happen\nat each step are very sparse compared to the complete\n\n� Liang et al.’s data and code are available at https://gamma.umd.edu/\nresearchdirections/virtualtryon/differentiablecloth\n\n\n\n164 J. Liang, M. C. Lin\n\nFig. 6 Differentiable simulation embedding example from Ref. [21]. The loss can be backpropagated through the physics simulator to the\nneural network, enabling learning tasks such as material modeling and motion control.\n\ndata. Therefore, a dynamic approach is used that\nincorporates collision detection and response.\n\nCollision handling in their implementation is based\non impact zone optimization. It finds all colliding\ninstances using continuous collision detection and\nsets up the constraints for all collisions. In order\nto introduce minimum change to the original mesh\nstate, a QP problem is developed to determine the\nconstraints. Since the signed distance function is\nlinear in x, the optimization takes a quadratic form,\nas shown originally in Ref. [24]:\n\nminimize\nz\n\n1\n2\n\n(z − x)TW (z − x),\n\nsubject to Gz + h � 0\nwhere W is a constant diagonal weight matrix related\nto the mass of each vertex, and G and h are constraint\nparameters. The numbers of variables and constraints\nare n and m, i.e. x ∈ R\n\nn, h ∈ R\nm, and G ∈ R\n\nm×n.\nNote that this optimization problem has inputs x,\nG, and h, and output z. The goal here is to derive\n∂L/∂x, ∂L/∂G, and ∂L/∂h given ∂L/∂z, where L\nis the loss function.\n\nWhen computing the gradient using implicit\ndifferentiation, the dimensionality of the linear system\ncan be very high. Their key observation here is that\nn >> m > rank(G), since one contact often involves 4\nvertices (thus 12 variables) and some contacts may be\nlinearly dependent (e.g., multiple adjacent collision\n\npairs). They minimize the size of the linear equation\nbased on the QR decomposition of G, which is the key\nto accelerating backpropagation of high dimensional\nQP problems.\n\nOne of their experiments shows its ability to\noptimize material parameters from observation. The\nscene features a piece of cloth hanging under gravity\nand subjected to a constant wind force. The material\nmodel consists of three parts: density d, stretching\nstiffness S, and bending stiffness B. The stretching\nstiffness quantifies the reaction force when the cloth\nis stretched; the bending stiffness models how easily\nthe cloth can be bent and folded. Table 1 shows\nresults. They achieve a much smaller error in most\nmeasurements in comparison to the baselines; the\nlinear part of the stiffness matrix is modeled well.\nWith the computed gradient using their model, one\ncan effectively optimize the unknown parameters that\ndominate cloth movement to fit the observed data.\n\nIn follow-up work, Qiao et al. extended the\ndifferentiable simulation pipeline to couple with\nrigid body dynamics, formulated using generalized\ncoordinates:\n\nd\ndt\n\n⎛\n⎝ q\n\nq̇\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nq̈\n\n⎞\n⎠ =\n\n⎛\n⎝ q̇\n\nM−1f(q, q̇)\n\n⎞\n⎠\n\nand update the optimization formulation for collision\nresponse accordingly (see Ref. [21] for details):\n\nTable 1 Material parameter estimation results from Ref. [24]. Their proposed method runs faster than L-BFGS. Values of material parameters\nare Frobenius norms of the difference normalized by the Frobenius norm of the target. Values of the simulated result are the average pairwise\nvertex distances normalized by the size of the cloth. The gradient-based method yields much smaller errors than the baselines\n\nMethod\nRuntime\n\n(sec/step/iter)\n\nDensity\n\nerror (%)\n\nLinear stretching\n\nstiffness error (%)\n\nBending stiffness\n\nerror (%)\n\nSimulation\n\nerror (%)\n\nBaseline — 68 ± 46 160 ± 119 70 ± 42 12 ± 3.0\n\nL-BFGS 2.89 ± 0.02 4.2 ± 5.6 72 ± 90 70 ± 43 4.9 ± 3.3\n\nLiang et al. [24] 2.03 ± 0.06 1.8 ± 2.0 45 ± 41 77 ± 36 1.6 ± 1.4\n\n\n\nMachine learning for digital try-on: Challenges and progress 165\n\nminimize\nq′\n\n1\n2\n\n(q − q′)TM̂(q − q′)\n\nsubject to Gf(q′) + h � 0\n\nDue to the inclusion of rigid bodies, the constraints\nused in the optimization are no longer linear. When\ncomputing gradients, they linearize the constraints\naround a neighborhood as an approximation to enable\nQR decomposition for acceleration as previously\nmentioned.\n\n5 Garment modeling and design\nRealistic apparel model generation has become\nincreasingly popular, due to the rapid changes in\nfashion trends and the growing need for garment\nmodels in different applications such as virtual try-\non. It is already used even for state-of-the-art\ninteractive apparel design systems [27]. Application\nrequirements mean that it is important to have a\ngeneral cloth model that can represent a diverse set\nof garments. However, there are many challenges\nin automatic garment model generation. Firstly,\ngarments usually have different types of topology,\nespecially for fashion apparel, that makes it difficult\nto design a universal pipeline. Moreover, it is often\nnot straightforward for general garments design to\nbe retargeted onto another body shape, making\ncustomization difficult.\n\nPrevious work has addressed this problem to some\nextent. Huang et al. [28] proposed a realistic 3D\ngarment generation algorithm based on front and\nback image sketches, but it cannot readily retarget\ngenerated garments to other body shapes. Wang et\nal. [29] proposed an algorithm which can conveniently\nperform retargeting, but permits limited topology\nlike T-shirts or skirts. There is no recent work that\naddresses these two problems at the same time.\n\nWe introduce a learning-based parametric\ngenerative model to overcome the above difficulties,\ngiven garment sewing patterns and human body\nshapes as input. One possible approach would be to\ncompute a displacement image on the U–V space\nof the human body as a unified representation of\nthe garment mesh. Different topology and sizes\nof the garment are represented by different values\nin the image. The 2D displacement image, as the\nrepresentation of the 3D garment mesh data, can\n\nthen be fed into a conditional generative adversarial\nnetwork (cGAN) for latent space learning. The 2D\nrepresentation for the garment mesh can transfer\nthe irregular 3D mesh data to regular image data\nwhere a traditional CNN can easily learn. It can also\nextract relative geometric information with respect\nto the human body, enabling garment retargeting to\na different person.\n\n6 Conclusions\nAlthough virtual reality and digital try-on have\nexcellent potential and are rapidly developing, there\nremain open problems before online try-on systems\ncan be widely adopted. We have listed three major\nchallenges, all of which can be addressed or further\nimproved using machine learning algorithms. For\ngarment material prediction, state-of-the-art methods\nare still limited in that the training data is highly\nconstrained: the scenario contains only a piece\nof cloth floating in the wind. To improve its\napplicability to daily tasks, it is necessary to focus\non solving the problem on a more diverse set of\ninputs. Predicting the material from a garment\non a fixed human body could be a good start,\nbefore generalizing to arbitrary human motions and\npredicting multiple garments on the same body. In\nthe area of human shape estimation, it would be\ninteresting to learn how external constraints could\nimprove estimation accuracy. For example, the shape\nand size of the garment are hard constraints to\nwhich the predicted body should conform. While\noptimization-based methods can integrate these\nconstraints fairly easily, doing so remains elusive\nfor learning-based approaches. One possibility is\nto jointly estimate body and garment together and\nintroduce an intersection loss. This approach would\nrequire a new solution to the open problem of unified\ndeep garment representation, if we do not want to\ntrain one model for every garment type, which could\nbe even more challenging. We believe that substantial\nbreakthroughs in digital try-on are achievable with\nmore investigation in these directions.\n\nAcknowledgements\nThis research was supported in part by the Iribe\nProfessorship and the National Science Foundation.\n\n\n\n166 J. Liang, M. C. Lin\n\nReferences\n\n[1] Zheng, Z. H.; Zhang, H. T.; Zhang, F. L.; Mu, T. J.\nImage-based clothes changing system. Computational\nVisual Media Vol. 3, No. 4, 337–347, 2017.\n\n[2] Dibra, E.; Jain, H.; Öztireli, C.; Ziegler, R.; Gross,\nM. HS-Nets: Estimating human body shape from\nsilhouettes with convolutional neural networks. In:\nProceedings of the 4th International Conference on\n3D Vision, 108–117, 2016.\n\n[3] Bălan, A. O.; Black, M. J. The naked truth: Estimating\nbody shape under clothing. In: Computer Vision –\nECCV 2008. Lecture Notes in Computer Science, Vol.\n5303. Forsyth, D.; Torr, P.; Zisserman, A. Eds. Springer\nBerlin, 15–29, 2008.\n\n[4] Lassner, C.; Romero, J.; Kiefel, M.; Bogo, F.; Black,\nM. J.; Gehler, P. V. Unite the people: Closing the\nloop between 3D and 2D human representations. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 4704–4713, 2017.\n\n[5] Loper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.;\nBlack, M. J. SMPL: A skinned multi-person linear\nmodel. ACM Transactions on Graphics Vol. 34, No. 6,\nArticle No. 248, 2015.\n\n[6] Wei, S.-E.; Ramakrishna, V.; Kanade, T.; Sheikh, Y.\nConvolutional pose machines. In: Proceedings of the\nIEEE conference on Computer Vision and Pattern\nRecognition, 4724–4732, 2016.\n\n[7] Cao, Z.; Simon, T.; Wei, S.; Sheikh, Y. Realtime multi-\nperson 2D pose estimation using part affinity fields.\nIn: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 1302–1310, 2017.\n\n[8] Mehta, D.; Sridhar, S.; Sotnychenko, O.; Rhodin, H.;\nShafiei, M.; Seidel, H.-P.; Xu, W.; Casas, D.; Theobalt,\nC. VNect: Realtime 3D human pose estimation with\na single RGB camera. ACM Transactions on Graphics\nVol. 36, No. 4, Article No. 44, 2017.\n\n[9] Alldieck, T.; Magnor, M.; Xu, W.; Theobalt, C.; Pons-\nMoll, G. Video based reconstruction of 3D people\nmodels. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 8387–8397,\n2018.\n\n[10] Kanazawa, A.; Black, M. J.; Jacobs, D. W.; Malik, J.\nEnd-to-end recovery of human shape and pose. In:\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 7122–7131, 2018.\n\n[11] Varol, G.; Ceylan, D.; Russell, B.; Yang, J.; Yumer,\nE.; Laptev, I. Bodynet: Volumetric inference of 3D\nhuman body shapes. In: Proceedings of the European\nConference on Computer Vision, 20–36, 2018.\n\n[12] Zheng, Z.; Yu, T.; Wei, Y.; Dai, Q.; Liu, Y. Deephuman:\n3D human reconstruction from a single image. In:\n\nProceedings of the IEEE International Conference on\nComputer Vision, 7739–7749, 2019.\n\n[13] Saito, S.; Huang, Z.; Natsume, R.; Morishima, S.;\nKanazawa, A.; Li, H. PIFu: Pixel-aligned implicit\nfunction for high-resolution clothed human digitization.\nIn: Proceedings of the IEEE International Conference\non Computer Vision, 2304–2314, 2019.\n\n[14] Xu, Y.; Zhu, S.-C.; Tung, T. Denserac: Joint 3D pose\nand shape estimation by dense render-and-compare. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 7760–7770, 2019.\n\n[15] Smith, D.; Loper, M.; Hu, X.; Mavroidis, P.; Romero,\nJ. FACSIMILE: Fast and accurate scans from an image\nin less than a second. In: Proceedings of the IEEE\nInternational Conference on Computer Vision, 5329–\n5338, 2019.\n\n[16] Alldieck, T.; Magnor, M.; Bhatnagar, B. L.; Theobalt,\nC.; Pons-Moll, G. Learning to reconstruct people in\nclothing from a single RGB camera. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern\nRecognition, 1175–1186, 2019.\n\n[17] Kolotouros, N.; Pavlakos, G.; Black, M. J.; Daniilidis,\nK. Learning to reconstruct 3D human pose and shape\nvia modelfitting in the loop. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n2252–2261, 2019.\n\n[18] Liang, J.; Lin, M. C. Shape-aware human pose and\nshape reconstruction using multi-view images. In:\nProceedings of the IEEE International Conference on\nComputer Vision, 4352–4362, 2019.\n\n[19] Yang, S.; Pan, Z. R.; Amert, T.; Wang, K.; Yu, L.\nC.; Berg, T.; Lin, M. C. Physics-inspired garment\nrecovery from a single-view image. ACM Transactions\non Graphics Vol. 37, No. 5, Article No. 170, 2018.\n\n[20] Yang, S.; Liang, J.; Lin, M. C.; Learning-based cloth\nmaterial recovery from video. In: Proceedings of the\nIEEE International Conference on Computer Vision,\n4383–4393, 2017.\n\n[21] Qiao, Y. L.; Liang, J. B.; Koltun, V.; Lin, M. C.\nScalable differentiable physics for learning and control.\narXiv preprint arXiv:2007.02168, 2020.\n\n[22] De Avila Belbute-Peres, F.; Smith, K. A.; Allen, K.;\nTenenbaum, J.; Kolter, J. Z. End-to-end differentiable\nphysics for learning and control. In: Proceedings of the\nAdvances in Neural Information Processing Systems,\n2018.\n\n[23] Degrave, J.; Hermans, M.; Dambre, J.; Wyffels, F.\nA differentiable physics engine for deep learning in\nrobotics. Frontiers in Neurorobotics Vol. 13, 6, 2019.\n\n[24] Liang, J.; Lin, M.; Koltun, V. Differentiable cloth\nsimulation for inverse problems. In: Proceedings of\nthe 33rd Conference on Neural Information Processing\nSystems, 2019.\n\n\n\nMachine learning for digital try-on: Challenges and progress 167\n\n[25] Hu, Y.; Liu, J.; Spielberg, A.; Tenenbaum, J.\nB.; Freeman, W. T.; Wu, J.; Rus, D.; Matusik,\nW. ChainQueen: A real-time differentiable physical\nsimulator for soft robotics. In: Proceedings of the\nInternational Conference on Robotics and Automation,\n6265–6271, 2019.\n\n[26] Hu, Y. M.; Anderson, L.; Li, T. M.; Sun, Q.; Carr, N.;\nRagan-Kelley, J.; Durand, F. DiffTaichi: Differentiable\nprogramming for physical simulation. arXiv preprint\narXiv:1910.00935, 2019.\n\n[27] Liu, K. X.; Zeng, X. Y.; Bruniaux, P.; Tao, X. Y.; Yao,\nX. F.; Li, V.; Wang, J. 3D interactive garment pattern-\nmaking technology. Computer-Aided Design Vol. 104,\n113–124, 2018.\n\n[28] Huang, P.; Yao, J.; Zhao, H. Automatic realistic\n3D garment generation based on two images. In:\nProceedings of the International Conference on Virtual\nReality and Visualization, 250–257, 2016.\n\n[29] Wang, T. Y.; Ceylan, D.; Popović, J.; Mitra, N. J.\nLearning a shared shape space for multimodal garment\ndesign. ACM Transactions on Graphics Vol. 37, No. 6,\nArticle No. 203, 2018.\n\nJunbang Liang is a 4th-year Ph.D.\nstudent in the University of Maryland,\nCollege Park. He received his B.E.\ndegree from Tsinghua University in\n2016, and his M.S. degree from the\nUniversity of North Carolina in 2018.\nHis research interests are physics-based\ncloth simulation, computer vision, and\n\nmachine learning.\n\nMing C. Lin is a Distinguished\nUniversity Professor and Elizabeth\nStevinson Iribe Chair of Computer\nScience at the University of Maryland\nCollege Park and John R. and Louise S.\nParker Distinguished Professor Emerita\nof Computer Science at the University\nof North Carolina, Chapel Hill. She\n\nobtained her B.S., M.S., and Ph.D. degrees in electrical\nengineering and computer science from the University of\nCalifornia, Berkeley. She is a Fellow of ACM, IEEE,\nand Eurographics, and a member of ACM SIGGRAPH\nAcademy.\n\nOpen Access This article is licensed under a Creative\nCommons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduc-\ntion in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link\nto the Creative Commons licence, and indicate if changes\nwere made.\n\nThe images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and\nyour intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission\ndirectly from the copyright holder.\n\nTo view a copy of this licence, visit http://\ncreativecommons.org/licenses/by/4.0/.\nOther papers from this open access journal are available\nfree of charge from http://www.springer.com/journal/41095.\nTo submit a manuscript, please go to https://www.\neditorialmanager.com/cvmj.\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvTGlhbmctTGluMjAyMV9BcnRpY2xlX01hY2hpbmVMZWFybmluZ0ZvckRpZ2l0YWxUcnktby5wZGY1", "metadata_author": "Administrator", "metadata_title": "01-CVM0189.pdf", "metadata_creation_date": "2021-04-14T08:29:06Z", "keyphrases": [ "CVM0189" ] }, { "@search.score": 1, "content": "\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 \nDOI 10.1186/s40467-015-0033-9\n\nRESEARCH Open Access\n\nExtraction methods for uncertain inference\nrules by ant colony optimization\nLing Chen, Yun Sun* and Yuanguo Zhu\n\n*Correspondence:\nchinalsy_881220@163.com\nSchool of Science, Nanjing\nUniversity of Science and\nTechnology, Nanjing 210094, China\n\nAbstract\n\nIn recent years, the research on data mining methods has received increasing\nattention. In this paper, we design an uncertain system with the extracted uncertain\ninference rules to solve the classification problems in data mining. And then, two\nextraction methods integrated with ant colony optimization are proposed for the\ngeneration of the uncertain inference rules. Finally, two applications are given to verify\nthe effectiveness and superiority of the proposed methods.\n\nKeywords: Uncertain inference rule; Uncertain system; Ant colony optimization\nalgorithm; Rules extraction; Data classification\n\nIntroduction\nNowadays, databases and computer networks, coupled with the use of advanced auto-\nmated data generation and collection tools, are widely used in many different fields such\nas finance, E-commerce, logistics, etc. As a result, the amount of data that people have\nto deal with is dramatically increasing. People hope to carry out scientific research, busi-\nness decision, or business management on the basis of the analysis of the existing data.\nHowever, the current data analysis tools have difficulty in processing the data in depth.\nTo compensate for this deficiency, there come the data mining techniques. Data mining is\nthe computational process of discovering some interesting, potentially useful patterns in\nlarge data sets. Those patterns can be concepts, rules, laws, and modes. The overall goal\nof data mining is to extract information from a data set and transform it into an under-\nstandable structure for further use. Data mining helps us to discover valuable information\nand knowledge. Data mining is applied tomany fields in reality. There are many successful\nexamples [1] of data mining in business and science research. For instance, data mining is\nwidely used in financial data analysis, telecommunication, retail, and biomedical research.\nTherefore, the study of data mining technology has an important practical significance.\nThe main jobs of data mining are data description, data classification, data dependency,\n\ndata compartment analysis, data regression, data aggregate, and data prediction. What\ndata classification does is to find a couple of models or functions that can accurately\ndescribe the characteristics of the data sets. Then, we can identify the categories of the\npreviously unknown data. After obtaining themodels or functions from the set of training\ndata with data mining algorithms, we use many methods to describe the output such as\nclassification rules (if-then), decision trees, mathematical formula, and neutral network.\n\n© 2015 Chen et al.; licensee Springer. This is an Open Access article distributed under the terms of the Creative Commons\nAttribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work is properly credited.\n\nmailto: chinalsy_881220@163.com\nhttp://creativecommons.org/licenses/by/4.0\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 2 of 19\n\nThere are a variety of approaches in data mining. For mining objects in different fields,\nmany different specifiedmethods are invented. The approaches we usually used are statis-\ntical methods, machine learning methods, and modern intelligent optimization methods.\nThe statistical methods are very effective methods from the start. In addition, many other\ndata mining methods are invented based on the statistical methods. When dealing with\nclassification problems, Bayesian classification and Bayesian belief network are important\nclassification methods that based on the statistical principle. Machine learning methods\nare mainly used to solve the conceptual learning, pattern classification, and pattern clus-\ntering problems. The core content of machine learning is inductive learning. And there\nalready exist a number of mature technology methods, such as decision tree method for\nclassification problems. Decision trees method is one of the most popular classification\nmethods. The early decision trees algorithm is ID3 method. Later, based on ID3, many\nalgorithms such as C4.5 method [2] are proposed. Besides, there are some variants of the\ndecision trees algorithm including incremental tree structure ID4, ID5, and expandable\ntree structure SLIQ for massive data set.\nIn recent years, intelligent optimization algorithms are widely applied into data min-\n\ning. Neutral network is a simulation model for complex system with nonlinear relations.\nIt is very suitable to deal with complex nonlinear relations in spatial data. Researchers\nhave already proposed different network models to realize the clustering, classification,\nregression, and pattern recognition of the data. Furthermore, many evolution algorithms\nsuch as simulated annealing algorithm are introduced into neutral network algorithm\nas the optimization strategies. Genetic algorithm is a global search algorithm that sim-\nulates the biological evolution and genetic mechanism. It plays an important role in\noptimization and classification machine learning. Mixed algorithms of genetic algorithm\nand other algorithms, such as decision trees, neutral network, have been applied to the\ndata mining technology. Ant colony optimization algorithm is a bionic optimization algo-\nrithm that simulates the behavior of the ants. Based on that, a data mining technique\nant-miner [3] was invented. And Herrera [4] applied it to fuzzy rules learning. How-\never, ant colony optimization algorithm has some weakness such as slow convergence,\nrandom initial solutions. For this reason, some improved ant colony optimization algo-\nrithms are proposed. Zhu proposed an improved ant colony optimization algorithm\n(ACOA) [5] and a mutation ant colony optimization algorithm (MACO) [6] to speed up\nthe algorithms and avoid the solutions getting stuck in local optimums. Hybrid genetic\nant colony optimization [7] and hybrid particle swarm ant colony optimization algo-\nrithm [8] significantly improve the performance of the original ant colony optimization\nalgorithm.\nThe real world is so complex that human being may face different types of indetermi-\n\nnacy everyday. To get a better understanding of the real world, many mathematical tools\nare created. One of them is probability theory which is used to model indeterminacy from\nsamples. However, in many cases, no samples are available to estimate a probability distri-\nbution. In this situation, we have no choice but to invite some domain experts to evaluate\nthe belief degree that each event may occur. We cannot use probability theory to deal\nwith belief degree since human beings usually overweight unlikely events which makes\nthe belief degrees deviate far from the frequency. In view of this, Liu [9] founded uncer-\ntainty theory based on normality axiom, duality axiom, subadditivity axiom, and product\nmeasure axiom. It has become a powerful mathematical tool dealing with indeterminacy.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 3 of 19\n\nMany researchers have done a lot of theoretical work related to uncertainty theory. In\n2008, Liu [10] presented the uncertain differential equation. Later, the existence and\nuniqueness theorem was given [11]. And the stability of uncertain differential equation\nwas discussed [12,13]. Also, some analysis and numerical methods for solving uncertain\ndifferential equation were proposed. With uncertain differential equation describing the\nevolution of the system, we may solve some practical problems. Peng and Yao [14] stud-\nied an option pricing models for stocks. Zhu [15] proposed an uncertain optimal control\nmodel in 2010.\nIn [16,17], Liu proposed and studied the uncertain systems based on the concepts of\n\nuncertain sets, membership functions, and uncertain inference rules. An uncertain sys-\ntem is a function from its inputs to outputs based on the uncertain inference rule. Usually,\nan uncertain system consists of five parts: inputs, rule-base, uncertain inference rules,\nexpected value operator, and outputs. Following that, Gao et al. [18] generalized uncertain\ninference rules and described uncertain systems with them. Peng and Chen [19] proved\nthat uncertain systems are universal approximator and then demonstrated that the uncer-\ntain controller is a reasonable tool. Gao [20] designed an uncertain inference controller\nthat successfully balanced an inverted pendulum with 5 × 5 if-then rules. What is more\nimportant is that this uncertain inference controller has a good ability of robustness.\nOn the basis of uncertainty theory, we consider two extraction methods for uncertain\n\ninference rules by ant colony optimization algorithm. In the next section, we review the\nant colony optimization algorithm and give some basic concepts about uncertain sets.\nThen, we formulate a model to extract inference rules based on data set. And then, we\npropose an extraction method for uncertain inference rules by ant colony optimization\nalgorithm with a mutation operation. Finally, we combine the ant colony optimiza-\ntion algorithm with simulated annealing algorithm to speed up the extraction method.\nIn the last section, we discuss two typical classification problems in data mining with\nour results.\n\nPreliminary\nIn this section, we review the ant colony optimization algorithm. And then, we give some\nbasic concepts on uncertainty sets.\n\nAnt colony optimization algorithm\n\nAnt colony optimization algorithm, initiated by Dorigo, is a heuristic optimization\napproach. It simulates the behavior of real ants when they forage for food which relies on\nthe pheromone communication. In ant colony optimization algorithm, each path of artifi-\ncial ants walking from the food sources to the nest is a candidate solution to the problem.\nWhen walking on the path, the ants will release pheromone which evaporates over time.\nAnd the artificial ants will lay down more pheromone on the path corresponding to the\nbetter solution. While one ant has many paths to go, it will make a choice according to\nthe amount of the pheromone on the paths. The more pheromone there is on the path,\nthe better the solution is. As a result, bad paths will disappear since the pheromone evap-\norates over time. And good paths will be reserved since ants walking on it increases the\npheromone levels. Finally, one path which is used by most of the ants is left. Then, the\noptimal solution to the problem is obtained.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 4 of 19\n\nConsider the following optimization problem:\n\n⎧⎪⎪⎪⎨\n⎪⎪⎪⎩\nmin f (x)\ns.t.\n\ng(x) ≥ 0\nx ∈ D\n\n(1)\n\nwhere x is the decision variable in the domain D. And f (x) is the objective function while\ng(x) is the constraint function.\nWe can use ant colony optimization algorithm to obtain the optimal solution to the\n\nproblem (1). The parameters in the algorithm are initial pheromone τ0, ant transfer prob-\nability p, number of ants M, pheromone evaporation rate ρ, and number of iterations T .\nThe procedures are as follows.\n\nStep 1 Randomly generate a feasible solution x0 and set optimal solution s = x0. Initialize\nall pheromone trails with the same pheromone level τ0. Set k ← 0.\nStep 2 The artificial ant generates a walking path x in some probability p according to\n\nthe pheromone trails. If x ∈ D, then go to Step 3; otherwise, repeat Step 2 until x ∈ D.\nStep 3 Repeat Step 2 until for each ant and generate M feasible solutions. Let sk be the\n\nbest solution in this iteration.\nStep 4 If f (sk) < f (s), then s ← sk and update the pheromone trails according to the\n\noptimal solution in the current iteration.\nStep 5 If k < T , then k ← k + 1 and go to Step 2; otherwise, terminate.\nStep 6 Report the optimal solution.\n\nUncertain set\n\nLet � be a nonempty set and L be σ -algebra over �. Each � ∈ L is called an event. For\nany �, M{�} ∈ [0, 1]. The set function M defined on L is called an uncertain measure\nif it satisfies the following three axiom: M{�} = 1; M{�} + M{�c} = 1 for any � ∈ L;\nM\n\n{⋃∞\ni=1 �i\n\n} ≤ ∑∞\ni=1M{�i} for all �1,�2, · · · ∈ L. Then, the triplet (�,L,M) is called\n\nan uncertainty space [9]. The product uncertain measureM is an uncertain measure sat-\nisfying M\n\n{∏∞\ni=1 �k\n\n} = ∞∧\ni=1\n\nMk{�k}, where �k are arbitrarily chosen events from Lk for\nk = 1, 2, · · · , respectively.\n\nDefinition 1. [16] An uncertain set is a function ξ from an uncertainty space (�,L,M)\n\nto a collection of sets of real numbers such that both {B ⊂ ξ} and {ξ ⊂ B} are events for\nany Borel set B.\n\nExample 1. Take (�,L,M) to be {γ1, γ2, γ3} with power set L. Then, the set-valued\nfunction\n\nξ(γ ) =\n\n⎧⎪⎪⎨\n⎪⎪⎩\n[ 1, 3] , if γ = γ1\n\n[ 2, 4] , if γ = γ2\n\n[ 3, 5] , if γ = γ3\n\nis an uncertain set on (�,L,M).\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 5 of 19\n\nDefinition 2. [16] The uncertain sets ξ1, ξ2, ξ3, · · · , ξn are said to be independent if for\nany Borel sets B1,B2,B3, · · · ,Bn, we have\n\nM\n\n{ n⋂\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∧\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\nand\n\nM\n\n{ n⋃\ni=1\n\n(\nξ∗\ni ⊂ Bi\n\n)} =\nn∨\n\ni=1\nM\n\n{\nξ∗\ni ⊂ Bi\n\n}\n\nwhere ξ∗\ni are arbitrarily chosen from\n\n{\nξi, ξ ci\n\n}\n, i = 1, 2, · · · , n, respectively.\n\nDefinition 3. [21] An uncertain set ξ is said to have a membership function μ if for any\nBorel set B of real numbers, we have\n\nM{B ⊂ ξ} = inf\nx∈Bμ(x),M{ξ ⊂ B} = 1 − sup\n\nx∈Bc\nμ(x).\n\nThe above equations will be called measures inversion formulas.\n\nRemark 1. When an uncertain set ξ does have a membership function μ, it follows\nfrom the first measure inversion formula that\n\nμ(x) = M{x ∈ ξ}.\n\nExample 2. An uncertain set ξ is called triangular if it has a membership function\n\nμ(x) =\n⎧⎨\n⎩\n\nx−a\nb−a , a ≤ x ≤ b\n\nx−c\nb−c , b ≤ x ≤ c\n\n(2)\n\ndenoted by (a, b, c) where a, b, c are real numbers with a < b < c.\n\nDefinition 4. [21]Amembership functionμ is said to be regular if there exists a point x0\nsuch that μ(x0) = 1, and μ(x) is unimodal about the mode x0. That is, μ(x) is increasing\non (−∞, x0] and decreasing on [ x0,+∞).\n\nDefinition 5. [16] Let ξ be an uncertain set. Then, the expected value of ξ is defined by\n\nE[ ξ ]=\n∫ +∞\n\n0\nM{ξ \n r}dr −\n\n∫ 0\n\n−∞\nM{ξ � r}dr\n\nprovided that at least one of the two integrals is finite and\n\nM{ξ \n r} = 1\n2\n(M{ξ ≥ r} + 1 − M{ξ < r}),\n\nM{ξ � r} = 1\n2\n(M{ξ ≤ r} + 1 − M{ξ > r}).\n\nTheorem 1. [13] Let ξ be an uncertain set with regular membership function μ. Then\n\nE[ ξ ]= x0 + 1\n2\n\n∫ +∞\n\nx0\nμ(x)dx − 1\n\n2\n\n∫ x0\n\n−∞\nμ(x)dx, (3)\n\nwhere x0 is a point such that μ(x0) = 1.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 6 of 19\n\nExample 3. Let ξ be a triangular uncertain set denoted by (a, b, c). Then, according to\nTheorem 1, we have\n\nE[ ξ ]= a + 2b + c\n4\n\n.\n\nIn fact, it follows from Equations 2 and 3 that\n\nE[ ξ ] = b + 1\n2\n\n∫ c\n\nb\n\nx − c\nb − c\n\ndx − 1\n2\n\n∫ b\n\na\n\nx − a\nb − a\n\ndx\n\n= b − 1\n4\n(b − c) − 1\n\n4\n(b − a)\n\n= a + 2b + c\n4\n\n.\n\nUncertain inference rule\n\nHere, we introduce concepts of the uncertain inference and uncertain system. Inference\nrules are the key points of the inference systems. In fuzzy systems, CRI approach [22],\nMamdani inference rules [23] and Takagi-Sugeno inference rules [24] are the most com-\nmon used inference rules. Fuzzy if-then inference rules use fuzzy sets to describe the\nantecedents and the consequents. Unlike fuzzy inference, both antecedents and conse-\nquents in uncertain inference are characterized by uncertain sets. Uncertain inference\n[16] is a process of deriving consequences from human knowledge via uncertain set\ntheory. First, we introduce the following inference rule.\n\nInference Rule 1. [16] Let X and Y be two concepts. Assume a rule ‘if X is an uncertain\nset ξ , then Y is an uncertain set η’. From X is a constant a, we infer that Y is an uncertain\nset\n\nη∗ = η|a∈ξ\n\nwhich is the conditional uncertain set of η given a ∈ ξ . The inference rule is represented by\n\nRule: If X is ξ , then Y is η\n\nFrom: X is a constant a\n\nInfer: Y is η∗ = η|a∈ξ\n\nTheorem 2. [16] Let ξ and η be independent uncertain sets with membership functions\nμ and ν, respectively. If ξ∗ is a constant a, then the Inference Rule 1 yields that η∗ has a\nmembership function\n\nν∗(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nν(y)\nμ(a) , if ν(y) <\n\nμ(a)\n2\n\nν(y)+μ(a)−1\nμ(a) , if ν(y) > 1 − μ(a)\n\n2\n\n0.5, otherwise.\n\nBased on Inference Rule 1, Gao et al. [18] proposed the multi-input, multi-if-then-rule\ninference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 7 of 19\n\nInference Rule 2. [13] Let X1,X2, · · · ,Xm,Y be concepts. Assume rules ‘if X1 is ξi1\nand · · · and Xm is ξim, then Y is ηi’ for i = 1, 2, · · · , k. From X1 is a constant a1 and · · ·\nand Xm is a constant am, we infer that\n\nη∗ =\nk∑\n\ni=1\n\nci · ηi|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n, (4)\n\nwhere the coefficients are determined by\n\nci = M{(a1 ∈ ξi1) ∩ (a2 ∈ ξi2) ∩ · · · ∩ (am ∈ ξim)}\nfor i = 1, 2, · · · , k. The inference rule is represented by\n\nRule 1: If X1 is ξ11 and · · · and Xm is ξ1m, then Y is η1\nRule 2: If X1 is ξ21 and · · · and Xm is ξ2m, then Y is η2\n\n· · ·\nRule k: If X1 is ξk1 and · · · and Xm is ξkm, then Y is ηk\nFrom: X1 is a1 and · · · and Xm is am\nInfer: Y is determined by Eq. (4)\n\nTheorem 3. [13] Assume ξi1, ξi2, · · · , ξim, ηi are independent uncertain sets with mem-\nbership functions μi1,μi2, · · · ,μim, νi, i = 1, 2, · · · , k, respectively. If ξ∗\n\n1 , ξ∗\n2 , · · · , ξ∗\n\nm are\nconstants a1, a2, · · · , am, respectively, then the Inference Rule 2 yields\n\nη∗ =\nk∑\n\ni=1\n\nci · η∗\ni\n\nc1 + c2 + · · · + ck\n\nwhere η∗\ni are uncertain sets whose membership functions are given by\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nUncertain system\n\nUncertain system, proposed by Liu [16], is a function from its inputs to outputs based\non the uncertain inference rule. Usually, an uncertain system consists of five parts: inputs\nthat are crisp data to be fed into the uncertain system; a rule-base that contains a set of\nif-then rules provided by the experts; an uncertain inference rule that infers uncertain\nconsequents from the uncertain antecedents; an expected value operator that converts\nthe uncertain consequents to crisp values; and outputs that are crisp data yielded from\nthe expected value operator.\nNow, we consider an uncertain system with m crisp inputs α1,α2, · · · ,αm, and n crisp\n\noutputs β1,β2, · · · ,βn. We have the following if-then rules:\n\nIf X1 is ξ11 and · · · and Xm is ξ1m, then Y1 is η11 and Y2 is η12 and · · · and Yn is η1n\nIf X1 is ξ21 and · · · and Xm is ξ2m, then Y1 is η21 and Y2 is η22 and · · · and Yn is η2n\n\n· · ·\nIf X1 is ξk1 and · · · and Xm is ξkm, then Y1 is ηk1 and Y2 is ηk2 and · · · and Yn is ηkn\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 8 of 19\n\nThus, according to Inference Rule 1 and 2, we can infer that Yj(j = 1, 2, · · · , n) are\n\nη∗\nj =\n\nk∑\ni=1\n\nci · ηij|(a1∈ξi1)∩(a2∈ξi2)∩···∩(am∈ξim)\n\nc1 + c2 + · · · + ck\n,\n\nwhere ci = M{(a1 ∈ ξi1)∩ (a2 ∈ ξi2)∩· · ·∩ (am ∈ ξim)} for i = 1, 2, · · · , k. Then, by using\nthe expected value operator, we obtain\n\nβj = E\n[\nη∗\nj\n\n]\nfor j = 1, 2, · · · , n. Now, we construct a function from crisp inputs α1,α2, · · · ,αm to crisp\noutputs β1,β2, · · · ,βn, i.e.,\n\n(β1,β2, · · · ,βn) = f (α1,α2, · · · ,αm).\n\nThen, we get an uncertain system f. For the uncertain system we proposed, we have the\nfollowing theorem.\n\nTheorem 4. [13] Assume that ξi1, ξi2, · · · , ξim and ηi1, ηi2, · · · , ηin are indepen-\ndent uncertain sets with membership functions μi1,μi2, · · · ,μim, νi1, νi2, · · · , νin, i =\n1, 2, · · · , k, respectively. Then, the uncertain system from α1,α2, · · · ,αm to β1,β2, · · · ,βn is\n\nbj =\nk∑\n\ni=1\n\nci · E[ η∗\nij]\n\nc1 + c2 + · · · + ck\n,\n\nwhere j = 1, 2, · · · , n and η∗\nij are uncertain sets whose membership functions are given by\n\nν∗\nij(y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνij(y)\nci , if νij(y) < ci\n\n2\n\nνij(y)+ci−1\nμ(a) , if νij(y) > 1 − ci\n\n2\n\n0.5, otherwise\n\nand ci = min\n1≤l≤m\n\nμil(al) are constants.\n\nNext, we discuss the expected value of a special triangular uncertain set.Without loss of\ngenerality, we assume n = 1. Then the uncertain system proposed in the above becomes:\n\nb =\nk∑\n\ni=1\n\nci · E[ η∗\ni ]\n\nc1 + c2 + · · · + ck\n, (5)\n\nν∗\ni (y) =\n\n⎧⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎩\n\nνi(y)\nci , if νi(y) < ci\n\n2\n\nνi(y)+ci−1\nμ(a) , if νi(y) > 1 − ci\n\n2\n\n0.5, otherwise,\n\n(6)\n\nci = min\n1≤l≤m\n\nμil(al). (7)\n\nTheorem 5. Assume we have an uncertain system with m inputs and 1 output consist-\ning of k inference rules. The antecedents of the rules are represented by the uncertain sets ξi\nwith membership functions μi1,μi2, · · · ,μim, i = 1, 2, · · · , k. And the consequent is repre-\nsented by an triangular uncertain set ηi = (αi,βi, γi) with a membership function νi, where\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 9 of 19\n\nthe coefficients satisfy\n\nαi + γi = 2βi, i = 1, 2, · · · , k. (8)\n\nWe have\n\nE\n[\nη∗\ni\n] = βi, i = 1, 2, · · · , k.\n\nProof. Given the m input data a1, a2, · · · , am, we can calculate ci from Equation 7.\nThen, we can get the membership functions ν∗\n\ni of the consequence uncertain sets η∗\ni\n\naccording to Equation 6. Next, the computation of the expected value of uncertain\nconsequence breaks into three cases.\nCase 1: Assume ci/2 = 0.5. We can immediately have ν∗\n\ni (y) = νi(y), thus\n\nE[ η∗\ni ]=\n\nαi + 2βi + γi\n4\n\n= βi.\n\nCase 2: Assume ci/2 < 0.5. Let yi11 and yi12\n(\nyi11 < yi12\n\n)\nbe the two points that satisfy\n\nthe equation νi(y) = ci/2. Similarly, yi21 and yi22\n(\nyi21 < yi22\n\n)\nsatisfy the equation νi(y) =\n\n1 − ci/2. Since the membership function of a triangular uncertain set has a symmetry\nproperty, we have\n\nyi11 + yi12 = 2βi, yi21 + yi22 = 2βi. (9)\n\nThen, we can rewrite the membership function of ηi as follows:\n\nν∗\ni =\n\n⎧⎪⎪⎪⎪⎪⎪⎪⎪⎨\n⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nνi(y)\nci , if αi ≤ y < yi11\n\nνi(y)+ci−1\nci , if yi21 ≤ y < yi22\n\nνi(y)\nci , if yi12 ≤ y < γi\n\n0.5, otherwise.\n\n(10)\n\nAnd ν∗\ni (βi) = 1. Together with Equations 3, 8, and 9, we have\n\nE[ η∗\ni ] = βi + 1\n\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi12\n\nyi22\n0.5dy +\n\n∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy\n)\n\n−1\n2\n\n(∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy +\n∫ yi21\n\nyi11\n0.5dy +\n\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n= βi + 1\n2\n\n(∫ yi22\n\nβi\n\nνi(y) + ci − 1\nci\n\ndy −\n∫ βi\n\nyi21\n\nνi(y) + ci − 1\nci\n\ndy\n)\n\n+1\n2\n\n(∫ γi\n\nyi12\n\nνi(y)\nci\n\ndy −\n∫ yi11\n\nαi\n\nνi(y)\nci\n\ndy\n)\n\n+1\n2\n\n(∫ yi12\n\nyi22\n0.5dy −\n\n∫ yi21\n\nyi11\n0.5dy\n\n)\n\n= βi.\n\nCase 3: Assume ci > 0.5. Similarly, we have E[ η∗\ni ]= βi. Thus, we have proved the\n\ntheorem.\n\nProblem formulation\nIn this section, we propose an extraction model to obtain uncertain inference rules.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 10 of 19\n\nLet X = (x1, x2, · · · , xn) be the decision vector, which represents a rule base consisting\nof n rules. Each rule has m antecedents which are described by Q uncertain sets and one\nconsequent which is described by R uncertain sets. Each variable xi represents a sequence\nxi1xi2 · · · ximxim+1, where xij ∈ {0, 1, 2, · · · ,Q}(i = 1, 2, · · · , n; j = 1, 2, · · · ,m) represent\nthe antecedents of the inference rule. And xim+1 ∈ {0, 1, 2, · · · ,R}(i = 1, 2, · · · , n) repre-\nsent the consequent. Thus, each variable of decision vector represents one inference rule.\nSome xij = 0 means this antecedent is not included. And some xim+1 = 0 means this\ninference rule will not be included in the rule base. For example, assume that we have one\ninference rule consists of 4 antecedents and 1 consequent. They are described by 5 uncer-\ntain sets which refer to five descriptions: very low, low, medium, high, and very high. We\nuse 1, 2, 3, 4, 5 to denote them. Thus, sequence “23045”, for example, represents the rule:\n“if input 1 is low, input 2 is medium, and input 4 is high, then the output is very high”.\nUncertain systems can be used for classification. But which uncertain system is better\n\ndepends on the rule base. Here, we try to find best rule base by comparing the mean\nabsolute errors of the origin output and the system output. That is,\n\nMAE = 1\nP\n\nP∑\ni=1\n\n|oi − ti|, (11)\n\nwhere P is the number of training data, oi, ti(i = 1, 2, · · · ,P) are the system outputs and\norigin outputs, respectively. If we find the rule base with the least mean absolute error, we\nextract the uncertain inference rules successfully. We can obtain the system outputs by\nEquation 5. However, they may not be integers. To avoid this nonsense, for a classification\nproblem with C classes, we can divide interval that covers all the system outputs into C\nsubintervals. Then, if the output from Equation 5 is in the ith subinterval, we have oi = i.\nThus, we transfer the classification problem to the following optimization model:⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨\n\n⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩\n\nmin\nX\n\nF(X) = MAE\n\ns.t.\nX = (x1, x2, · · · , xn)\nxi = xi1 · · · ximxim+1\nxij ∈ {0, 1, · · · ,Q}\nxim+1 ∈ {0, 1, · · · ,R}\ni = 1, 2, · · · , n\nj = 1, 2, · · · ,m\n\nExtractionmethod for uncertain inference rules withmutations\nIn this section, we propose the extraction method for uncertain inference rules with\nmutations by ant colony optimization algorithm.\nAs stated before, each xi is a sequence of m values in {0, 1, 2, · · · ,Q} and 1 value in\n\n{0, 1, 2, · · · ,R}. Without loss of generality, we set Q = R. Each number in {0, 1, 2, · · · ,Q}\nis a node. Let ants walking across these nodes. Ants choose the next node in probability\nbased on the pheromone levels in the Q + 1 choices at every step. Once ants movem + 1\nsteps, a candidate decision variable is generated. After repeat this process n times, we get\na candidate solution. After all ants finish their walk, update the pheromone trails. Denote\nthe pheromone trail by τi;k,j(t) associated to the node j at step k of xi in iteration t. The\nprocedures are described as follows.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 11 of 19\n\n(1) Initialization: Randomly generate a feasible solutionX0, and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following\n\npk;k+1 = τi;k+1,j(t)\nQ∑\n\nq=0\nτi;k+1,q(t)\n\n. (12)\n\nIn this way, we could get a sequence xi1xi2 · · · xim+1. To speed up the algorithm, wemutate\nthis sequence to get a new candidate sequence. The mutation is made as follows: ran-\ndomly add 1 or subtract 1 to each element xij in the sequence; if the element is 0, the\nmutated element is 1; if the element is Q, the mutated element is Q − 1. Assume X ′ is\nthe mutated solution, if \rF = F(X ′\n\n) − F(X) ≤ 0, then X ← X ′ ; otherwise, keep the\ncurrent solution. If Q is very large, we could repeat this mutation until some termination\ncondition is satisfied.\n(3) Pheromone Update: At each iteration t, let X̂ be the optimal solution found so far\n\nand Xt be the best feasible solution in the current iteration. Assume F(X̂) and F(Xt) are\nthe corresponding objective function values.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nReinforce the pheromone trails on nodes of X̂ and evaporate the pheromone trails on\n\nthe left nodes:\n\nτi;j,k(t) =\n{\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1), otherwise\n\n(13)\n\nwhere ρ (0 < ρ < 1) is the evaporation rate, g(x)(0 < g(x) < +∞) is a function with that\ng(x) ≥ g(y) if F(x) < F(y), for example, g(x) = L/(|F(x)| + 1) is a function satisfying the\ncondition where L > 0.\n\nLet τ0 be the initial value of pheromone trails, n be the number of decision variables,\nM be the number of ants, ρ be evaporation rate and T be the number of iterations. Now,\nwe summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set l ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until X = (x1, x2, · · · , xn) is generated; mutate every xi: thus, gen-\n\nerate a new decision vector X ′ = (x′\n1, x\n\n′\n2, · · · , x\n\n′\nn); if \rF = F(X ′\n\n) − F(X) ≤ 0, then\nX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 for allM ants.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xl.\nStep 6 If F(Xl) < F(X̂), then X̂ ← Xl; update the pheromone trails according to\n\nEquation 13.\nStep 7 l ← l + 1; if l = T , terminate; otherwise, go to Step 2.\nStep 8 Report the optimal solution X̂.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 12 of 19\n\nWith this algorithm above, we obtain an uncertain rule base. Then, we successfully\ndesign an uncertain system and can use it for classification.\n\nExtractionmethod for uncertain inference rules with SA\nIn the previous section, to speed up the algorithm, we introduce a mutation operation.\nHere, we introduce the simulated annealing algorithm as the local search operation.\nSimulated annealing algorithm was initiated by Metropolis in 1953, applied to portfolio\n\noptimization by Kirkpatrick [25] in 1983. The name and inspiration come from anneal-\ning in metallurgy, a technique involving heating and controlled cooling of a material to\nincrease the size of its crystals and reduce their defects. Simulated annealing algorithm is\nexcellent at avoiding getting stuck in local optimums. It has a good robust property and is\nuniversal and easy to implement.\nFor optimization problem (1), we can use simulated annealing algorithm to search for\n\nthe optimal solution. The algorithm is as follows.\n\nStep 1 Randomly generate a initial solution x0; x ← x0; k ← 0; t0 ← tmax(initial\ntemperature);\nStep 2 If the temperature satisfies the inner cycle termination criterion, go to Step 3;\n\notherwise, randomly choose a point x′ in the neighborhood N(x), calculate \rf = f (x′\n) −\n\nf (x). If \rf ≤ 0, then x ← x′ ; otherwise, according to Metropolis acceptance criterion, if\nexp(−\rf /tk) > random(0, 1), then x ← x′ . Repeat Step 2.\nStep 3 tk+1 = d(tk) (temperature decrease); k ← k + 1; if the termination criterion is\n\nsatisfied, stop and report the optimal solution; otherwise, go to Step 2.\n\nIn this section, we combine ant colony optimization algorithm and simulated annealing\nalgorithm. In each iteration of ant colony optimization algorithm, we get a feasible solu-\ntion. Then, we use it as the initial solution of the simulated annealing algorithm to get a\nneighbor solution. This neighbor solution will be accepted in probability. And for each\ndecision vector X = (x1, x2, · · · , xn), xi = xi1xi2 · · · xim+1, we build the neighbor solution\nas follows: for each xi, for some randomly generated p and q (1 ≤ p < q ≤ m), reverse the\norder of the sequence xip · · · xiq, i.e., x′\n\ni = xi1 · · · xip−1xiqxiq−1 · · · xip+1xipxiq+1 · · · xim+1.\nFor example, assume xi is 0123456, p = 2, q = 6, and the neighbor solution x′\n\ni is 0543216.\nIn this way, we obtain a neighbor solution X ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ;\notherwise, if exp(−\rF/tk) > random(0, 1), then X ← X ′ ; otherwise, abandon this neigh-\nbor solution. Still denote the pheromone trail by τi;k,j(t). The procedure are described as\nfollows.\n\n(1) Initialization: Generate a feasible solution X0 randomly and set the optimal solution\nX̂ = X0. Set τi;k,j(0) = τ0, i = 1, 2, · · · , n, k = 1, 2, · · · ,m + 1, j = 0, 1, 2, · · · ,Q, where τ0\nis a fixed parameter.\n(2) Ant movement: At each step k after building the sequence xi1xi2 · · · xik , select the\n\nnext node in probability following Equation 12. In this way, we could get a sequence\nxi1xi2 · · · xim+1. In order to expand the search range, we use simulated annealing algo-\nrithm to search locally around the solution at this step. Assume the neighbor solution is\nX ′ . If \rF = F(X ′\n\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) > random(0, 1)\nwhere tk is the current temperature and tk → 0 when k → ∞, then X ← X ′ ; otherwise,\nabandon this neighbor solution and still choose the original feasible solution.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 13 of 19\n\n(3) Pheromone Update: Let X̂ be the optimal solution found so far and Xt be the best\nfeasible solution in the current iteration t. Assume F(X̂) and F(Xt) are the correspond-\ning objective function values. To avoid the optimal solution X̂ getting stuck in local\noptimums, we also use acceptance function here.\n\nIf F(Xt) < F(X̂), then X̂ ← Xt .\nBuild a neighbor solution X̂ ′ .\nIf F(X̂ ′\n\n) ≤ F(X̂), then X̂ ← X̂ ′ ;\nIf F(X̂ ′\n\n) > F(X̂), check the Metropolis acceptance criterion, i.e., if\nexp(−\rF̂/Tt) > random(0, 1), Tt → 0, t → ∞, then X∗ ← X̂ ′ .\n\nReinforce the pheromone trails on the nodes of X̂ andX∗ and evaporate the pheromone\ntrails on the left nodes:\n\nτi;j,k(t) =\n\n⎧⎪⎨\n⎪⎩\n\n(1 − ρ)τi;j,k(t − 1) + ρg(X̂), if (k, j) ∈ X̂\n(1 − ρ)τi;j,k(t − 1) + ρ\n\n2 g(X̂), if (k, j) ∈ X∗\n\n(1 − ρ)τi;j,k(t − 1), otherwise\n(14)\n\nwhere, ρ (0 < ρ < 1) is the evaporate rate, and g(x) (0 < g(x) < +∞) is a function with\nthat g(x) ≥ g(y) if F(x) < F(y). For example, g(x) = L/(|F(x)|+1) is an available function\nif L > 0.\n\nNow, we summarize the algorithm as follows.\n\nStep 1 Initialize all pheromone trails with the same pheromone level τ0. Randomly\ngenerate a feasible solution X0, and set optimal solution X̂ = X0. Set t ← 0.\nStep 2 Ant movement in probability following Equation 12. Generate a decision variable\n\nxi afterm + 1 steps.\nStep 3 Repeat Step 2 until decision vector X = (x1, x2, · · · , xn) is generated. Build the\n\nneighbor solution X ′ . If \rF = F(X ′\n) − F(X) ≤ 0, X ← X ′ ; otherwise, if exp(−\rF/tk) >\n\nrandom(0, 1)where tk is the current temperature and tk → 0 when k → ∞, thenX ← X ′ .\nStep 4 Repeat Step 2 and Step 3 until all ants finish their walk, and generate M candidate\n\nsolutions.\nStep 5 Calculate the system outputs by Equation 5. Then, calculate the objective function\n\nvalues for the M candidate solutions by Equation 11. Denote the best solution in this\niteration by Xt .\nStep 6 If F(Xt) < F(X̂), then X̂ ← Xt . Build the neighbor solution of X̂, which is denoted\n\nby X̂ ′ . If \rF̂ = F(X̂ ′\n) − F(X̂) ≤ 0, then X̂ ← X̂ ′ ; otherwise, if Metropolis acceptance\n\ncriterion is satisfied, i.e., if exp(−\rF̂/Tt) > random(0, 1),Tt → 0, t → ∞, thenX∗ ← X̂ ′ .\nStep 7 Update the pheromone trails according to Equation 14.\nStep 8 t ← t + 1; if t = T, terminate; otherwise, go to Step 2.\nStep 9 Report the optimal solution X̂.\n\nTable 1 Parameters\n\nap bp cp\n\np = 1 0.5 1.01 1.52\n\np = 2 1.7 2.74 4.48\n\np = 3 5 6.07 7.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 14 of 19\n\nFigure 1 Results of method A.\n\nExperiments\nIn this section, we use our two extraction methods to extract uncertain inference rules.\nAnd then use the uncertain systems to solve some classification problems.We applied our\nmethods to the IRIS [26] classification problem and the Wisconsin Breast Cancer (WBC)\n[27] classification problem.\n\nIRIS classification\n\nIRIS data set is the typical date set in data classification. It contains 150 instances of 3\nclasses, which are Setosa, Versicolor, and Virginica. Each class has 50 instances. Each\ninstance has 4 attributes which are sepal length (SL), sepal width (SW), petal length (PL),\n\nFigure 2 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 15 of 19\n\nTable 2 Accuracy comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 97.33\n\nMethod B This paper 97.5\n\nC4.5 [2] 94.0\n\nACOA [5] 96.6\n\nMACO [6] 95.53\n\nHNFQ [28] 98.67\n\nTable 3 IRIS classification rules extracted bymethod A\n\nIF THEN\n\nSL SW PL PW Class\n\n1 3 1 3 1\n\n1 0 1 1 1\n\n1 2 3 2 1\n\n1 1 2 1 2\n\n2 1 0 3 2\n\n3 2 0 2 3\n\n1 1 3 3 3\n\nTable 4 IRIS classification rules extracted bymethod B\n\nIF THEN\n\nSL SW PL PW Class\n\n3 2 3 1 2\n\n1 1 0 0 2\n\n0 2 1 1 3\n\n0 1 1 3 1\n\n1 1 3 3 2\n\n1 1 3 1 1\n\n2 1 1 2 1\n\nTable 5 Parameters\n\nap bp cp\n\np = 1 0.3 1.01 1.72\n\np = 2 2 6.07 10.14\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 16 of 19\n\nFigure 3 Results of method A.\n\nand petal width (PW). They are described by 3 uncertain sets: low (1), medium (2), and\nhigh (3). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.618 and Vq = q−1\n2 , q = 1, 2, 3. Based on these 4 attributes,\n\nwe try to infer which class does the instance belong to. We use 3 triangular uncertain\nsets ηp = (ap, bp, cp) (p = 1, 2, 3) to describe the possible classes (class 1: Setosa; class 2:\nVersicolor; class 3: Virginica). And the parameters ap, bp, cp ∈ R are listed in Table 1.\nFirst, we normalize the data to [0, 1] to simplify the computation. IRIS data set is our\n\ntraining set while it is also used for testing. Then, we set maximum number of rules n =\n10, number of ants M = 10, evaporate rate ρ = 0.3, and number of iterations T = 300.\nEach algorithm runs ten times. The results are in Figures 1 and 2. Denote the extraction\n\nFigure 4 Results of method B.\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 17 of 19\n\nTable 6 Accuracy rate comparison\n\nMethod Paper Accuracy rate (%)\n\nMethod A This paper 98.3\n\nMethod B This paper 98.33\n\nC4.5 [2] 94.25\n\nACOA [5] 97.91\n\nMACO [6] 97.07\n\nFMM [29] 97.86\n\nmethod with mutation by A and the method with SA by B. It can be seen that the method\nA converges fast at about 120th iteration. And method B converges a little slower at about\n150th iteration.\nThen, we can classify the IRIS data with the uncertain systems we introduced. We\n\nfind the average accuracy rates of the two methods are 97.33% and 97.5%, respectively.\nComparison with other methods are listed in Table 2.\nList the rule bases we get with the highest accuracy rates (98.0% and 98.67%, respec-\n\ntively) in Tables 3 and 4. Note that although the maximum number of rules is 10, the final\nrule bases we obtain has only 7 rules.\n\nWisconsin Breast Cancer classification\n\nWisconsin Breast Cancer data set is a common medical date set. It contains 699 instances\nof 2 classes, which are sick and healthy. Two hundred forty-one instances are sick and 458\ninstances are healthy. Each instances has 9 attributes, which are clump thickness (CT),\nuniformity of cell size (UCS), uniformity of cell shape (UCCS), marginal adhesion (MA),\nsingle epithelial cell size (SPCS), bare nuclei (BN), bland chromatin (BC), normal nucleoli\n(NN), and mitoses (MT). They are described by 5 uncertain sets: very low (1), low (2),\nmedium (3), high (4), and very high (5). The membership functions are\n\nμq(x) = exp\n(\n\n− (x − Vq)2\n\n2β2\n\n)\n,\n\nwhere x is the input, β = 0.4247, and Vq = q−1\n2 , q = 1, 2, 3, 4, 5. Based on these attributes,\n\nwe diagnose whether one instance is sick or not. We use 2 triangular uncertain sets\nηp = (ap, bp, cp) (p = 1, 2) to describe the possible classes (sick and healthy). And the\nparameters ap, bp, cp ∈ R are listed in Table 5.\n\nTable 7WBC classification rules extracted bymethod A\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n1 5 0 3 2 0 4 2 1 2\n\n1 2 1 1 4 4 2 1 0 1\n\n1 3 5 2 3 2 1 1 4 2\n\n1 3 4 2 3 1 2 2 1 1\n\n2 4 4 1 1 2 4 5 1 2\n\n3 3 4 5 4 3 2 4 4 2\n\n5 2 4 0 3 0 0 2 1 1\n\n2 4 4 1 1 2 4 5 1 1\n\n2 4 2 3 5 3 2 5 5 2\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 18 of 19\n\nTable 8WBC classification rules extracted bymethod B\n\nIF THEN\n\nCT UCS UCCS MA SPCS BN BC NN MT Class\n\n5 3 3 3 3 2 3 2 4 2\n\n0 0 0 0 0 0 0 4 0 1\n\n4 4 4 4 0 1 1 1 4 2\n\n1 4 4 1 1 2 1 5 1 2\n\n1 1 1 2 0 3 5 5 5 2\n\n0 3 0 0 0 0 0 1 2 1\n\nFirst, we normalize the data to [0, 1] to simplify the computation. The first 460 instances\nare used for training while the left 239 instances are used for testing. Then, we set max-\nimum number of rules n = 10, number of ants M = 20, evaporate rate ρ = 0.3, and\nnumber of iterations T = 200. Each algorithm runs ten times. The results are in Figures 3\nand 4. We still find that method A converges faster than method B. Method A stabilizes\nat about 50th iteration while method B stabilizes until about 80th iteration.\nThen, we test the uncertain systems we get with the later 239 instances. We find the\n\naverage accuracy rates of the two methods on the training set are 96.0% and 96.26%,\nrespectively. Using the uncertain system with the highest accuracy rate of each method\non the test set, we find the accuracy rates are 98.37% and 98.33%. Comparison with other\nmethods are listed in Table 6.\nThe rule base with the highest accuracy rates (98.37% and 98.33%, respectively) on the\n\ntest set are listed in Tables 7 and 8. Method A gives us a rule base of 9 rules, and method\nB provides a rule base of 6 rules.\nWe apply our two extraction methods to the classification problems of IRIS data set\n\nand WBC data set. Compare our results with other researchers’ work, we can find that\nboth methods have higher accuracy rate than ACOA and MACO in two classification\nproblems. And for IRIS data set, accuracy rates of method A and B are lower than HNFQ\nbut higher than C4.5. For WBC data set, their accuracy rates are higher than C4.5 and\nFMM.\n\nConclusions\nIn this paper, we designed an uncertain system for data classification. And we proposed\ntwo extraction methods for uncertain inference rules by using ant colony optimization\nalgorithm. Then, we applied our methods to IRIS classification problem and WBC clas-\nsification problem. Our methods are shown to be superior in accuracy to some existing\nmethods.\n\nAcknowledgements\nThis work is supported by the National Natural Science Foundation of China (No.61273009).\n\nReceived: 14 December 2014 Accepted: 20 April 2015\n\nReferences\n1. Kantardzic, M: Data Mining: Concepts, Models, Methods, and Algorithms. 2nd ed. Wiley, Hoboken (2011)\n2. Quinlan, JR: Improved use of continuous attributes in C4.5. J. Artif. Intell. Res. 4(1), 77–90 (1996)\n3. Parpinelli, RS, Lopes, HS, Freitas, AA: Data mining with an ant colony optimization algorithm. IEEE Trans. Evolut.\n\nComput. 6(4), 321–332 (2002)\n4. Casillas, J, Cordón, O, Herrera, F: Learning fuzzy rules using ant colony optimization algorithms. In: Proceedings of\n\nthe 2nd International Workshop on Ant Algorithms: From Ant Colonies to Artificial Ants, pp. 13–21, Brussels, (2000)\n\n\n\nChen et al. Journal of Uncertainty Analysis and Applications (2015) 3:9 Page 19 of 19\n\n5. Zhu, Y: Ant colony optimization-based hybrid intelligent algorithms. World J. Modell. Simul. 2(5), 283–289 (2006)\n6. Zhu, Y: An intelligent algorithm: MACO for continuous optimization models. J. Intell. Fuzzy Syst. 24, 31–36 (2013)\n7. Lee, Z, Su, S, Chuang, C, Liu, K: Genetic algorithm with ant colony optimization (GA-ACO) for multiple sequence\n\nalignment. Appl. Soft Comput. 8(1), 55–78 (2008)\n8. Shelokar, PS, Siarry, P, Jayaraman, VK, Kulkarni, BD: Particle swarm and ant colony algorithms hybridized for improved\n\ncontinuous optimization. Appl. Math. Comput. 188(1), 129–142 (2007)\n9. Liu, B: Uncertainty Theory. 2nd ed. Springer, Berlin (2007)\n10. Liu, B: Fuzzy process, hybrid process and uncertain process. J. Uncertain Syst. 2(1), 3–16 (2008)\n11. Chen, X, Liu, B: Existence and uniqueness theorem for uncertain differential equations. Fuzzy Optimization Decis.\n\nMak. 9(1), 69–81 (2010)\n12. Liu, B: Some research problems in uncertainty theory. J. Uncertain Syst. 3(1), 3–10 (2009)\n13. Liu, B: Uncertainty Theory: A Branch of Mathematics for Modeling Human Uncertainty. Springer, Berlin (2010)\n14. Peng, J, Yao, K: A new option pricing model for stocks in uncertainty markets. Int. J. Oper. Res. 8(2), 18–26 (2011)\n15. Zhu, Y: Uncertain optimal control with application to a portfolio selection model. Cybern. Syst. 41(7), 535–547 (2010)\n16. Liu, B: Uncertain set theory and uncertain inference rule with application to uncertain control. J. Uncertain Syst. 4(2),\n\n83–98 (2010)\n17. Liu, B: Uncertain logic for modeling human language. J. Uncertain Syst. 5(1), 3–20 (2011)\n18. Gao, X, Gao, Y, Ralescu, DA: On Liu’s inference rule for uncertain systems. Int. J. Uncertain. Fuzz. Knowledged-Based\n\nSyst. 18(1), 1–11 (2010)\n19. Peng, Z, Chen, X: Uncertain systems are universal approximators. J. Uncertainty Anal. Appl. 2, Article, 13 (2014)\n20. Gao, Y: Uncertain inference control for balancing inverted pendulum. Fuzzy Optimization Decis. Mak. 11(4), 481–492\n\n(2012)\n21. Liu, B: Membership functions and operational law of uncertain sets. Fuzzy Optimization Decis. Mak. 11(4), 387–410\n\n(2012)\n22. Zadeh, LA: Outline of a new approach to the analysis of complex systems and decision processes. IEEE Trans. Syst.\n\nMan Cybern. 3(1), 28–44 (1973)\n23. Mamdani, EH: Applications of fuzzy algorithms for control of a simple dynamic plant. Proc. Institution Electr. Eng.\n\nControl Sci. 121(12), 1585–1588 (1974)\n24. Takagi, K, Sugeno, M: Fuzzy identification of system and its applications to modeling and control. IEEE Trans. Syst.\n\nMan Cybern. 15(1), 116–132 (1985)\n25. Kirkpatrick, S, Gelatt, CD, Vecchi, MP: Optimization by simmulated annealing. Science. 220(4598), 671–680 (1983)\n26. Iris dataset (1936). https://archive.ics.uci.edu/ml/datasets/Iris\n27. Wisconsin Breast Cancer Dataset (1992). https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n28. de Souza, FJ, Vellasco, M, Pacheco MA: Hierarchical neuro-fuzzy quadtree models. Fuzzy Sets Syst. 130(2), 189–205\n\n(2002)\n29. Gabrys, B, Bargiela, A: General fuzzy min-max neural network for clustering and classification. IEEE Trans. Neural\n\nNetwor. 11(3), 769–783 (2000)\n\nSubmit your manuscript to a \njournal and benefi t from:\n\n7 Convenient online submission\n\n7 Rigorous peer review\n\n7 Immediate publication on acceptance\n\n7 Open access: articles freely available online\n\n7 High visibility within the fi eld\n\n7 Retaining the copyright to your article\n\n Submit your next manuscript at 7 springeropen.com\n\nhttps://archive.ics.uci.edu/ml/datasets/Iris\nhttps://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)\n\n\tAbstract\n\tKeywords\n\n\tIntroduction\n\tPreliminary\n\tAnt colony optimization algorithm\n\tUncertain set\n\tUncertain inference rule\n\tUncertain system\n\n\tProblem formulation\n\tExtraction method for uncertain inference rules with mutations\n\tExtraction method for uncertain inference rules with SA\n\tExperiments\n\tIRIS classification\n\tWisconsin Breast Cancer classification\n\n\tConclusions\n\tAcknowledgements\n\tReferences\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNDY3LTAxNS0wMDMzLTkucGRm0", "metadata_author": null, "metadata_title": null, "metadata_creation_date": "2015-05-16T18:35:45Z", "keyphrases": [] }, { "@search.score": 1, "content": "\nSentiment analysis and the complex \nnatural language\nMuhammad Taimoor Khan1*, Mehr Durrani2, Armughan Ali2, Irum Inayat3, Shehzad Khalid1 and Kamran \nHabib Khan4\n\nIntroduction\nSentiment analysis (Pang and Lillian 2008) is a type of text classification that deals with \nsubjective statements. It is also known as opinion mining, since it processes opinions in \norder to learn about public perception. Sentiment analysis and opinion mining are the \nsame, and are used interchangeably throughout the document. It uses natural language \nprocessing (NLP) to collect and examine opinion or sentiment words. SA is explained \nas identifying the sentiments of people about a topic and its features (Pang and Lillian \n2008). The reason for the popularity of opinion mining is because people prefer to take \nadvice from others in order to invest sensibly. Determining subjective attitudes in big \nsocial data is a hotspot in the field of data mining and NLP (Hai et al. 2014).\n\nAbstract \n\nThere is huge amount of content produced online by amateur authors, covering a \nlarge variety of topics. Sentiment analysis (SA) extracts and aggregates users’ senti-\nments towards a target entity. Machine learning (ML) techniques are frequently used \nas the natural language data is in abundance and has definite patterns. ML techniques \nadapt to domain specific solution at high accuracy depending upon the feature set \nused. The lexicon-based techniques, using external dictionary, are independent of data \nto prevent overfitting but they miss context too in specialized domains. Corpus-based \nstatistical techniques require large data to stabilize. Complex network based tech-\nniques are highly resourceful, preserving order, proximity, context and relationships. \nRecent applications developed incorporate the platform specific structural information \ni.e. meta-data. New sub-domains are introduced as influence analysis, bias analysis, and \ndata leakage analysis. The nature of data is also evolving where transcribed customer-\nagent phone conversation are also used for sentiment analysis. This paper reviews \nsentiment analysis techniques and highlight the need to address natural language \nprocessing (NLP) specific open challenges. Without resolving the complex NLP chal-\nlenges, ML techniques cannot make considerable advancements. The open issues and \nchallenges in the area are discussed, stressing on the need of standard datasets and \nevaluation methodology. It also emphasized on the need of better language models \nthat could capture context and proximity.\n\nKeywords: Sentiment analysis, Machine learning, Sentiment orientation, Complex \nnetworks\n\nOpen Access\n\n© 2016 Khan et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\n\nREVIEW\n\nKhan et al. Complex Adapt Syst Model (2016) 4:2 \nDOI 10.1186/s40294-016-0016-9\n\n*Correspondence: \ntaimoor.muhammad@gmail.\ncom \n1 Bahria University, Shangrilla \nRoad, Sector E-8, Islamabad, \nPakistan\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40294-016-0016-9&domain=pdf\n\n\nPage 2 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nManufacturers are also interested to know which features of their products are more \npopular in public, in order to make profitable business decisions. There is a huge reposi-\ntory of opinion content available at various online sources in the form of blogs, forums, \nsocial media, review websites etc. They are growing, with more opinionated content \npoured in continuously. It is, therefore, beyond the control of manual techniques to \nanalyze millions of reviews and to aggregate them towards a rapid and efficient deci-\nsion. Sentiment analysis techniques perform this task through automated processes with \nminimal or no user support. The online datasets may also contain objective statements, \nwhich do not contribute effectively in sentiment analysis. Such statements are filtered at \npre-processing.\n\nOpinion mining deals with identifying opinion patterns and presenting them in a \nway that is easy to understand. The outcome of sentiment analysis can be in the form \nof binary classification, such as categorizing opinions as recommended or not recom-\nmended. It can be considered as a multi-class classification problem on a given scale of \nlikeness. Cambria et al. (2013) used common-sense knowledge to improve the results of \nsentiment analysis. The results can be presented in the form of a short summary gen-\nerated from the overall analysis. Sentiment analysis has various sub streams including \nemotion analysis, trend analysis, and bias analysis etc. Its applications has outgrown \nfrom business to social, political and geographical domains. Sentiment analysis is \napplied to emails for gender identification through emotion analysis (Mohammad and \nYang 2011). Emotion is applied to fairy tales to draw interesting patterns (Mohammad \n2011). Considering text a complex network of words that are associated to each other \nwith sentiments, graph based analysis techniques are used for NLP tasks.\n\nNatural language processing\n\nOpinion mining requires NLP, to extract semantics of opinion words and sentences. \nHowever, NLP has open challenges that are too complex to be handled accurately till \ndate. Since sentiment analysis makes extensive use of NLP, it has this complex behav-\nior reflected. The assumptions in NLP for text categorization do not work with opinion \nmining, as they are different in nature. Documents having high frequency of matching \nwords may not necessarily possess same sentiment polarity. It is because, a fact in text \ncategorization could be either correct or incorrect, and is well known to all. Unlike facts, \na variety of opinions can be correct about the same product, due to its subjective nature. \nAnother difference is that, opinion mining is sensitive to individual words, where a sin-\ngle word like NOT may change the whole context. The open challenges are negations \nwithout using NOT word, sarcastic and comparative sentences etc. The later section has \na detailed discussion on NLP issues that affect sentiment analysis.\n\nThe subjective content from the online sources have simple, compound or complex \nsentences. Simple sentences possess single opinion about a product, while compound \nsentences have multiple opinions expressed together. Complex sentences have implicit \nmeaning and are hard to evaluate. Regular opinions pertain to a single entity only, while \ncomparative opinions have an object or some of its aspects discussed in comparison to \nanother object. Comparative opinions can either be objective or subjective. An example \nof a subjective sentence having comparison is “The sound effects of game X are much \nbetter than that of game Y” whereas an example of objective sentence with comparison is \n\n\n\nPage 3 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\n“Game X has twice as many control options as that of Game Y”. Opinion mining expects \na variety of sentence types, since people follow different writing styles in order to express \nthemselves in a better way.\n\nSentiment analysis\n\nThe machine learning (ML) based techniques are supervised, semi supervised or unsu-\npervised. The supervised techniques require labeled data, while the semi supervised \ntechniques need manual tuning from domain experts. The unsupervised techniques \nmake use of statistical analysis on large volume of data. ML techniques has a large fea-\nture set using Bag-of-words (BOW). Results are improved by pruning repetitive and \nlow quality features. The opinion words are extracted to identify the polarity of opinion \nexpressed for a feature. The performance of a classifier is measured through its effective-\nness at the cost of efficiency. Effectiveness is calculated as precision/recall and F-meas-\nure, which are measurements of relevance.\n\nSentiment analysis can also be considered as a complex network. It consists of nodes \nand edges joining them. Many complex systems from a variety of domains are repre-\nsented as network including environmental modeling (Niazi et al. 2010), business sys-\ntems (Aoyama 2002), wireless sensors, and ad-hoc networks (Niazi and Hussain 2009). \nNetworks are rich in information, having a range of local and global properties. Text cor-\npora can be used with words as nodes and edges representing the structural or seman-\ntic association between them. The adjacent nodes sharing a link are closely associated \nand directly affect each other through the weight of the link they share. Representing \ntext as complex network, various properties like centrality, degree distribution, com-\nponents, communities, paths etc. can be used to explore the data thoroughly. Through \nmulti-partite graphs, nodes can be distributed among various clusters with inter-cluster \nedges only. It separates different types of entities discussed in comparison. Entities are \nlinked to their respective aspects/features and then to the sentiments associated. The \nsentiments can be linked with the reasons shared in support of those sentiments.\n\nData sources\n\nOpinion mining has diverse subjective data sources that are available online. They cover \na large number of topics and are up-to-date with current issues. Introduction of Web2.0 \nin the last decade has enabled people to post their thoughts and opinions on a range of \ntopics. The data produced online is growing all the time produced by people from differ-\nent backgrounds (Katz et al. 2015). Opinion mining makes use of this data generated by \nmillions of users all over the world. According to Business Week survey in 2009, 70 % of \nthe people consult online reviews and ratings to make a purchase. Comscore/The Kelsey \ngroup in 2007 reported that 97 % of the people who made purchases based on online \nreviews, found them to be honest.\n\nThe user generated subjective content is of value to be assessed and summarized for \nprospective customers. These online data sources are in the form of blogs, reviews and \nsocial media websites. The popularity of blogging is on the rise, where people from dif-\nferent walks of life express their opinions about various entities and events and get com-\nments on them. At times, it leads to a form of discussion among the author and various \nusers commenting on them. A detailed analysis on blogging styles of authors, as they \n\n\n\nPage 4 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nfollow their own unique approaches for expressing their feelings is provided in (Chau \nand Xu 2007). Blogs contain opinions about various products, services, their features, \npackages and promotions. Most of the online studies on opinion extraction use blogs as \ndatasets (Qiang and Rob 2009) to perform detailed analysis.\n\nThere are professional review websites providing customers’ feedbacks, used for sen-\ntiment analysis. E-commerce websites allow customers to comment on their products. \nSocial media is another popular medium of sharing information among like-minded \npeople. Here, a variety of subjects are discussed where people express their opinions, \nbased on their own experience. Social media websites have a very complex struc-\nture for extracting information having user opinions. They allow users to express their \nviews through sharing articles and other media sources as an external link. Twitter, also \nreferred to as microblogging, has the problem of reviews being too short and at times \nmiss the context.\n\nThis review article is organized into the following divisions. Section 2 reviews the Sen-\ntiment analysis techniques and the NLP issues. Section 3 provides a discussion on the \nreview studied and Sect. 4 list the application areas for sentiment analysis. Section 5 has \nconcluded the study to important issues drawn from the study. Section 6 has distribu-\ntion of the work carried out by the authors.\n\nReview\nThe sentiment analysis techniques categorize reviews into positive and negative bins or \nmultiple degrees of it. The social data can be analyzed at three different levels i.e. user \ndata, relationship data and content (Tang et al. 2014). In survey (Guellil and Boukhalfa \n2015) these categories are further elaborated. Recommender systems are extended to \nsupport textual content using knowledge (Tang et al. 2013). In our previous work (Khan \nand Khalid 2015) sentiment analysis is highlighted to address health care problems from \nthe view point of a user. The issues faced in SA also depend on the data sources and \nnature of analysis required. An important aspect of social data analysis is the identifi-\ncation of sentiments and sentiment targets (Tuveri and Angioni 2014; Zhang and Liu \n2014). Opinion mining also consider the additional features of opinion holder and time. \nSentiment analysis techniques can be separated into three groups: supervised, semi-\nsupervised and unsupervised techniques.\n\nThe supervised techniques are the machine learning classifiers. They are more accu-\nrate, however, need to be trained on a relevant domain. The unsupervised statistical \ntechniques do not require training. They are efficient in dynamic environment but at the \ncost of accuracy. Sentiment analysis techniques analyze opinion datasets to generate a \ngeneral perception that people have about a product. The classification of sentiments in \na review document is performed through identifying and separating all the positive and \nnegative opinion words. Considering the strength of these words, along with their polar-\nity, helps in multi-class classification. Machine learning classifiers such as Naive-Bayes, \nk-nearest neighbor and centroid based classifier etc., are successfully used for this pur-\npose. Semantic orientation based techniques used for opinion mining are Lexicon based \nand statistical analysis. Lexicon based technique works with individual words while sta-\ntistical analysis incorporates words co-occurrence using point wise mutual information \n(PMI) and latent semantic analysis (LSA). Semi-supervised techniques start with a small \n\n\n\nPage 5 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nset of opinion words from the given domain, and expand on it. More opinion words are \nexplored by querying the starting seeds. The newly found words are queried again to find \nmore words until no new words are returned. Orientation of the opinion word form the \nbasis for classification. Other attributes used are frequency of occurrence, location and \nco-occurrence with other words. The taxonomy of these approaches is shown in Fig. 1.\n\nSentiment classification\n\nThese are the machine learning classifiers used for sentiment analysis. They can be \napplied to text documents at three levels for analysis. A document level approach, \nwhich studies the whole document as a single entity is appropriate for text categoriza-\ntion. However, document level approach is not viable for sentiment analysis with docu-\nments having multiple opinions. Therefore, sentiment analysis is performed extensively \nat sentence or word level. Word level analysis is also known as sentiment level analysis. \nML techniques suits sentiment analysis as the data is in abundance and there is obvious \npresence of patterns (Schouten and Frasincar 2015). The classifiers are trained on label \ndataset having samples representing all classes. A test dataset is used to evaluate the per-\nformance of the classifiers for the given task. Let the set of documents as {D = d1,…,dn}, \nand set of classes labeled as {C = c1,…,cn}, then the task is to classify document di in D \nwith a label ci in C. This task can be performed using supervised classifiers. The more \nfrequently used classifiers for sentiment analysis are discussed below.\n\nNaïve Bayes\n\nNaive Bayes (NB) classifier is extensively used for text classification. It learns from a \ntraining dataset of annotated feature vectors, with labels as positive and negative (in case \nof binary classification). The probability of a feature vector is calculated with each label \nusing the annotated training dataset. The feature vector is assigned a label that has high-\nest probability for it. If this information is preserved, it can be used to show confidence \nin a label for a feature vector. In further modifications of NB a fuzzy region is defined \nin which feature vectors hold both labels with a certain level of confidence. Text data \nnormally have high dimensional feature vectors. Therefore, the process of calculating \nprobability is repeated for each feature vector, and then all the probabilities contribute \ntowards the final decision. The feature set is represented as F = f1, f2…fm}, where prob-\nability of a document belonging to a class shown as:\n\nFig. 1 Taxonomy of expository literature on sentiment analysis\n\n\n\nPage 6 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nShows the probability of a document dj represented by its vector dj* belonging to a class \nci. It is the product of probabilities for all the features in the feature set. The document \nvector dj\n\n* is assigned to a class ci in order to maximize P\n(\n\nci\n\n∣\n\n∣\n\n∣\nd∗j\n\n)\n\n. The logarithm of prob-\nabilities are summed up to classify an opinion document. It is preferred over product of \nprobabilities to avoid underflow. It addresses the missing value problem as well. Slack \nvariables add smoothing effect against noisy data. Weights can also be assigned to fea-\ntures which define their contribution towards the classification. It is a biased approach, \nwhere prominent features are given high weights to play a major role in choose a senti-\nment label.\n\nNaive Bayes works on the assumption that all the sentences of a review document are \nopinion sentences. It also assumes that features of a document are independent of each \nother. Despite of this unrealistic assumption, Naïve Bayes is very successful and is used \nin various practical applications. The assumption of treating features as independent of \neach other makes Naive Bayes highly efficient (Dai et al. 2007). Although, Naive Bayes \nclassifier is simple, yet it is effective because of its robustness to irrelevant features. It \nperforms well in domains with many equally important features. It is considered to be \nmore reliable for text classification and sentiment analysis. The accuracy of the classifier \nimproves with pre-processing noise. It also used as transfer learning when trained on a \ndataset similar to the target dataset.\n\nNearest neighbor\n\nk-nearest neighbor classifier has been frequently used in literature for text classifica-\ntion. It considers the labels of k nearest neighbors to classify a test document. A special \ncase of the k-NN problem is typically referred to as classimbalance problem identified \nin (Yang and Liu 1999). Classes with more training data have higher influence to predict \nsame label for the new document. There are fewer chances of acquiring a class label if \nthat class has fewer training examples. (Li et al. 2003) catered this problem by using vari-\nable value of k for each class. Thus, the class having more training data will have higher \nvalue of k as compared to the one having few samples. This solution is helpful in online \nclassification, where there is time constraint on trying different values of k.\n\nA study on performance of k-NN using pre-processed dataset is conducted in (Shin \net al. 2006) claiming 10 % improvement when noise and outliers are filtered out. An opti-\nmum value is chosen as threshold to separate regular data from noise. Sentiment analy-\nsis is performed with a reduced set of feature vector in (Sreemathy and Balamurugan \n2012) to avoid the curse of dimensionality. Accuracy of the model improves as irrelevant \nfeatures were removed. Features are assigned weights to vary their contribution towards \ndecision making. Weights are extracted from probability of information in documents \nacross different categories. Tree-fast k-NN is introduced as fast kNN model (Soucy and \nMineau 2001). This tree based indexing of retrieval system improves the accuracy of \nk-NN in distance calculation. Its effective against large feature sets. The order of features \nand their thresholds are identified from within the training data. k-NN has promising \n\n(1)P(ci\n∣\n\n∣dj\n∗\n) =\n\np(ci)(\n∏m\n\ni=1 p(fi|ci) )\n\np(d∗j )\n\n\n\nPage 7 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nresults in sentiment analysis; however, it is more susceptible to noise and high dimen-\nsional feature set. Therefore, more of the work in k-NN for text classification has focused \non feature selection and reduction techniques as they are the driving factors of k-NN’s \nperformance.\n\nCentroid based\n\nCentroid based (CB) classifier calculates centroid vector or prototype vector for each \nclass in the training dataset. Centroid vector is the central point of the class and may not \nrepresent an actual training data. The distance of each test document is calculated with \nthe prototype vector of the class and is classified based on similarity with it. Its perfor-\nmance depends on the chosen centroid vectors. It is efficient since time and space com-\nplexities are proportional to the number of classes rather than training documents. To \ndouble the training data reverse of reviews are generated in (Xia et al. 2015) by invert-\ning the sentiment terms and their labels. Using both sets of training data with Mutual \nInformation (MI) the results were improved when only selected reviews were inverted. \nExternal dictionary WordNet is used to generate inverse for sentiment terms, however, \npseudo-antonyms can be generated internally using the corpus.\n\nms, however, pseudo-antonyms can be generated internally using the corpus. A variety \nof approaches have been used for CB classifier. Rocchio algorithm calculates centroid \nto represent feature space of documents (Ana and Arlindo 2007; Tan 2007a, b). Cen-\ntroid is computed through average of positive examples in (Han and Karypis 2000) and \nsum of positive cases i.e. the related training examples (Chuang et al. 2000). Normalized \nsum of positive vectors used in (Lertnattee and Theeramunkong2004), cosine similar-\nity between the test document and the Centroid of a class (Hidayet and Tunga 2012). \nCentroid is used with inverse of class similarity as well improving the accuracy close to \n100 % on the given dataset when characters are chosen as features instead of n-grams.\n\nCentroid evaluation is sensitive to noise in the training dataset which affects the over-\nall performance of the classifier. This shortfall is exposed when Centroid classifier is \napplied to a slightly different domain. The reason for this drawback is that some opinion \nwords are domain dependent. They have different polarity or strength of polarity when \nused in a different domain. Smoothing techniques have being proposed in (Tan 2007a, b; \nLertnattee and Theeramunkong 2006; Guan 2009) that minimizes the effect of noise \nin the dataset. (Chizi et al. 2009) defined a weighting scheme giving higher weights to \nexplicit opinion words. Characters and special characters for feature selection are used \nin (Ozgur and Gungor 2009). The work in (Shankar and Karypis 2000; Tan et al. 2005) \nis focused on adjusting the value of centroid based with feedback looping, hypothesis \nmargin and weight-adjustment respectively. They try to rectify class Centroid, if it is not \ncalculated accurately. Centroid based classifier performs efficiently as it doesn’t consider \ntraining data each time to decide a test document.\n\nSupport vector machine\n\nSupport vector machine classifier is used for text classification in various studies. It finds \na separation among the data using the annotated training dataset. The margin of sep-\naration between classes, which is known as hyperplane, is used to classify the incom-\ning data. The hyperplane should give maximum separation between the classes. It is \n\n\n\nPage 8 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\napplicable even in the presence of high dimensional feature set representation. It classify \nbased o hyperplane among classes. Like centroid-based, SVM also consider the hyper-\nplane to classify a test document. (Brown et  al. 1997) has compared SVM with artifi-\ncial neural networks for text classification and has found it better. Since it has promising \nresults in text classification, it also performs well for opinion mining. They have also \nclaimed in (Brown et al. 1997) that SVM is better than Naive Bayes and decision trees \nclassification algorithms. However, SVM consumes more resources at the training \nstage. Although, it is efficient with large feature set, Feldman et al.(2011) has shown that \ndimensionality reduction in feature set further improves the performance of SVM. It \nexhibits linear complexity and can scale up to a large dataset.\n\nSVM has a limitation of over-reliance on selection of suitable kernel function. Kernel \nis calculated through Linear, Polynomial, Gaussian or sigmoid methods but they tend to \nbe domain specific. Kernel functions that perform well for one domain may not repeat it \nfor next. Its accuracy is also sensitive to number of training samples close to hyperplane. \nSlack variables are introduced to limit the impact of boundary samples by generalizing \nthe classifier, known as soft margin classification. They also help to avoid over-fitting the \ntraining data.\n\nUnsupervised techniques\n\nThe unsupervised sentiment analysis techniques do not require training data and rather \nrely on semantic orientation. They make use of lexicons to identify the positive or neg-\native semantics of opinion words. The meaning of the word, expressed by its use in a \ncontext is called lexicon. An online or off-line dictionary is consulted for this purpose. \nStatistical analysis techniques are also unsupervised, identifying the orientation of senti-\nment words through statistical evaluations. They require large volume of data for high \naccuracy.\n\nLanguages consists of lexicons that are the words used for a particular sense, and a \ngrammar that connect these lexicons. Part-of-speech rules are used to extract senti-\nment phrases from text document. Search engines are used to identify the orientation \nof sentiment words that are missing in the dictionary. Its polarity is identified through \nthe nearby words brought by search engines. They purely rely on external sources and \ntherefore cannot address the context. Lexicon based techniques perform well for general \ndomains while statistical techniques addresses the context and are useful in specialized \ndomains. The two types of approaches are discussed in detail.\n\nDictionary (Lexicon) based techniques\n\nLexicon based techniques extract opinion lexicons from the document and analyzes \nits orientation without the support of any training data. These techniques process the \nopinion words separately, ignoring the relationship between them. Lexicons refer to the \nsemantic orientation. Lexicons are independent of the source data and therefore it does \nnot fall for over-fitting. But context not addressed either in this approach (Katz et  al. \n2015; Cambria 2013). Search engines are used to find the meaning of unknown opinion \nlexicons. They are searched and the top N results are accepted to identify its orientation. \nThe semantics of lexicons can be categorized as positive or negative with weights rep-\nresenting their strength. This approach struggles with lexicons having domain specific \n\n\n\nPage 9 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\npolarity. For example, good has positive polarity in any type of domain but “heavy \nweight” has positive polarity for bike domain but negative for the domain of electronic \ndevices.\n\nIn its simplest form, sentiment words are split into positive and negative as binary \ndistribution. A more sophisticated approach has fuzzy lexicons, introducing a grey area \nbetween the two categories. These fuzzy lexicons exist in both the classes with a score \nassociated to it, representing the strength of each label. Various manual and semi-auto-\nmatic techniques can be used for building lexicons. Princeton University’s WordNet is \na popular lexicon source available for sentiment analysis. Dictionaries like WordNet, \nextracts synonyms and antonyms for the provided opinion words. Manual cleansing is \nemployed to rectify the lists generated for the unknown sentiment words. These opinion \nwords are used to classify a review as positive or negative.\n\nFixed syntactic patterns are also used for expressing opinions which are composed of \npart-of-speech (POS) tags. The basic idea of this technique is to identify the patterns in \nwhich words co-occur with each other and to exploit those patterns for understanding \nits semantic orientation. One example of such pattern is an adverb followed by an adjec-\ntive. A more sophisticated approach was proposed by (Mohammad and Yang 2011), \nwhich used a WordNet distance based method to determine the sentiment orientation. \nThe distance d(t1, t2) between terms t1 and t2 is the length of the shortest path that con-\nnects them in WordNet, as shown in Eq. 2. The semantic orientation (SO) of an adjective \nterm t is determined by its relative distance from two reference (or seed) terms good and \nbad. The polarity of opinion term t is resolved through eq.\n\nStatistics (Corpus) based techniques\n\nStatistical analysis of large corpus of text can also be used to determine the sentiment \norientation of words. Co-occurrence of words is evaluated without consulting any exter-\nnal support. Two methods are used for this purpose which are point wise mutual infor-\nmation (PMI) and latent semantic analysis (LSA). PMI method for co-occurrence is \ngiven as:\n\nwhere w1 and w2 refers to two words in a given sentence. The main concept behind PMI \nbased techniques is that the semantic orientation of a word has a tendency of being \nclosely related to that of its neighbors. Equation 3 gives the probability of words w1 and \nw2 to co-exist, based on the measure of degree of statistical dependence between the \ntwo. This approach is, however, implemented differently in LSA based techniques. In \nLSA, matrix factorization technique is used with singular value decomposition to dem-\nonstrate the statistical co-occurrence of words. More formally, this process can be speci-\nfied as:\n\n(2)SO(t) =\nd(t, bad)− d(t, good)\n\nd(bad, good)\n\n(3)p(w1,w2) =\np(w1,w2)\n\np(w1) p(w2)\n\n(4)LSA(w) = LSA(w, {+paradigms})− LSA(w, {−paradigms})\n\n\n\nPage 10 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nwhere a word w is passed to LSA with positive and negative paradigms. LSA based tech-\nniques develop a matrix having rows as words and columns as sentences or paragraphs. \nEach cell possesses a weight corresponding to the relation of the word in row with the \nsentence or paragraph in columns. This matrix is decomposed into three matrices using \nsingular value decomposition (SVD).\n\nComplex challenges\n\nOpinion mining is a relatively new area of research and there are open challenges that \nneed to be answered. Some of the challenges are common to opinion mining in general \nwhile others are related to their own sources and context depending upon the domain of \nthe dataset. These issues affect the performance of machine learning techniques, but it \nhas little control on them. Figure 2 gives NLP challenges faced in sentiment analysis, dis-\ntributing them into their logical groups. The groupings are based on the parsing level, at \nwhich these issues occur. The following sub section has detailed discussion on the NLP \nissues.\n\nDocument level\n\nDocument level NLP challenges are the ones that are faced at the document or review \nlevel. They deal in general with the review document or the reviewer style. It is common \nto find reviews that have the information about an object, given in an informal manner. \nCapitalization is over or under used. Spelling mistakes are ignored or words being short-\nened. It makes the analysis very difficult for the automatic techniques to identify features \nand associate them. The unknown words (shortened/miss spelled) are matched with \nsimilar words to identify the aspect or opinion words. Slang specific to a certain region \nare also occasionally used in reviews and discussions. Reviews having sarcastic expres-\nsions are the hardest to deal with. Even though they have the opinion words explicitly \nmentioned, they do not serve the purpose for which they are normally used. For exam-\nple, “What an awesome phone! It stopped responding in few hours”. These types of \nexpressions are quite frequent in political reviews.\n\nThere are some document level challenges that are specific to certain domains only. \nThe opinion words can also be domain dependent, where they have different orientation \ndepending upon the domain in which they are used. This problem arise in specialized \ndomains e.g. medical or astronomy etc. The opinion words in this case cannot be evalu-\nated correctly, without domain knowledge. The general opinion words, however, have \nsame orientation irrespective of the domain in which they are used e.g. good, bad etc. \nOpinion data attained from platforms like blogs and forums face the problem of dealing \nwith discussions. They allow their users to comment on reviews, which at times gets into \nthe shape of a debate. In discussions, users may agree with each other on some points \nwhile disagree on others. They need to be tackled differently, as they require the flow of \ncontext to be maintained from comment to comment in a sequence. Although, they are \ntough to process, discussions are very informative in which authors not only show their \nliking or disliking, but also support it through reasoning.\n\nSpamming has been an issue faced at multiple frontiers of online data and sentiment \nanalysis is no different. In fact it is the most highlighted area of sentiment analysis, con-\nsidering its popularity and impact for industries. Spammers post false reviews about \n\n\n\nPage 11 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nproducts either to promote or demote it and if there are more numbers of such reviews \nit will affect the performance of all opinion mining techniques adversely. (Mukherjee \net al. 2011; Mukherjee and Liu 2012) has stressed on the identification and filtering of \nspam reviews, prior to applying any mining techniques. Spam reviews can be written by \nindividuals or commercial companies, dealing in such business. Spam opinions include \na fake review where the author does not write his own feelings about a product. Other \ntypes of spamming include irrelevant, non-review content and advertising text etc. Psy-\nchological studies are used to identify spamming, that helps to find patterns when peo-\nple lie. Meta information can also provide insight into it, as spammers normally tend to \npost more content in shorter time. This information can be very helpful if thoroughly \nstudied to search for outliers. In (Lim et al. 2010; Kamps et al. 2004; Mohammad and \nTony 2011) different machine learning techniques are used to detect spammers, who are \nthen assigned a spamming behavior number to keep track of them.\n\nSentence level\n\nThese challenges are faced at the sentence level while parsing a review. They arise when \nthe sentence expressing a review is not a simple sentence, that is expressing a single \n\nFig. 2 Complex NLP challenges in sentiment analysis\n\n\n\nPage 12 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nproduct or feature with a single opinion word. Complex sentences can be comparative, \nconditional or having grouped opinions etc. (Narayanan et al. 2009) worked with sen-\ntences that are in the form of a question for the opinion audience. These sentences place \na condition on an entity and are hard to parse and evaluate for a certain opinion. For \nexample “if you are not happy with your notepad ++ code editor, try this new version of \ndreamweaver”. In this sentence author is positive about dreamweaver” whereas he/she \ndoes not say anything about the “notepad ++”. Without “if” it would be clearly a nega-\ntive opinion of the “notepad ++” but now its inconclusive towards it.\n\nComparative sentences express a situation in which the opinion about one feature is \ndiscussed in comparison to another. In this situation, identifying the target of opinion \nwords is very important, as there are more than one targets discussed. Secondly, aspects \nare to be associated to their respective products discussed in comparison to each other. \nFor example “HP Laptops are stylish as compared to Dell and Sony”. In order to resolve \nthis opinion, the information about the style of HP, Dell and Sony is crucial. In this case, \nwithout identifying the target features and their related opinions, the situation cannot be \nresolved (Jindal and Liu 2006). Must-link refer to the situation in which a single opinion \nword is shared by more than one features or entities. For example “My new office has \nattractive furniture, coloring and decoration”. In this sentence, the opinion attractive is \nbeing shared between three features of the entity office. Reviews are more opinion cen-\ntered as compared to blogs and forums where the focus may deviate from the topic. In \nsuch discussions, not all the sentences can be evaluated for extracting opinions. There \nare certain sentences that do not bear any opinion which needs to be filtered. For exam-\nple “Our team has strong batting line-up” is evaluative whereas “I am excited for our \nteam’s batting” is non-evaluative (Zhai et al. 2011). Mihalcea and Carlo 2009 has consid-\nered the problem of identifying words that make sense subjectively.\n\nThe opinion source and target identification is very important to classify opinions \naccurately. A target is the receiving entity of the opinion to which the opinion is enti-\ntled whereas; source is the person holding the opinion. Source identification is a concern \nwhen authors present the opinion of a third person. In the example “I bought a pen 2 days \nago. It was such a nice pen. Its feel in hand is really cool. Its tip is soft and is very fluent. \nHowever, my mother was mad with me as I did not tell her before I bought it. She also \nthought the pen was too expensive, and wanted me to return it to the shop”. The author \nhimself/herself is the source of the first four lines whereas the source of the last two lines \nis author’s mother, (Patella and Ciaccia 2009). Dealing with negation is also of high impor-\ntance, since it overturns the orientation of the opinion words. For example, the sentence \n“I am not interested in this car” is negative. Negation words need to be dealt with a lot of \ncare as not all occurrences of such words mean negation. For example, not in “not only \nbut also” is not used for negation. Similarly negation can also be used without explicitly \nusing any negation words like “Theoretically it takes care of the screen resolution”.\n\nFeature level\n\nThese are the open issues faced at the features level in sentiment analysis. Natural lan-\nguages are highly rich allowing a variety of words and phrases that could be used to \nexpress one’s feelings. They consist of words that are used interchangeably for the same \nfeature. If these synonyms are not identified, it will result in redundant features, which \n\n\n\nPage 13 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nat worse may have different opinion classification (Zhai et al. 2010). The features that are \nreferred using different words, are grouped together and all their opinions are merged to \nget an aggregate opinion. For example “picture” and “photo” refers to the same feature \nof camera. Such features needs to be grouped based on synonyms otherwise they might \nbe missed out or classified incorrectly. Feature stemming and pruning are also essential \nto identify similar opinion words and group them together. It reduces the set of opin-\nion words that are used for classification. For example words like attraction, attractive, \nattracted, attracting are stemmed to the word attract and are considered as a single opin-\nion word. Since all of the above words have same opinion with same orientation there-\nfore, stemming them will enable the classifier to treat them as the same word attract. \nReducing the feature set improves the performance of the classifiers. The opinion tar-\nget may also be implicit, in which case the opinion is mentioned without explicitly giv-\ning the product or its feature. It normally happens when the target product or feature is \nalready in discussion in previous sentence.\n\nComplex products like phones, laptops cannot be recommended or not recommended \nas a whole. They have many features or aspects which need more in-depth study. If the \nopinion words are associated directly to the target domain, while by passing its feature, \na lot of valuable information is missed. Aspect-based sentiment analysis (ABSA) con-\nsider aspect or features as the opinion targets. The features are associated to products \nto aggregate features opinions for products also. For example “Dell laptops have power-\nful batteries”. Such opinions need to be associated to the battery, which is a feature of \n“Dell laptop” and should not be referenced to it as a whole. This is also important for the \nreason that potential buyers are after certain strong features in the product considering \ntheir own situation and liking (Somprasertsri and Latitrojwong 2010).\n\nLexicon level\n\nThe problems faced at lexicon level are related to identifying the semantics of the word \nused. Dual meaning words and expressions depends on context of use. These words can-\nnot be considered as positive or negative without having context knowledge. For exam-\nple “the battery of this phone works for longer duration but the start-up takes longer \ntoo”. Here “longer” is a positive opinion for battery backup but a negative opinion for \nstart-up time (Ding and Liu 2007). The general opinion lexicon refers to opinion words \nlike good, excellent, bad and poor etc. Most of the sentence and document based opin-\nion mining use them as core of their techniques. There is only a small set of opinion \nlexicons publicly available. A universal opinion lexicon is required that would provide \ninformation on all such words (Qiu et  al. 2011). A semi-automatic technique of deal-\ning with this problem is to find synonyms and antonyms of initially given lexicon seeds \npassed to search engine. The process is repeated several times to explore as many opin-\nion words as possible.\n\nDing and Liu (2010) refers to the problem of product-aspect co-reference. It is \nrequired in scenarios where products and their aspects along with the associated opin-\nions, are not expressed in the same sentence. This is called opinion passage on aspect, \nexpressing opinion as a group of consecutive sentences. For example, “I bought a Honda \nbike yesterday. It looks beautiful. I took it out for a ride yesterday. That was a great feel-\ning”. In this example, It refers to bike whereas, that refers to ride which is a feature of \n\n\n\nPage 14 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nbike. This co-relation among products and their aspects need to be identified for associ-\nating products to their aspects in multiple sentences. It is called co-reference resolution \nproblem was identified by Ott et al. (2011). Partially supervised clustering techniques are \nfavored for this problem. Linguistic rules are very important to get sense of the opinions, \nrather than trusting the sentiment word only. Although, they are hard to apply, but are \nhelpful in exploring the implicit meaning of words, for the sense in which they are used. \nFor example “This car has good interior and not only that, the price is affordable too”. In \nspite of having the word not, the meaning of the sentence is not inverted as is normally \nthe case with the negation word.\n\nDiscussion\nSentiment analysis is an area of diversified research fields including machine learning, \nnatural language processing, language identification and text summarization. Most of \nits issues are related to NLP which are quite complex and under research focus. The text \nobtained from reviews need to be classified into different languages, while working with a \nmulti-lingual system. For each language, evaluative and subjective sentences are identified \nwhile others are discarded. Trimming is applied on the subject data for reducing the fea-\nture set which are further classified into either positive and negative (binary classification) \nor greater number of classes. Regression techniques are preferred for using multi-class \nproblems. Table 1 provides a comparative analysis of the techniques used for sentiment \nanlaysis. Opinion mining has certain common grounds with text classification using tech-\nniques from Information retrieval. The NLP issues discussed affect all Sentiment analysis \ntechniques, however, supervised techniques are more vulnerable to it. Opinion orienta-\ntion has a context inclined towards psychology and linguistics. Complex networks can \n\nTable 1 Comparison of the techniques and approaches included in the study\n\n1 require training, 2 use training data to classify, 3 probabilistic approach, 4 driving factor, 5 similarity metric, 6 strength, 7 \nweakness, 8 support for streaming data\n\nSNo. Naïve Bayes k-Nearest neighbor Centroid\n\n1 Yes Yes Yes\n\n2 Yes Yes No\n\n3 Yes No No\n\n4 Word probability Value of k Centroid vector\n\n5 Probability weights Distance similarity Vector distance\n\n6 Simple and fast Handle co-related features Classify on vector distance\n\n7 Assume feature independence Sensitive to irrelevant features Sensitive to noise\n\n8 Yes Too expensive No\n\nSNo. Support vector machine Lexicon (dictionary) based Statistical (corpus) based\n\n1 Yes No No\n\n2 No NA NA\n\n3 No No Yes\n\n4 Kernel function Word polarity Feature matrix\n\n5 Hyperplane Word polarity Word distance\n\n6 Classify on hyperplane Can identify new lexicons Handle online data\n\n7 Require more resources Struggle with domain context Conceptual document size\n\n8 No Yes Yes\n\n\n\nPage 15 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nhelp in resolving context through preserving sequence and by associating words in a sen-\ntence, sentences in a paragraph and even paragraphs in an article. Sentiment analysis has \nits roads crossed with many different research areas and therefore, its problems are to be \naddressed with solutions coming from areas other than machine learning.\n\nSentiment analysis and opinion mining is out of its earlier stages and there is a strong \nneed to standardize the datasets and evaluation methodology (Schouten and Frasincar \n2015). Accuracy, area under the curve, precision/recall and F-measure are frequently \nused for evaluation. The bagof-words approach do not contain information about con-\ntext and proximity and therefore, needs to be replaced with concept-centric approach. \nIn survey (Cambria and White 2014) the need for bag-of-concept is emphasized and \neven bag-of-narratives was suggested. Sentiments also need to be contextualized and \nconceptualized (Gangemi et al. 2014). The work of Weichselbraun et al. (2014) is a step-\nping stone towards contextualizing sentiment analysis by integrating different semantic \nrepositories i.e. WordNet, SentiWordNet, WordNetAffect etc. It also help to distinguish \namong specific aspects that were previously studied in isolation. SentiWordNet has low \nstrength sentiments that are not contributing positively (Tsai et al. 2013).\n\nThe techniques developed for sentiment analysis needs to focus on the type of sup-\nporting application as they have different content style. Microblogging (twitter) and \ntranscribed text is unstructured having more noise and therefore, lexicon-based tech-\nniques do not perform well (Katz et al. 2015). Similarly depending upon the nature of \nplatform structural information can also be incorporated e.g. likes, share, retweets, \nhashtags etc. Ofek (2014) showed drop in accuracy when twitter data was used instead \nof Wall street journal content, even after including emoticons and hashtags (Ofek 2014). \nMachine learning techniques are more supportive to accommodate structural informa-\ntion e.g. meta-data as non-textual features (Katz et al. 2015). ML techniques depend on \nthe feature set to which proximity and context based features can also be added. Tran-\nscribed text is used in Takeuchi and Yamaguchi (2014) and Cailliau and Cavet (2013) \nintroducing new type of textual content. It also contain terms like “Emm” and “Aah” etc. \nthat doesn’t have any meaning but are used while speaking. Similarly sentences are left \nincomplete and grammar is ignored. This opens new avenues to these techniques to deal \nwith this type of content.\n\nApplication areas\nPreviously if customers want to know about something, they would ask their friends and \nfamily while businesses would conduct surveys and polls. Sentiment analysis applica-\ntions have spread to almost every possible domain, from consumer products, services, \nhealth care, and financial services to social events and political elections etc. Customers \nmay analyze the feedback of various features of the product given by other customers in \na way that would help in decision making. The sentiment analysis outcome of products \nand its features can be compared for competing products. Jaafar et al. (2015) consider \nbig social data analysis as a concern for search engines and industries.\n\nAn application having opinion reason mining could be more helpful for both custom-\ners and companies towards making a sound decision. In opinion reason mining, not only \nthe opinions about aspects are extracted but reason of the opinion is also extracted. It \nfurther helps manufacturers as their problems are highlighted. In (Zhang and Skiena \n\n\n\nPage 16 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\n2010; Sakunkoo and Nathan 2009) expert investors use twitter moods to predict stock \nmarket. Blog and news sentiment were used to study trading strategies (Groh and Jan \n2011). In (Stoyanov and Claire 2006) social influences in online book reviews were \nstudied. Sentiment analysis is used to characterize social relations (Akkaya et al. 2009). \n(Mohammad and Tony 2011; Mohammad 2011) worked with emotion analysis on vari-\nous sources. This is a very interesting study that can help to find what male and female \ncustomers look for, in a certain product that can be focused on.\n\nOpinion mining on social media can have applications to rank celebrities, sportsmen \nand championships based on their popularity in public. It can be used to find the popu-\nlarity of politicians prior to election etc. Influence analysis is performed in Nguyen et al. \n(2015) while Rabade et al. (2014) has discussed different influence indicators. There are \nvery useful applications of opinion mining available that may be used online for finding \nthe orientation of a text. Some of the notable ones are online message sentiment filter-\ning, e-mail sentiment classification, web blog author’s attitude analysis etc. Data leakage \nanalysis is an emerging area that can be focused on in security systems (Katz et al. 2014).\n\nConclusion\nOpinion mining has its boundaries extended from computer science to management sci-\nences. Sentiment analysis, though recently introduced as in research focus for commer-\ncial and social content. A detailed analysis of the problem through ML based techniques \nhas made it clear that SA and NLP has many open issues that are beyond the control of \nthe methods in practice. Having close relevance to NLP, sentiment analysis faces NLP \nissues like co-reference resolution, negation handling, and word sense disambiguation \netc., which add more difficulties due to their variation. However, it is also useful to real-\nize that sentiment analysis is a highly restricted NLP problem because the system does \nnot need to fully understand the semantics of each and every word. Complex network \nanalysis has been popularly used for various problems and can produce useful patterns \nin subjective text. Knowledge-bases systems incorporate domain specific guidance from \na knowledge source to improve results in specialized domains. More ML based solutions \nproposed, however, there is a strong need for considering solutions coming from dif-\nferent research domains. Machines generated data needs to be considered as meta-data \nalong the content dimension for many useful purposes.\nAuthors’ contributions\nMTK performed critical analysis on the data, drafted the manuscript and concluded the concepts presented. II helped \nwith data acquisition about machine learning sentiment analysis techniques and drawing important interpretations \nfrom it. MD identified the key challenges in Natural Language processing and categorized them based on the level \nwhere they tend to occur. AA carried out a study to investigate applications of Sentiment analysis and how they can \naffect the future of business intelligence. SK supervised the whole activity, helped in drafting and revised the whole \ndocument critically for proof of concepts. KHK helped in improving the quality of content with the revised version of the \nmanuscript. All authors read and approved the final manuscript.\n\nAuthor details\n1 Bahria University, Shangrilla Road, Sector E-8, Islamabad, Pakistan. 2 COMSATS Institute of Information Technology, \nKamra Road, Attock, Pakistan. 3 University of Malaya, Kuala Lumpur, Malaysia. 4 University of Haripur, Hattar Road, Haripur, \nPakistan. \n\nAcknowledgments\nWe would like to thank Bahria University, Islambad for providing the necessary environment and support to carry out this \nwork.\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\n\n\nPage 17 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nReceived: 1 November 2015 Accepted: 18 January 2016\n\nReferences\nAkkaya Cem, Janyce Wiebe, Rada Mihalcea (2009) Subjectivity word sense disambiguation. In: Proceedings of the 2009 \n\nConference on Empirical Methods in Natural Language Processing EMNLP\nAna C, Arlindo LO (2007) Semi-supervised single-label text categorization using centroid-based classifiers. ACM 844–851\nAoyama M (2002) A business-driven web service creation methodology, saint-w. IEEE\nBar-Haim R, Dinur E, Feldman R, Fresko M, Goldstein G (2011) Identifying and following expert investors in stock micro-\n\nblogs, In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2011)\nBasu A et al (2003) Support vector machines for text categorization. In: Proceedings of the IEEE Hawaii International \n\nconference on system sciences\nBrown MPS et al (1997) Support vector machine classification of microarray gene expression data. In: Proceedings of the \n\nNational academy of science, p 262–267\nCailliau F, Cavet A (2013) Mining automatic speech transcripts for the retrieval of problematic calls. Comput Linguist Intell \n\nText Process 83–95\nCambria E, Schuller B, Xia Y, Havasi C (2013) New avenues in opinion mining and sentiment analysis. IEEE Intell Syst \n\n2:15–21\nCambria E, White B (2014) Jumping NLP curves: a review of natural language processing research [review article]. Com-\n\nput Intell Mag IEEE 9(2):48–57\nChau M, Xu J (2007) Mining communities and their relationships in blogs: a study of online hate groups. Int J Hum \n\nComput Stud 65(1):57–70\nChizi B, Rokach L, Maimon O (2009) A survey of feature selection techniques. Encyclopedia of data warehousing and \n\nmining. 1888–1895\nChuang W, Tiyyagura A, Yang J, Giuffrida G (2000) A fast algorithm for hierarchical text classification. In: 2nd International \n\nConference on Data Warehousing and Knowledge Discovery, p 409–418\nDai Qingliang Miao Qiudan Li Ruwei (2009) AMAZING: a sentiment mining and retrieval system. Expert Syst Appl \n\n36:7192–7198\nDai W et al (2007) Transferring Naive Bayes Classifiers for Text Classification. In: Proceedings of the Twenty-Second AAAI \n\nConference on Artificial Intelligence 2007\nDing X, Liu B (2007) The utility of linguistic rules in opinion mining. In: Proceedings of the 30th Annual International ACM \n\nSIGIR Conference on Research and Development in Information Retrieval, Amsterdam, The Netherlands\nDing X, Liu B (2010) Resolving object and attribute conference in opinion mining, COLING 2010. In: 23rd International \n\nConference on Computational Linguistics, Proceedings of the Conference 2010\nFeldman R et al (2011) The stock sonar-sentiment analysis of stocks based on a hybrid approach. In: Twenty-Third IAAI \n\nConference\nGangemi A, Presutti V, Recupero RD (2014) Frame-based detection of opinion holders and topics: a model and a tool. \n\nComput Intell Magaz IEEE 9(1):20–30\nGroh G, Hauffa J (2011) Characterizing social relations via NLP based sentiment analysis. In: Proceedings of the 5th Inter-\n\nnational AAAI Conference on Weblogs and Social Media ICWSM, 2011\nGuan H, Zhou J, Guo M (2009) A class-feature-centroid classifier for text categorization, In Proceedings of the 18th inter-\n\nnational conference on World wide web Madrid. 2009\nGuellil I, Boukhalfa K (2015) Social big data mining: a survey focused on opinion mining and sentiments analysis. In: \n\nProgramming and Systems (ISPS), 2015 12th International Symposium IEEE, p 1–10\nHai Z, Chang K, Kim JJ, Yang CC (2014) Identifying features in opinion mining via intrinsic and extrinsic domain relevance. \n\nIEEE Trans Knowl Data Eng 26(3):623–634\nHan EH and Karypis G (2000) Analysis and experimental results. In: Principles of Data Mining and Knowledge Discovery, \n\nproceedings of the 4th European conference on centroid based document classification, p 424–431\nHidayet T, Tunga G (2012) A high performance centroid-based classification approach for language identification. Pattern \n\nRecognit Lett 33:2077–2084\nHu G, Jingyu Z, Minyi G (2009) A class-feature-centroid classifier for text categorization. ACM 201–210\nHull D (1994) Improving text retrieval for the routing problem using latent semantic indexing. In: Proceedings of SIGIR-94, \n\np 282–289\nJaafar N, Al-Jadaan M, Alnutaifi R (2015) Framework for social media big data quality analysis. In New Trends in Database \n\nand Information Systems II. Springer International Publishing, p 301–314\nJindal N, Liu B (2006) Mining comparative sentences and relations. AAAI Press, Menlo Park, pp 1331–1336\nJoachims T (1998) Text categorization with support vector machines: learning with many relevant features. In: Proceed-\n\nings of the European conference of machine learning\nKamps J, Marx M, Mokken RJ, De Rijke M (2004) Using WordNet to measure semantic orientation of adjectives. In: Pro-\n\nceedings of 4th International Conference on Language Resources and Evaluation, p 1115-1118\nKatz G, Elovici Y, Shapira B (2014) CoBAn: a context based model for data leakage prevention. Inform Sci 262:137–158\nKatz G, Ofek N, Shapira B (2015) ConSent: context-based sentiment analysis. Knowl Syst 84:162–178\nKhan MT, Khalid S (2015) Sentiment Analysis for Health Care. Int J Priv Health Inform Manag 3(2):78–91. doi:10.4018/\n\nIJPHIM.2015070105\nLertnattee V, Theeramunkong T (2004) Effect of term distributions on centroid-based text categorization. Inf Sci \n\n158(1):89–115\nLertnattee V, Theeramunkong T (2006) Class normalization in centroid-based text categorization. Inf Sci \n\n176(12):1712–1738\n\nhttp://dx.doi.org/10.4018/IJPHIM.2015070105\nhttp://dx.doi.org/10.4018/IJPHIM.2015070105\n\n\nPage 18 of 19Khan et al. Complex Adapt Syst Model (2016) 4:2 \n\nLi B, Yu S, Lu Q (2003) An improved k-nearest neighbor algorithm for text categorization. CoRR 0306099\nLim EP, Nguyen VA, Jindal N, Liu B, Lauw HW (2010) Detecting product review spammers using rating behaviors. In: \n\nProceedings of ACM International Conference on Information and Knowledge Management CIKM\nMachova K, Marhefka L (2014) Opinion classification in conversational content using N-grams. In: Recent Developments \n\nin Computational Collective Intelligence, Springer International Publishing, p 177–186\nMedhat W, Hassan A, Korashy H (2014) Sentiment analysis algorithms and applications: a survey. Ain Shams Eng J \n\n5(4):1093–1113\nMihalcea R, Strapparava C (2009) The lie detector: Explorations in the automatic recognition of deceptive language, In: \n\nProceedings of the ACL-IJCNLP 2009\nMohammad S (2011) Once upon a time to happily ever after: tracking emotions in novels and fairy tales. In: Proceedings \n\nof the ACL 2011 Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) \n2011\n\nMohammad SM, Yang TW (2011) Tracking sentiment in mail: how genders differ on emotional axes. In: Proceedings of \nthe ACL Workshop on ACL 2011, Workshop on Computational Approaches to Subjectivity and Sentiment Analysis \nWASSA 2011\n\nMukherjee A et al (2011) Detecting group review spam. ACM 93-94\nMukherjee A, Liu B (2012) Spotting fake reviewer groups in consumer reviews. ACM\nMukherjee A et al (2011) Detecting group review spam. ACM, p 93–94\nNarayanan R et al (2009) Sentiment analysis of conditional sentences. ACL, p180–189\nNguyen DT, Hwang D, Jung JJ (2015) Time-frequency social data analytics for understanding social big data. In Intelligent \n\nDistributed Computing VIII. Springer International Publishing 223–228\nNiazi M, Hussain A (2009) Agent-based tools for modeling and simulation of selforganization in peer-to-peer, ad hoc, and \n\nother complex networks. IEEE Commun Magaz 47(3):166–173\nNiazi M and Hussain A (2011) Social network analysis of trends in the consumer electronics domain, consumer electron-\n\nics (ICCE) 2011. IEEE international conference, p 219–220\nNiazi MA, Siddique Q, Hussain A, Kolberg M (2010) Verification and validation of an agent-based forest fire simulation \n\nmodel. In: Proceedings of the 2010 Spring Simulation Multiconference\nOfek N, Rokach L, Mitra P (2014) Methodology for connecting nouns to their modifying adjectives. In Comput Linguist \n\nIntell Text Process 271–284\nOtt M, Choi Y, Cardie C, Hancock JT (2011) Finding deceptive opinion spam by any stretch of the imagination. In: Pro-\n\nceedings of the 49th Annual Meeting of the Association for Computational Linguistics ACL\nOzgur L, Gungor T (2009) Text classification with the support of pruned dependency patterns. Pattern Recognit Lett. \n\n31:1598–1607\nPang B, Lee L (2008) Opinion mining and sentiment analysis. Fund Trends Inf Ret 2:1–2\nPatella M, Ciaccia P (2009) Approximate similarity search: a multi-faceted problem. J Discrete Algorithms 7:36–48\nPoria S, Gelbukh A, Hussain A, Howard N, Das D, Bandyopadhyay S (2013) Enhanced SenticNet with affective labels for \n\nconcept-based opinion mining. IEEE Intell Syst 2:31–38\nQiu G et al (2011) Opinion word expansion and target extraction through double propagation. Comput Linguist l37:9–27\nRabade R, Mishra N, Sharma S (2014) Survey of influential user identification techniques in online social networks. In: \n\nRecent Advances in Intelligent Informatics. Springer International Publishing, p 359–370\nSakunkoo P, Sakunkoo N (2009) Analysis of social influence in online book reviews, Proceedings of 3rd International AAAI \n\nConference on Weblogs and Social Media ICWSM, 2009\nSchouten K, Frasincar F (2015) Survey on aspect-level sentiment analysis. 99\nShankar S, Karypis G (2000) Weight adjustment schemes for a centroid based classifier, Army High Performance Comput-\n\ning Research Center\nShin K, Abraham A, Han S (2006) Improving kNN text categorization by removing outliers from training set. Comput \n\nLinguis Intell Text Process 3878:563–566\nSomprasertsri G, Latitrojwong P (2010) Mining feature-opinion in online customer reviews for opinion summarization. \n\n16:938–955\nSoucy P, Mineau GW (2001) A simple K-NN algorithm for text categorization. In: Proceedings of ICDM-01, IEEE Interna-\n\ntional Conference on Data Mining, p 647–648\nSreemathy J, Balamurugan PS (2012) An efficient text classification using K-NN and Naive Bayesian. Engg Journals Publi-\n\ncations, London\nStoyanov V, Cardie C (2006) Partially supervised conference resolution for opinion summarization through structured rule \n\nlearning. In: Proceedings of Conference on Empirical Methods in Natural Language Processing EMNLP 2006\nTakeuchi H, Yamaguchi T (2014) Text mining of business-oriented conversations at a call center. In Data Mining for \n\nService, p 111–129\nTan S (2007a) Large margin dragpushing strategy for centroid text categorization. Expert Syst Appl 33(1):215–220\nTan S (2007b) An improved centroid classifier for text categorization. Expert Syst Appl 35(1–2):279–285\nTan S, Cheng X, Ghanem M, Wang B, Xu H (2005) A novel refinement approach for text categorization. CIKM 469-476\nTang J, Chang Y, Liu H (2014) Mining social media with social theories: a survey. ACM SIGKDD Explor Newslett 15(2):20–29\nTang J, Hu X, Liu H (2013) Social recommendation: a review. Soc Netw Anal Min 3(4):1113–1133\nTsai ACR, Wu CE, Tsai RTH, Hsu JYJ (2013) Building a concept-level sentiment dictionary based on commonsense. Knowl \n\nIEEE Intell Syst 2:22–30\nTuveri F, Angioni M (2014) An opinion mining model for generic domains distributed systems and applications of infor-\n\nmation filtering and retrieval. Springer,", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwMjk0LTAxNi0wMDE2LTkucGRm0", "metadata_author": "Muhammad Taimoor Khan", "metadata_title": "Sentiment analysis and the complex natural language", "metadata_creation_date": "2016-02-02T06:54:52Z", "keyphrases": [ "complex natural language", "Sentiment analysis" ] }, { "@search.score": 1, "content": "\nMobile marketing recommendation method \nbased on user location feedback\nChunyong Yin1 , Shilei Ding1 and Jin Wang2*\n\nIntroduction\nIn recent years, the e-commerce industry has developed rapidly with the popularization \nof the Internet. At this time, famous e-commerce platforms such as Alibaba and Ama-\nzon were born. E-commerce moved physical store products to a virtual network plat-\nform. On the one hand, it is convenient for users to buy various products without leaving \nthe home. On the other hand, it is also convenient for sellers to sell their own goods \nand reduce costs. However, the various products have made it more difficult for users \nto select products. E-commerce platform can generate a large amount of user location \nfeedback data which contains a wealth of user preference information [1]. It is significant \nto predict the location of the next consumer’s consumption from these behavioral data. \nAt present, most of the recommended methods focus on the user-product binary matrix \nand directly model their binary relationships [2]. The users’ location information and \nshopping location information are considered as the third factor. In this case, you can \nonly use the limited check-in data. The users’ location feedback behavior and the timeli-\nness of behavior are often overlooked.\n\nThe mobile recommendation system takes advantage of the mobile network environ-\nment in terms of information recommendation and overcomes the disadvantages. Filter-\ning irrelevant information by predicting potential mobile user preferences and providing \n\nAbstract \n\nLocation-based mobile marketing recommendation has become one of the hot spots \nin e-commerce. The current mobile marketing recommendation system only treats \nlocation information as a recommended attribute, which weakens the role of users and \nshopping location information in the recommendation. This paper focuses on location \nfeedback data of user and proposes a location-based mobile marketing recommenda-\ntion model by convolutional neural network (LBCNN). First, the users’ location-based \nbehaviors are divided into different time windows. For each window, the extractor \nachieves users’ timing preference characteristics from different dimensions. Next, we \nuse the convolutional model in the convolutional neural network model to train a \nclassifier. The experimental results show that the model proposed in this paper is better \nthan the traditional recommendation models in the terms of accuracy rate and recall \nrate, both of which increase nearly 10%.\n\nKeywords: Location feedback, Mobile marketing, Convolutional neural network, \nSequential behavior\n\nOpen Access\n\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat iveco mmons .org/licen ses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.\n\nRESEARCH\n\nYin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \nhttps://doi.org/10.1186/s13673-019-0177-6\n\n*Correspondence: \njinwang@csust.edu.cn \n2 School of Computer & \nCommunication Engineering, \nChangsha University \nof Science & Technology, \nChangsha 410004, China\nFull list of author information \nis available at the end of the \narticle\n\nhttp://orcid.org/0000-0001-5764-2432\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s13673-019-0177-6&domain=pdf\n\n\nPage 2 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nmobile users with results that meet users’ individual needs gradually become an effec-\ntive means to alleviate “mobile information overload” [3]. Mobile users have different \npreferences in different geographical locations. For this problem, how to use location \ninformation to obtain mobile users’ preferences and provide accurate personalized \nrecommendations has become a hot topic in mobile recommendation research [4]. \nAlthough there are many researches based on location recommendation, they mainly \nfocus on service resources without positional relevance. To solve the shortcomings of \nresearch on location relevance of service resources is few [5], Zhu et  al. [6] proposed \nthe method which is based on the user’s context information to analyze the user’s pref-\nerences and retrograde. Their approach is to derive user preferences by proposing two \ndifferent assumptions and then recommending user models based on preference analy-\nsis. Yin et al. [7] proposed LA-LDA. The method is a location-aware based generation \nprobability model, which uses scoring based on location to model user information and \nrecommend to users. However, these methods only treat location information as an \nattribute without considering the spatial information of users or items and weaken loca-\ntion information’s role in the recommendation. There are some studies determine user \npreferences by the distance between the mobile user and the merchant [8], but only set \nthe area based on the proximity of the distance and ignore the spatial activities of the \nmobile user [9]. However, these methods were limited to the analysis of user informa-\ntion and product information, and did not carefully consider the importance of user and \nbusiness location information. Therefore, the user preference model based on location \nrecommendation they created has some gap.\n\nConsidering the core of mobile marketing recommendation is location movement, \nLian et al. [10] proposed an implied feature-based cognitive feature collaborative filter-\ning (ICCF) framework, which avoids the impact of negative samples by combining con-\nventional methods and semantic content. In terms of algorithms, the author proposed \nan improved algorithm that can expand according to data size and feature size. To deter-\nmine the relevance of the project to user needs, Lee et al. [11] developed context infor-\nmation analysis and collaborative filtering methods for multimedia recommendations in \nmobile environments. Nevertheless, these methods only used small-scale training data \nand could not achieve accurate prediction of long-term interest for users. In this paper, \ndeep learning and time stamps are used to compensate for these shortcomings.\n\nWith great achievements in visual and speech tasks, the Deep Learning (DL) model \nhas become a novel field of study [12]. Because of the interventional optimization of \ndeep learning algorithms, artificial intelligence has made great breakthroughs in many \naspects. It is well known that models obtained through deep learning and machine learn-\ning models have very similar effects, which learns advanced abstract features from the \noriginal input features by simulating the network structure of the human nervous sys-\ntem. Experiments show that the deep model can express the characteristics of the data \nbetter than the shallow model [13]. Weight sharing by convolution makes CNN similar \nto biological neural networks, which reduces the difficulty of network structure and the \nnumber of weights. The structure of CNN is roughly divided into two layers. It is well \nknown that the first layer is a convolutional layer. Each neuron’s input is connected to the \nprevious layer through a convolution kernel and the local features are extracted. Next \nlayer is a pooling layer. In this layer, the neurons in the network are connected through \n\n\n\nPage 3 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\na convolution kernel to extract the overall features. Convolutional neural networks have \ngreat advantages in processing two-dimensional features [14], such as images.\n\nBased on our detailed comparative analysis, this paper proposes a location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN). \nFirstly, we use user-product information as a training sample, and treat this problem as \na two-class problem. The category of the problem is divided into the purchase behav-\nior and the purchase behavior of the product at the next moment. In order to capture \nthe user’s timing preference characteristics, we divide the behavior of the merchandise \naccording to a certain length of time window and dig deeper into the behavior charac-\nteristics of each time window. Secondly, we consider the users’ timing preferences and \noverall preferences for the product. Then, the features of time window are used to train \nconvolutional neural network models. Finally, we input the sample features of the test \nset into the model and generate the Top-K sample as the location-based purchase fore-\ncast results [15].\n\nRemain of the paper is divided into four sections. Related work is shown in “Related \nwork” section. Necessary definitions and specific implementation of the location-based \nmobile marketing recommendation model by convolutional neural network (LBCNN) \nare shown in “Location-based mobile marketing recommendation model by CNN” sec-\ntion. In “Experimental analysis” section, experimental analysis is introduced. “Conclu-\nsion” section summarizes the strengths and weaknesses of the paper and proposes plans \nfor future progress.\n\nRelated work\nIn the current chapter, we will review existing methods for recommending systems \nthat can be broadly divided into three parts: content filtering, collaborative filtering \nand hybrid methods. We also discuss the establishment of feature models based on \ntime series to clearly represent the differences between our research and other existing \nmethods.\n\nTraditional recommendation method\n\nIn the general products recommendation system, the similarity between users is calcu-\nlated by the user’s interest feature vector. Then, the system recommends some products \nwith similarity greater than a certain threshold or the similar Top-N products to the tar-\nget user. This is a traditional recommendation algorithm based on content and the rec-\nommendation is based on comparing users.\n\na. Content‑based recommendation method\n\nContent-based information filtering has proven to be an effective application for \nlocating text documents related to topics. In particular, we need to focus on the \napplication of content-based information filtering in the recommendation system. \nContent-based methods allow for accurate comparisons between different texts \nor projects, so the recommended results are similar to the historical content of the \nuser’s consumption. The content-based recommendation algorithm involves the fol-\nlowing aspects. User description file describes the user’s preferences, which can be \nfilled by the user and dynamically updated based on the user’s feedback information \n\n\n\nPage 4 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\n(purchasing, reading, clicking, etc.) during the operation of the system. The project \nprofile describes the content characteristics of each project, which constitutes the \nfeature vector of the project. In addition, the similarity calculation is the similarity \nbetween the user’s description file and the item feature vector.\n\nThe similarity calculation of the content-based recommendation algorithm usually \nadopts the cosine similarity algorithm. The algorithm needs to calculate the similarity \nbetween the feature vector of user u and the feature vector of item i. The calculation \nformula is as shown in Formula (1).\n\nwhere ⇀u denotes the user feature vector, \n⇀\n\ni denotes the project feature vector, \n⇀\n\n|u| is the \nmodulus of the user feature vector and \n\n⇀\n\n|i| is the model of the project feature vector.\nRepresentative content-based recommendation systems mainly include Lops, \n\nGemmis, and Semeraro [16]. Compared to other methods, content-based recom-\nmendations have no cold-start issues and recommendations are easy to understand. \nHowever, the content filtering based recommendation method has various draw-\nbacks, such as strongly relying on the availability of content and ignoring the context \ninformation of the recommended party. The content-based recommendation method \nalso has certain requirements for the format of the project. Besides, it is difficult to \ndistinguish the merits of the project. The same type of project may have the same type \nof features, which are difficult to reflect the quality of the project.\n\nb. Collaborative filtering method\n\nThe recommendation based on collaborative filtering solves the recommendation \nproblem by using the information of similar users in the same partition to analyze and \nrecommend new content that has not been scored or seen by the target user.\n\nRegarding the traditional collaborative filtering method based on memory, we \nunderstand that this method is based on the different relationships between users and \nprojects. According to expert research, the traditional collaborative filtering method \nbased on memory should be divided into the following three steps.\n\nStep 1: collection of user behavior data, this step represents the user’s past behav-\nior with a m * n matrix R. The matrix Umn represents the feedback that the user m \nhas on the recommended object n. Rating is a range of values and different values \nrepresent how much the user likes the recommended object.\n\nStep 2: establishment of a user neighbor: establish mutual user relationships by \nanalyzing all user historical behavior data.\n\n(1)sim(u, i) =\n\n⇀\nu ·\n\n⇀\n\ni\n\n⇀\n\n|u|\n⇀\n\n|i|\n\nU =\n\n\n\n\n\n\n\nU11 U12 . . . U1n\n\nU21 U22 . . . U2n\n\n. . . . . . . . . . . .\n\nUm1 Um2 . . . Umn\n\n\n\n\n\n\n.\n\n\n\nPage 5 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nStep 3: generate recommendation results: find the most likely N objects from the rec-\nommended items selected by similar user sets.\n\nTherefore, recommendations are made by mining common features in similar users’ pref-\nerence information [17]. The normal methods in this classification include k-nearest neigh-\nbor (k-NN), matrix decomposition, and semi-supervised learning. According to the survey, \nAmazon uses an item-by-item collaborative filtering method to recommend personalized \nonline stores for each customer.\n\nCompared to other method, collaborative filtering has the ability to filter out informa-\ntion that can be automatically recognized by the machine and effectively use feedback from \nother similar users. However, collaborative filtering requires more ratings for the project, \nso it is affected by the issue of rating sparsity. In addition, this method does not provide a \nstandard recommendation for new users and new projects, which is called a cold start issue.\n\nc. Hybrid recommendation method\n\nThe hybrid recommendation method combines the above techniques in different ways to \nimprove the recommended performance and optimize the shortcomings of the conven-\ntional method. Projects that cannot be recommended for collaborative filtering are gener-\nally addressed by combining them with content-based filtering [18].\n\nThe core of this method is to independently calculate the recommendation results of the \ntwo types of recommendation algorithms, and then mix the results. There are two specific \nhybrid methods. One method is to mix the predicted scores of the two algorithms linearly. \nAnother hybrid method is to set up an evaluation standard, compare the recommended \nresults of the two algorithms, and take the recommendation results of the higher evaluation \nalgorithms. In general, the hybrid recommendation achieves a certain degree of compensa-\ntion between different recommendation algorithms. However, the hybrid recommendation \nalgorithm still needs improvement in complexity.\n\nd. Recommendation based on association rules\n\nThe association rule algorithm is a traditional data mining method that has been widely \nused in business for many years. The core idea is to analyze the rules of user historical \nbehavior data to recommend more similar behavioral items [19]. Rules can be either user-\ndefined or dynamically generated by using rule algorithms. The effect of the algorithm \ndepends mainly on the quantity and quality of the rules so the focus of the algorithm is on \nhow to develop high quality rules.\n\nDefine N as the total number of transactions, R is the total project and U and V are two \ndisjoint sets of items (U∩V ≠ ∅, U∈R, V∈R). The association rule is essentially an IF–Then \nstatement, here is expressed by U → V. The strength of the association rule U → V can be \nmeasured by two criteria: support and confidence. S is the ratio containing U and V data \nwhich both represent the number of transactions, which is shown in Formula (2).\n\nC is the ratio of U, V data to the only U data which represents the number of transac-\ntions, as shown in Formula (3)\n\n(2)S(U → V ) =\nN (U ∪ V )\n\nN\n.\n\n\n\nPage 6 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nThe recommendation process of the algorithm is shown in below.\nFirstly, according to the items of interest to the user, the user’s interest in other \n\nunknown items is predicted by rules. Secondly, compare the support of the rules. Finally, \nthe recommended items of TOP-N are obtained to the user.\n\nThe recommendation system based on association rules includes three parts: the key-\nword, the presentation and the user interface. The keyword layer is a set of keyword \nattributes and dependencies between keywords. The description layer connects the \nkeyword layer and the user layer and the main function is to describe the user and the \nresource. The user interface layer is the layer that interacts directly with the user. How-\never, the system becomes more and more difficult to manage as the rules increasing. In \naddition, there is a strong dependence on the quality of the rules and a cold start prob-\nlem is existed.\n\nMost of the recommendation systems use collaborative filtering algorithm to recom-\nmend for users. However, the traditional algorithm can only analyze ready-made data \nsimply, and most systems simply preprocess the data. In our method, we preprocess the \ndataset by extending the time information of the data to a time label. The next section is \nan explanation of the specific implementation.\n\nConstruction of time series behavior’s preference features\n\nThe timing recommendation model is based primarily on the Markov chain. This model \nmakes full use of timing behavior data to predict the next purchase behavior based on \nthe user’s last behavior. The advantage of this model is that it can generate good recom-\nmendations by timing behavior.\n\nAs shown in Fig. 1, the prediction problem of product purchase can be expressed as \npredicts the user’s purchase behavior at time T by a user behavior record set D before \ntime T [20]. Different actions occur at different times. For example, user1 visit location \na and b when user1 purchasing b and c at T − 3. We need to predict T-time consumer \nbehavior based on different timing behavior characteristics.\n\nAccording to relevant professional research, we divide the data sets of user behav-\nior into three groups in a pre-processing manner. By the feature statistics method, the \n\n(3)C(U → V ) =\nN (U ∪ V )\n\nN\n.\n\nFig. 1 The time series of user position feedback\n\n\n\nPage 7 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nfeatures are divided into two types, as shown in Table 1. “True” indicates that the feature \ngroup has corresponding features. Conversely, “False” means no such feature. Next we \nexplain these features.\n\na. Counting feature\n\nFor each feature statistics window, we use the behavioral counting feature and the de-\nduplication counting feature. The behavior count is a cumulative measure of the num-\nber of behaviors that occurred in and before the current window. For the location visit \nbehavior, it represents the number of visits to the product location by the user, the total \nnumber of visits by the user and the total number of visits to the merchandise. The de-\nduplication count feature is similar to the behavioral count, but only the number of non-\nrepetitive behavioral data is counted.\n\nb. Mean feature\n\nIn order to describe the activity of the user and the popularity of the product better, \nthis article derives a series of mean-type features based on the counting features. Take \nthe location visit behavior as an example, the user characteristics group includes the \nuser’s average number of visiting to the product. The average number of visiting to \nthe product by user i is calculated as shown in Formula (4).\n\nc. Ratio feature\n\nThe ratio of user-product behavior to the total behavior of the user and the product \nis also an aspect affecting the user’s degree of preference for the product. In the time \nwindow t, the method to calculate the ratio of the user’s visit to the products’ total \nvisit is shown in Formula (5).\n\nOur work presents a mobile marketing recommendation model is trained by adding \nthe time axis to the user position features. Contrary to current research, it is highly \nusable and low difficulty of achievement for real-world work applications. Consider-\ning the speed of calculation, we study the method of directly embedding time series \ninformation into the collaborative filtering calculation process to improve the recom-\nmendation quality. Specific information will be covered in the following sections.\n\n(4)avgui(t, i, visit) =\naction_count(t,U ,Ui, visit)\n\nuser_unique_item(t,U ,Ui, visit)\n.\n\n(5)rate_ui_in_u(t, i, j, visit) =\naction_count(t,UI ,Ui, Ij, visit)\n\naction_count(t,U ,Ui, visit)\n.\n\nTable 1 Characteristic system diagram (True/False)\n\nFeature group Counting feature Mean feature Ratio feature\n\nUser-product True False True\n\nUser feature True True False\n\nProduct feature True True False\n\n\n\nPage 8 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nLocation‑based mobile marketing recommendation model by CNN\nCreating the model is one of the most important aspects, which is an evaluation crite-\nrion to make sure correctness of the next step. This section mainly describes the rel-\nevant definitions of LBCNN that are shown in “Relevant definitions of the LBCNN” \nsection, and specific implementation of the model is shown in “Specific implementa-\ntion of the model” section.\n\nRelevant definitions of the LBCNN\n\nIn order to get better feature expression, we consider the user’s timing sensitivity of the \nproduct preferences and the user’s overall preferences comprehensively. This paper uses a \nconvolutional neural network as the basis to build location-based mobile marketing recom-\nmendation model. In the next step, we give the relevant definition.\n\na. Definition 1 (Model framework): based on the above analysis and user’s timing behav-\nior preference feature. We use the convolutional neural network model shown in Fig. 2. The \nmodel is divided into four layers that are input layer, multi-window convolution layer, pool-\ning layer and output layer. The input layer is a well-constructed input feature which trans-\nforms the input features into a two-dimensional plane by time series. Each time window is \nexpressed as an eigenvector. The multi-window convolutional layer convolves the input fea-\nture plane through different lengths of time windows to obtain different feature maps. The \npooling layer reduces the dimension of the feature map to obtain a pooled feature vector. \nThe output layer and the pooling layer are fully connected network structures.\n\nb. Definition 2 (Convolution layer): assume that there are N time windows of the feature \nand each time window has K user preference feature for the commodity. Then input sam-\nple × can be expressed as a matrix of T × K. The feature map in the convolutional layer is \ncalculated by the input layer and the convolution kernel. The window length of the convolu-\ntion kernel is h. xi,i+j represents the eigenvector added by time window i and time window \ni + j. The convolution kernel w can be expressed as a vector of h × K. Feature map f = [f1, f2, \n…, fT−h+1]. The i-th feature fi is calculated according to Formula (6):\n\n(6)fi = σ(w · xi,i+h−1 + b)\n\nFig. 2 The framework of the LBCNN\n\n\n\nPage 9 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere b is an offset term and a real number. σ(x) is a nonlinear activation function. This \npaper uses ReLu and Tanh as an activation function. Relu is shown in Formula (7):\n\nc. Definition 3 (Max-pooling): the pooling layer is to scale the feature map while reduc-\ning the complexity of the network. The maximum features of the convolution kernel can \nbe obtained according to the maximum pooling operation. The feature map obtained \nat the kth product of the convolutional kernel is fk = [fk,1, fk,2, …, fk,T−h +1]. The pooling \noperation can be expressed as Formula (8):\n\nd. Definition 4 (Probability distribution): there are M convolution kernels and the output \nlayer has C categories [19]. The weight parameter θ of the output layer is a C × M matrix. \nThe pooled feature f̂ of x is an M-dimensional vector. The probability that x belongs to \nthe i-th category can be expressed as Formula (9):\n\nwhere bk represents the k-th offset of the fully connected layer. The loss function of the \nmodel can be obtained by the likelihood probability value, as shown in Formula (10):\n\nwhere T is the training data set, yi is the real category of the i-th sample, xi is the charac-\nteristic of the i-th sample and θ is the model’s parameters. We learn model parameters \nby minimizing the loss function. The training method adopts the improved gradient \ndescent method proposed by Zeiler. In addition, we have adopted Dropout process-\ning on the convolutional layer to prevent over-fitting of the trained model [21]. The \nDropout method randomizes the neurons in the convolutional layer to 0 with a certain \nprobability.\n\ne. Definition 5 (Latent factor): the value of the latent factor vector is true [22]. Whether \nan item belongs to a class is determined entirely by the user’s behavior. We assume that \ntwo items are liked by many users at the same time, then these two items have a high \nprobability of belonging to the same class. The weight of an item in a class can also be \ncalculated by itself. The implicit semantic model calculates the user’s (u) interest in the \nitem (i) are shown in Formula (11):\n\n(7)\nReLu = max(0, x).\n\nTanh(x) =\nex − e−x\n\nex + e−x\n.\n\n(8)Pool_feature(j) = down(fi).\n\n(9)p(i|x, θ) =\ne(θi·\n\n⌢\nf +bi)\n\n∑C\nk−1 e\n\n(θk ·\n⌢\nf +bk )\n\n(10)J (θ) = −\n\nk\n∑\n\ni=1\n\nlog(p(yi|x, θ))\n\n(11)R(u, i) = rui = pTu qi =\n\nF\n∑\n\nf=1\n\npu,kqi,k\n\n\n\nPage 10 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nwhere p is the relationship between the user interest and the kth implicit class. q is the \nrelationship between the kth implicit class and the item i. F is the number of hidden \nclasses, and r is the user’s interest in the item.\n\nSpecific implementation of the model\n\nWe can draw from Fig. 3 that the proposed model is divided into two processes. The first \nprocess is the training process and includes two parts. The top module shows how to gener-\nate CNN inputs and outputs from historical data. The other module in the training process \nshows that the traditional CNN parameters are trained by provided data. The second pro-\ncess finished a new location-based marketing resources recommendation. The recommen-\ndation process can work through the CNN parameters provided by the training process.\n\nTo achieve the features of users and location-based mobile marketing resources, the \nlatent factor model (LFM) is used. In traditional LFM, L2-norm regularization is often used \nto optimize training results. However, using L2-norm regularization often leads to excessive \nsmoothing problems. In our model, LFM results are used to represent the characteristics of \nthe training data. In this kind of thinking, we can learn from the training method of regres-\nsion coefficient in regression analysis, and construct a loss function. Therefore, it is more \nreasonable to use sparseness before the specification results. Based on these analyses, we \npropose an improved matrix decomposition method and try to normalize the solution by \n\nFig. 3 Location-based mobile marketing recommendation model by convolutional neural network\n\n\n\nPage 11 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nusing the premise of verifying the sparseness of the matrix. The model is presented as For-\nmula (12):\n\nThe next question is how to calculate these two parameters p and q. For the calculation \nof this linear model, this paper uses the gradient descent method. In the Formula (12), \n puk is a user bias item that represents the average of a user’s rating. qik is an item offset \nitem that represents the average of an item being scored. The offset term is an intrinsic \nproperty that indicates whether the item is popular with the public or a user is harsh \non the item. For positive samples, we specify ru,i = 1 based on experience and negative \nsample ru,i = 0, which is shown in Formula (11). The latter λ is a regularization term to \nprevent overfitting.\n\na. Description of the training section\n\nIn Fig. 3, If you want to train CNN, the first thing you need to solve is its input and out-\nput problems. For input, a language model is usually used.\n\nIn terms of output, we propose an improvement in model training by LFM, which is \nconstrained by the regularization of the L1-norm [23]. LFM training data is a historical \nscore between the user and the location-based marketing resources. The rating score can \nbe explicit because it is based on a user tag or an implied tag and it is predicted from the \nuser’s behavior. In this model, in order to ensure that the trained model is representative, \nthe training data we input is to select the existing authoritative standard training set.\n\nb. Description of the recommended part\n\nOnce the LBCNN model structure is established and the model parameters are trained \nusing the training data set, the recommended real-time performance can be achieved. \nThe real-time performance is based on the update of network model parameters in the \nbackground, and it uses some past behavior data and information of the recommended \npeople and products.\n\nUser information and product information can be obtained in advance and digitized. \nIn the offline training model phase, digitized user information, product information, and \nbehavior information are utilized [24]. The same model is trained for the same type of \nusers, and the parameters of the model are periodically updated within a certain period \nof time. In the real-time recommendation stage, real-time recommendation can be real-\nized only by integrating the collected behavior data with the previous data and inputting \nit into the model.\n\nExperimental analysis\nIn order to verify the advantages of convolutional neural network in capturing user’s \ntiming preferences for product and mining users’ temporal behavior characteristics, \nwe compare several commonly used classification models under the same conditions of \ntraining features. They are Linear Logistic Regression Classification Model (LR), Support \n\n(12)J (U ,V ) =\n∑\n\nu,i∈K\n\n(\n\nru,i −\n\nk\n∑\n\nk=1\n\npu,kqi,k\n\n)2\n\n+ ��puk�\n2 + ��qik�\n\n2.\n\n\n\nPage 12 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nVector Machine (SVM), Random Forest Model (RF) and Gradient Boosting Regression \nTree Model (GBDT) [25]. We also compare the products that have been visited for the \nlast 8 h. Experimental tool is sklearn kit. The hyper parameter settings for each model \nduring the experiment are:\n\na. LR: select L2 regular and the regularization coefficient is 0.1.\nb. SVM: choose radial basis kernel function (RBF) and gamma of kernel function is \n\n0.005.\nc. RF: the number of trees is 200, the entropy is selected as the feature segmentation \n\nstandard and the random feature ratio is 0.5.\nd. GBDT: the number of trees is 100, the learning rate is 0.1 and the maximum depth of \n\nthe tree is 3.\n\nDescription of the data set\n\nThe experiment in our paper uses the dataset disclosed according to the Alibaba Group’s \nmobile recommendation algorithm contest held in 2015. This data set contains 1 month \nof user behavior data and product information. The user’s behavior data includes 10 mil-\nlion users’ various behaviors on 2,876,947 items. Behavior types include clicks, shopping \ncarts and purchases. In addition, each behavior record identifies behavior time that is \naccurate to the hour. The product information includes product category information, \nand identifies whether the product is an online to offline type. In a real business sce-\nnario, we often need to build a personalized recommendation model for a subset of all \nproducts. In the process of completing this task, we not only need to take advantage of \nthe user’s behavior data on this subset of goods, but also need to use more abundant user \nbehavior data. We need to define the following symbols: U (User collection), I (Product \ncollection), P (Product subset, P ⊆ I), D (User behavior data collection for the complete \nset of products). Our goal is to use D to construct a recommendation model for users in \nU to products in P.\n\nThe data mainly consists of two parts. The first part is the mobile behavior data (D) of \n10 million users on the product collection, including the following fields, as shown in \nTable 2.\n\nFor example, “141278390, 282725298, 1, 95jnuqm, 5027, 2014-11-18 08” is one of \nthe data. The Behavior_type and the Time in these fields contain the largest amount \n\nTable 2 The mobile behavior data of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nUser_id User differentiation Sampling and data masking\n\nItem_id Product differentiation Data masking\n\nBehavior_type The type of behavior of the user on the \nproduct\n\nIncluding browsing, collecting, adding shop-\nping carts, and purchasing, the values are 1, 2, \n3, 4 respectively\n\nUser_geoinfo The spatial reference identifier of the user’s \nlocation\n\nFormed by latitude and longitude data through \na secret algorithm\n\nItem category Product classification identifier Field masking\n\nTime Action time Accurate to hour level\n\n\n\nPage 13 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nof information. The User_geohash field is basically unusable due to too many missing \nvalues.\n\nThe second part is the product subset (P), which contains the following fields, as \nshown in Table 3.\n\nSimilar to the above, “117151719, 96ulbnj, 7350” is one of the product information. \nThe training data contains the mobile behavior data (D) of a sample of a certain user \nwithin 1 month (11.18–12.18). The scoring data is the purchase data of the product sub-\nset (P) by these users 1 day (12.19) after this 1 month. We should be training the data \nmodel to output the predicted results of the user’s purchase behavior on the next day.\n\nData preprocessing\n\nWe found that there are some users have a lot of page views (maximum of 2 million), which \nis beyond reasonable levels. We analyze that these users may be crawler users, so the behav-\nior of these users on the goods is not the basis for predicting the user’s purchase. At the \nsame time, we predict the user product pairs that have appeared in all historical records. \nThe existence of these users will undoubtedly increase our forecasting amount and interfere \nwith our normal model training. Therefore, we choose to filter out these users, the filtering \nrules are shown as Fig. 4.\n\nTable 3 The product subset of the Ali mobile recommendation data set\n\nField Field description Extraction instruction\n\nItem_id Product differentiation Sampling and data masking\n\nItem_geohash Spatial information of the product location, \nwhich can be empty\n\nFormed by latitude and longi-\ntude data through a secret \nalgorithm\n\nItem_category Product classification identifier Data masking\n\nFig. 4 Data filtering rules\n\n\n\nPage 14 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nEvaluation index\n\nThe purpose of the proposed method is to predict the user’s purchased business in the next \nposition based on the user’s historical behavior record. Therefore, we evaluate the model \nwith the data of the last day. The sample construction of time series method is shown in \nFig.  1. F1-score can be viewed as a harmonic mean of accuracy and recall. At present, \nF1-score has been widely used in the evaluation of the recommendation system.\n\nwhere Formula (13) is the calculation method of the accuracy rate, Formula (14) is the \ncalculation method of the recall rate, and Formula (15) is the calculation method of \nF1-score. Prediction_set is the predicted purchase of the user-item. Answer_set is a real-\npurchased user-item collection.\n\nThe distribution of positive and negative samples used in this experiment is extremely \nunbalanced, and negative samples contain more noise. In order to make the model more \nsuitable for learning under unbalanced data, we perform under sampling on negative \nsamples. The model training process adopts AdaDelta Update Rule to adjust the param-\neters by using the stochastic gradient descent method. Hyper Parameters of the model \nare described in Table 4. The value in the table is the final hyper parameter when the \nerror of the validation set is minimal. Convolution time window in convolution kernel \nis 2 and 3. The number of convolution kernels for two different length windows is 200. \nIn this experiment, the training process needs to iterate ten times. To achieve the con-\nvergence of the model, we observe the accuracy of the training set every iteration in the \nmodel training process.\n\nIn Fig. 5, the abscissa indicates the number of iterations, and the ordinate indicates the \naccuracy of the sample. As we can see from the figure, the accuracy of the training set \nhas been increasing and the verification set accuracy has declined after the fifth iteration \nof the model [26]. This situation shows that the model training has been overfitting after \n\n(13)precision =\n\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n∣\n\n∣prediction_set\n∣\n\n∣\n\n(14)Recall =\n\n∣\n\n∣prediction_set ∩ answer_set\n∣\n\n∣\n\n|answer_set|\n\n(15)F1− score =\n2× precision× recall\n\nprecision+ recall\n\nTable 4 Parameter settings of convolutional neural networks\n\nParameter name Parameter value\n\nActivation function of convolution kernel Tanh\n\nSize of convolution kernel window [2, 3]\n\nNumber of convolution kernel 400\n\nDropout ratio 0.5\n\nBatch size 64\n\nEpoch 5\n\n\n\nPage 15 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nthe 5th iteration. In addition, we found that the test set accuracy is higher than the train-\ning set and verification set.\n\nExperimental results and comparison\n\nThe experimental results obtained using the above parameters are shown in Table 5. As \ncan be seen from Table  5, the machine learning model using the features designed in \nthis paper is superior to the traditional method. Our model achieves an 80% accuracy \nin predicting the accuracy of user behavior, which is significantly better than traditional \nmodels at least 10%. In terms of recall rate, LBCNN reached 8.14%, which is at least 2% \nhigher than the traditional method. Similarly, our model is up to 8.07% in F1-score.\n\nThis result shows that the user’s time-series behavior preference model is reasonable. \nThis solution works well for improving the accuracy and quality of recommendations. In \na single model, the LBCNN model works best. Since the linear model assumes that each \nfeature is independent, it is impossible to excavate the intrinsic relationship between \nfeatures. The proposed method can mine the intrinsic link between user timing prefer-\nence features better. The experimental results show that the user preferences we build \nare more accurate and convolutional neural networks have strong capabilities of feature \nextraction and model generalization.\n\nFig. 5 Training process of the LBCNN\n\nTable 5 Comparing the experimental results of each model (%)\n\nModel Accuracy Recall rate F1-score\n\nTraditional method 31.4 5.60 4.02\n\nLR 75.0 7.63 7.57\n\nSVM 70.0 7.12 7.06\n\nRF 57.5 5.85 5.80\n\nGBDT 62.5 6.36 6.13\n\nLBCNN 80.0 8.14 8.07\n\n\n\nPage 16 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\nConclusion\nThe current mobile marketing recommendation system only treats location informa-\ntion as a recommended attribute, which weakens the role of the location information in \nthe recommendation. For the implicit feedback behavior of users, this paper proposes \na location-based mobile marketing method by convolutional neural network. First, we \ndivide the user location-based behaviors into several time windows according to the \ntimestamp of these behaviors, and model the user preference in different dimensions \nfor each window. Then we utilize the convolutional neural network to train a classifier. \nFinally, the experimental process of this paper is introduced, and a good prediction effect \nis obtained on effective data sets. The final experimental results express that the pro-\nposed method has different feature extraction perspectives from other models. Because \nof using convolutional neural networks, the proposed method has stronger capability \nof feature extraction and generalization. This method helps to change the accuracy and \nquality of the recommendation system and user satisfaction.\n\nThe work introduced here is to show the prospects for further research. The method \nproposed in this paper has a certain dependence on the user’s geographical location \ninformation during the training process of the user preference model. In addition, the \nrecommendation system will encounter a cold-start problem with sparse user infor-\nmation. For dealing with these discovered issues, we plan to use the hot start case to \nimprove the recommended cold start problem. Meanwhile, we are investigating new \nmethod which uses a better big data framework (such as Hadoop MapReduce) to ensure \nthe efficiency of training large data sets. In the future, we will show recommended meth-\nods to improve performance in other applications.\nAuthors’ contributions\nCY conceptualized the study and analyzed all the data. SD performed all experiments and wrote the manuscript. JW \nadvised on the manuscript preparation and technical knowledge. All authors read and approved the final manuscript.\n\nAuthor details\n1 School of Computer and Software, Jiangsu Engineering Center of Network Monitoring, Nanjing University of Informa-\ntion Science & Technology, Nanjing 210044, China. 2 School of Computer & Communication Engineering, Changsha \nUniversity of Science & Technology, Changsha 410004, China. \n\nAcknowledgements\nIt was supported by the Priority Academic Program Development of Jiangsu Higher Education Institutions (PAPD), Post-\ngraduate Research & Practice Innovation Program of Jiangsu Province (KYCX18_1032).\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAvailability of data and materials\nWe declared that materials described in the manuscript will be freely available to any scientist wishing to use them for \nnon-commercial purposes.\n\nFunding\nThis work was supported by the National Natural Science Foundation of China (61772282, 61772454, 61811530332, \n61811540410).\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nReceived: 25 September 2018 Accepted: 4 April 2019\n\nReferences\n 1. Fernández-Tobías I, Braunhofer M, Elahi M, Ricci F, Cantador I (2016) Alleviating the new user problem in collabora-\n\ntive filtering by exploiting personality information. User Model User Adap Inter 26(2–3):221–255\n\n\n\nPage 17 of 17Yin et al. Hum. Cent. Comput. Inf. Sci. (2019) 9:14 \n\n 2. Koohi H, Kiani K (2016) User based collaborative filtering using fuzzy C-means. Measurement 91:134–139\n 3. Xu X, Fu S, Qi L, Zhang X, Liu Q, He Q, Li S (2018) An IoT-oriented data placement method with privacy preservation \n\nin cloud environment. J Netw Comput Appl 124:148–157\n 4. Yingyuan X, Pengqiang A, Ching-Hsien H, Hongya W, Xu J (2015) Time-ordered collaborative filtering for news \n\nrecommendation. China Commun 12(12):53–62\n 5. Kaminskas M, Ricci F (2011) Location-adapted music recommendation using tags. In: International conference on \n\nuser modeling, adaptation, and personalization. pp 183–194\n 6. Zhu H, Chen E, Xiong H, Yu K, Cao H, Tian J (2015) Mining mobile user preferences for personalized context-aware \n\nrecommendation. ACM Trans Intell Syst Technol TIST 5(4):58\n 7. Yin H, Cui B, Chen L, Hu Z, Zhang C (2015) Modeling location-based user rating profiles for personalized recommen-\n\ndation. ACM Trans Knowl Discov Data TKDD 9(3):19\n 8. Li X, Xu G, Chen E, Li L (2015) Learning user preferences across multiple aspects for merchant recommendation. In: \n\n2015 IEEE international conference on data mining (ICDM). pp 865–870\n 9. Yin C, Xi J, Sun R, Wang J (2018) Location privacy protection based on differential privacy strategy for big data in \n\nindustrial internet-of-things. IEEE Trans Industr Inf 14(8):3628–3636\n 10. Lian D, Ge Y, Zhang F, Yuan NJ, Xie X, Zhou T, Rui Y (2015) Content-aware collaborative filtering for location recom-\n\nmendation based on human mobility data. In: 2015 IEEE international conference on data mining. pp 261–270\n 11. Lee WP, Tseng GY (2016) Incorporating contextual information and collaborative filtering methods for multimedia \n\nrecommendation in a mobile environment. Multimedia Tools Appl 75(24):16719–16739\n 12. Bengio Y (2009) Learning deep architectures for AI. Found Trends Mach Learn 2(1):1–127\n 13. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444\n 14. Yuan C, Li X, Wu QJ, Li J, Sun X (2017) Fingerprint liveness detection from different fingerprint materials using convo-\n\nlutional neural network and principal component analysis. Comput Mater Continua 53(4):357–372\n 15. Yin C, Wang J, Park JH (2017) An improved recommendation algorithm for big data cloud service based on the trust \n\nin sociology. Neurocomputing 256:49–55\n 16. Längkvist M, Karlsson L, Loutfi A (2014) A review of unsupervised feature learning and deep learning for time-series \n\nmodeling. Pattern Recogn Lett 42:11–24\n 17. Tu Y, Lin Y, Wang J, Kim JU (2018) Semi-supervised learning with generative adversarial networks on digital signal \n\nmodulation classification. Comput Mater Continua 55(2):243–254\n 18. Nilashi M, Bin Ibrahim O, Ithnin N (2014) Hybrid recommendation approaches for multi-criteria collaborative filter-\n\ning. Expert Syst Appl 41(8):3879–3900\n 19. Zeng D, Dai Y, Li F, Sherratt RS, Wang J (2018) Adversarial learning for distant supervised relation extraction. Comput \n\nMater Continua 55(1):121–136\n 20. Yin C, Xia L, Zhang S, Sun R, Wang J (2018) Improved clustering algorithm based on high-speed network data \n\nstream. Soft Comput 22(13):4185–4195\n 21. Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet classification with deep convolutional neural networks. In: \n\nAdvances in neural information processing systems, pp 1097–1105\n 22. Li X, Yao C, Fan F, Yu X (2017) A text similarity measurement method based on singular value decomposition and \n\nsemantic relevance. J Inf Process Syst 13(4):863–875\n 23. Li CN, Shao YH, Deng NY (2015) Robust L1-norm two-dimensional linear discriminant analysis. Neural Networks \n\n65:92–104\n 24. Fattah MA (2017) A novel statistical feature selection approach for text categorization. J Inf Process Syst \n\n13(5):1397–1409\n 25. Wang Y, Feng D, Li D, Chen X, Zhao Y, Niu X (2016) A mobile recommendation system based on logistic regression \n\nand Gradient Boosting Decision Trees. In: International joint conference on neural networks. pp 1896–1902\n 26. Yin C, Zhang S, Xi J, Wang J (2017) An improved anonymity model for big data security based on clustering algo-\n\nrithm. Concurr Comput Pract Exp 29(7):e3902\n\n\n\tMobile marketing recommendation method based on user location feedback\n\tAbstract \n\tIntroduction\n\tRelated work\n\tTraditional recommendation method\n\ta. Content-based recommendation method\n\tb. Collaborative filtering method\n\tc. Hybrid recommendation method\n\td. Recommendation based on association rules\n\n\tConstruction of time series behavior’s preference features\n\ta. Counting feature\n\tb. Mean feature\n\tc. Ratio feature\n\n\n\tLocation-based mobile marketing recommendation model by CNN\n\tRelevant definitions of the LBCNN\n\tSpecific implementation of the model\n\ta. Description of the training section\n\tb. Description of the recommended part\n\n\n\tExperimental analysis\n\tDescription of the data set\n\tData preprocessing\n\tEvaluation index\n\tExperimental results and comparison\n\n\tConclusion\n\tAuthors’ contributions\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczEzNjczLTAxOS0wMTc3LTYucGRm0", "metadata_author": "Chunyong Yin ", "metadata_title": "Mobile marketing recommendation method based on user location feedback", "metadata_creation_date": "2019-04-05T10:16:31Z", "keyphrases": [ "Mobile marketing recommendation method", "user location feedback" ] }, { "@search.score": 1, "content": "\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 \nDOI 10.1186/s40493-015-0019-z\n\nRESEARCH Open Access\n\nToward a testbed for evaluating\ncomputational trust models: experiments\nand analysis\nPartheeban Chandrasekaran and Babak Esfandiari*\n\n*Correspondence:\nbabak@sce.carleton.ca\nDepartment of Systems and\nComputer Engineering, Carleton\nUniversity, 1125 Colonel By Drive,\nOttawa, Ontario K1s5B6, Canada\n\nAbstract\nWe propose a generic testbed for evaluating social trust models and we show how\nexisting models can fit our tesbed. To showcase the flexibility of our design, we\nimplemented a prototype and evaluated three trust algorithms, namely EigenTrust,\nPeerTrust and Appleseed, for their vulnerabilites to attacks and compliance to various\ntrust properties. For example, we were able to exhibit discrepancies between\nEigenTrust and PeerTrust, as well as trade-offs between resistance to slandering attacks\nversus self-promotion.\n\nKeywords: Trust testbed; Reputation; Multi-agent systems\n\nIntroduction\nMotivation\n\nWith the growth of online community-based systems such as peer-to-peer file-sharing\napplications, e-commerce and social networking websites, there is an increasing need to\nprovide computational trust mechanisms to determine which users or agents are honest\nand which ones are malicious. Many models calculate trust by relying on analyzing a\nhistory of interactions. The calculations can range from the simple averaging of ratings\non eBay to flow-based scores in the Advogato website. Thus for a researcher to evaluate\nand compare his or her latest model against existing ones, a comprehensive test tool is\nneeded. However, our research shows that the tools that exist to assist researchers are not\nflexible enough to include different trust models and their evaluations. Moreover, these\ntools use their own set of application-dependent metrics to evaluate a reputation system.\nThis means that a number of trust models cannot be evaluated for vulnerabilities against\ncertain types of attacks. Thus, there is still a need for a generic testbed to evaluate and\ncompare computational trust models.\n\nOverview of our solution and contributions\n\nIn this paper, we present a model and a testbed for evaluating a family of trust algo-\nrithms that rely on past transactions between agents. Trust assessment is viewed as a\nprocess consisting of a succession of graph transformations, where the agents form the\nvertices of the graph. The meaning of the edges depends on the transformation stage,\n\n© 2015 Chandrasekaran and Esfandiari. Open Access This article is distributed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to\nthe Creative Commons license, and indicate if changes were made.\n\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40493-015-0019-z-x&domain=pdf\nmailto: babak@sce.carleton.ca\nhttp://creativecommons.org/licenses/by/4.0/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 2 of 27\n\nand can refer to the presence of transactions between the two agents or the existence\nof a trust relationship between them. Our first contribution is to show that with this\nview, existing reputation systems can be adopted under a single model, but they work at\ndifferent stages of the trust assessment workflow. This allows us to present a new classi-\nfication scheme for a number of trust models based on where they fit in the assessment\nworkflow. The second contribution of our work is that this workflow can be described\nformally, and by doing this, we show that it is possible to model a variety of attacks\nand evaluation schemes. Finally, out of the larger number of systems we classified, we\nselected three reputation systems, namely EigenTrust [1], PeerTrust [2] and Appleseed\n[3], to exemplify the range and variety of reputation systems that our testbed can accom-\nmodate. We evaluated these three systems in our testbed against simple attacks and\nwe validated their compliance to basic trust properties. In particular, we were able to\nexhibit differences in the way EigenTrust and PeerTrust rank the agents, we observed\nthe subtle interplay between slandering and self-promoting attacks (higher sensitivity\nto one attack can lead to lower sensitivity to the other), and we verified that trust\nweakens along a friend-of-a-friend chain and that it is more easily lost than gained\n(as it should be).\n\nOrganization\n\nThis article is organized as follows: section ‘Background and literature review’ provides\nbackground and state of the art on trust models, attacks against them, and existing\ntestbeds for evaluation. Section ‘Problem description and model’ formulates the research\nproblem of this article and proposes our model for a testbed. Section ‘Classifying and\nchaining algorithms’ shows how some of existing trust algorithms can fit our model, and\nhow one can combine or compare them using our model and testbed. Section ‘Results and\ndiscussion’ describes the implementation details of our testbed prototype and presents\nevaluation results of three different trust algorithms, namely EigenTrust, PeerTrust, and\nAppleseed. Section ‘Conclusions’ concludes this article and summarizes the contributions\nand limitations of our work.\n\nBackground and literature review\nSocial trust models\n\nTrust management systems aid agents in establishing and assessing mutual trust. How-\never, the actual mechanisms used in these systems vary. For example, public key infras-\ntructures [4] rely on certificates whereas reputation-based trust management systems are\nbased on experiences of earlier direct and indirect interactions [5].\nIn this paper we will focus on social trust models based on reputation. The trust model\n\nshould provide a means to compare the trustworthiness of agents in order to choose a\nparticular agent to perform an action. For instance, on an e-commerce website like eBay,\nwe need to be able to compare the trustworthiness of sellers in order to pick the most\ntrustworthy one to buy a product from.\nSocial trust models rely on past experiences of agents to produce trust assertions. That\n\nis, the agents in the system interact with each other and record their experiences, which\nare then used to determine whether a particular agent is trustworthy. This model is self-\nsufficient because it does not rely on a third party to propagate trust, like it would in\ncertificate authority-based PKI trust models. However, there are drawbacks to having no\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 3 of 27\n\nroot of trust. For instance, agents evaluating the trustworthiness of agents with whom\nthere has been no interaction must use recommendations from others and, in turn,\nevaluate the trustworthiness of the recommenders. Social trust models must address this\nproblem.\n\nNature of input\n\nVarious inputs are used by social trust algorithms to measure the trustworthiness of\nagents. In EigenTrust [1], PeerTrust [2], TRAVOS [6] and Beta Reputation System (BRS)\n[7], agents rate their satisfaction after a transaction (e.g., downloading a file in a P2P\nfile-sharing network). These ratings are used to obtain a trust score that represents the\ntrustworthiness of the agent. In Aberer and Despotovic’s system [5]1, agents may file com-\nplaints (can be seen as dissatisfaction) about each other after a transaction. In Advogato\n[8], whose goal is to discourage spam on its blogging website, users explicitly certify\neach other as belonging to a particular level in the community. Trust algorithms may\nalso directly use trust scores among agents to compute an aggregated trustworthiness\nof agents, as in TidalTrust [9] and Appleseed [3]. In the specific context of P2P file-\nsharing, Credence [10] uses the votes on file authenticity to calculate a similarity score\nbetween agents and uses it to measure trust. The trust score is then used to recommend\nfiles.\n\nDirect vs. indirect trust\n\nThe truster may use some or all of its own and other agents’ past experiences with the\ntrustee to obtain a trust score. Trust algorithms often use gossiping to poll agents with\nwhom the truster has had interactions in the past.\nThe trust score calculated using only the experiences from direct interactions is\n\ncalled the direct trust score, while the trust score calculated using the recommenda-\ntions from other agents is called the indirect trust score [11]. As mentioned earlier,\nreputation systems use different inputs (satisfaction ratings, votes, certificates, etc.) to\ncalculate direct trust scores and indirect trust scores. PeerTrust uses satisfaction ratings\nto calculate both direct and indirect trust scores, whereas EigenTrust and TRAVOS\nuse satisfaction ratings to calculate direct trust scores, which they then use to calcu-\nlate indirect trust scores. Therefore, we can categorize the trust algorithms based on\nthe input required. But how do trust algorithms calculate the trust scores of agents\nusing the above information? It again varies from algorithm to algorithm. For instance,\nPeerTrust, EigenTrust, and Aberer use simple averaging of ratings, TRAVOS and BRS\nuse the beta probability density function, and Appleseed uses the Spreading Activation\nmodel.\n\nGlobal vs. local trust\n\nThe trust algorithm may output a global trust score or a local trust score [3, 12]. A global\ntrust score is one that represents the general trust that all agents have on a particular\nagent, whereas local trust scores represents the trust from the perspective of the truster\nand thus each truster may trust an agent differently. In our survey, we found PeerTrust,\nEigenTrust, and Aberer to be global trust algorithms whereas TRAVOS, BRS, Credence,\nAdvogato, TidalTrust, Appleseed, Marsh [13] and Abdul-Rahman [14] are local trust\nalgorithms.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 4 of 27\n\nTo trust or not to trust\n\nOnce the trust score is calculated, it can be used to decide whether to trust the agent. It\ncan be as simple as comparing the trust score against a threshold: if the trust score is above\na certain threshold, then the agent is trusted. Marsh [13], and Aberer [5] use thresholding\ntechniques. If the trust algorithm outputs normalized trust scores of agents as in Eigen-\nTrust, then the trust scores of agents are ranked. In this case, one may consider a certain\npercentage of the top ranked agents as trustworthy. In Appleseed, a graph is first obtained\nwith trust scores of agents as edge weights, and then, the truster agent is “injected” with\na value called the activation energy. This energy is spread to agents with a spreading fac-\ntor along the edges in the graph and the algorithm ranks the agents according to their\ntrust scores. Trust decisions can also be flow-based such as in Advogato, which calculates\na maximum “flow of trust” in the trust graph to determine which agents are trustworthy\nand which are not.\nIn short, social trust models focus on the following:\n\n1. What is the input to calculate the trust score of an agent?\n2. Does the trust algorithm use only direct experience or does it also rely on third\n\nparty recommendations?\n3. Is the trust score of an agent global or local?\n4. How does one decide whether to trust an agent?\n\nGiven the above discussion, and to assess the scope of our testbed, we propose tomodel,\nevaluate and compare three algorithms from fairly different families. The next sections\nprovide detailed descriptions of the trust models we selected and that we implemented in\nour testbed. The details are given to help understand the output of our experiments, but\nreaders familiar with EigenTrust, PeerTrust and/or AppleSeed may skip those respective\nsections.\n\nPeerTrust\n\nIn PeerTrust, agents rate each other in terms of the satisfaction received. These ratings\nare weighted by trust scores of the raters, and a global trust score is computed recursively\nusing Eq. 2.1, where:\n\n• T(u) is the trust score of agent u\n• I(u) is the set of transactions that agent u had with all the agents in the system\n• S(u, i) is the satisfaction rating on u for transaction i\n• p(u, i) is the agent that provided the rating.\n\nT(u) =\nI(u)∑\ni=1\n\nS(u, i) × T(p(u, i))∑I(u)\nj=1 T(p(u, j))\n\n(2.1)\n\nPeerTrust also provides a method for calculating local trust scores. In both local and\nglobal trust score computations, the trust score is compared against a threshold to decide\nwhether to trust or not.\n\nEigenTrust\n\nAgents in EigenTrust rate transactions as satisfactory or unsatisfactory [1]. These trans-\naction ratings are used as input, to calculate a local direct trust score, from which a global\ntrust score is then calculated.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 5 of 27\n\nAn agent i calculates the normalized local trust score of agent j, as shown in Eq. 2.2,\nwhere tij ∈ {+1,−1} is the transaction rating, and sij is the sum of ratings.\n\nsij =\n∑\nTij\n\ntrij\n\ncij = max(sij, 0)∑\nk max(sik , 0)\n\n(2.2)\n\nNote that we cannot use sij as the local trust score without normalizing, because mali-\ncious agents can arbitrarily assign high local trust values to fellow malicious agents and\nlow local trust values to honest agents.\nTo calculate the global trust score of an agent, the truster queries his friends for their\n\ntrust scores on the trustee. These local trust scores are aggregated, as shown in Eq. 2.3.\n\ntik =\n∑\nj\ncijcjk (2.3)\n\nIf we let C be the matrix containing cij elements, �ci be the local trust vector for i (each\nelement corresponds to the trust that i has in j), and �ti the vector containing tik , then,\n\n�ti = CT �ci (2.4)\n\nBy asking a friend’s friend’s opinion, Eq. 2.4 becomes �ti = (CT )2 �ci. If an agent keeps\nasking the opinions of its friends of friends, the whole trust graph can be explored, and\nEq. 2.4 becomes Eq. 2.5, where n is the number of hops from i.\n\n�t = (CT )n �ci (2.5)\n\nThe trust scores of the agents converge to a global value irrespective of the trustee.\nBecause EigenTrust outputs global trust scores (normalized over the sum of all agents),\n\nagents are ranked according to their trust scores (unlike PeerTrust). Therefore, an agent\nis considered trustworthy if it is within a certain rank.\n\nAppleseed\n\nAppleseed is a flow-based algorithm [3]. Assuming that we are given a directed weighted\ngraph with agents as nodes, edges as trust relationships, and the weight of an edge as\ntrustworthiness of the sink, we can determine the amount of trust that flows in the graph.\nThat is, given a trust seed, an energy in ∈ R\n\n+\n0 , spreading factor decay ∈[ 0, 1], and conver-\n\ngence threshold Tc, Appleseed returns a trust score of agents from the perspective of the\ntrust seed.\nThe trust propagation from agent a to agent b is determined using Eq. 2.6, where the\n\nweight of edge (a, b) represents the amount of trust a places in b, and in(a) and in(b)\nrepresent the flow of trust into a and b, respectively.\n\nin(b) = decay ×\n∑\n\n(a,b)∈E\nin(a) × weight(a, b)∑\n\n(a,c)∈E weight(a, c)\n(2.6)\n\nThe trust of an agent b (trust(b)) is then updated using Eq. 2.7, where the decay factor\nensures that trust in an agent decreases as the path length from the seed increases.\n\ntrust(b) := trust(b) + (1 − decay) × in(b) (2.7)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 6 of 27\n\nGenerally, trust graphs have loops, which makes Eq. 2.7 recursive. Thus a termination\ncondition like the one below is required, where Ai ⊆ A is the set of nodes that were\ndiscovered until step i and trusti(x) is the current trust scores for all x ∈ Ai:\n\n∀x ∈ Ai : trusti(x) − trusti−1(x) ≤ Tc (2.8)\n\nAfter Eq. 2.7 terminates, the trust scores of agents are ranked. Since this set is ranked\nfrom the perspective of the seed, Appleseed is a local trust algorithm.\nAs our brief survey shows, the trust models vary in terms of their input, output, and\n\nthe methods they use. To evaluate and compare them, testbeds are needed. In the next\nsection we take a look at existing testbeds.\n\nTestbeds\n\nWe investigated two testbed models, namely Guha’s [15] andMacau [16], and two testbed\nimplementations, namely ART [17] and TREET [18], which are used to evaluate trust\nalgorithms. This section provides details of our investigation.\n\nGuha\n\nGuha [15] proposes a model to capture document recommendation systems, where trust\nand reputation play an important role. The model relies on a graph of agents where the\nedges can be weighted based on their mutual ratings, and a rating function for documents\nby agents. Guha then discusses how trust can be calculated based on those ratings, and\nevaluates a few case studies of real systems that can be accommodated by the model.\nGuha’s model can capture trust systems that take a set of documents and their ratings\n\nas input (such as Credence [10]), but it cannot accommodate systems where the only\ninput consists of direct feedbacks between agents, such as in PeerTrust (global) [2] or\nEigenTrust [1]. Also, the rating of documents is itself an output of Guha’s model, and that\nis often not the purpose or output of many more general-purpose trust models.\nIn short, document recommendation systems can be viewed as a specialization or\n\nsubclass of more general trust systems, and Guha’s model is suitable for that subclass.\n\nMacau\n\nHazard and Singh’s Macau [16] is a model for evaluating reputation systems. The authors\ndistinguish two roles for any agent: a rater that evaluates a target. Transactions are viewed\nas a favor provided by the target to the rater. The target’s reputation, local to each rater-\ntarget pairing, is updated after each transaction and depends on the previous reputation\nvalue. The target’s payoff in giving a favor is also dependent on its current reputation but\nalso on its belief of the likelihood that the rater will in turn return the favor in the future.\nBased on the above definitions, the authors define a set of desirable properties for a\n\nreputation system:\n\n• Monotonicity: given two different targets a and b, the computed reputation of a\nshould be higher than that of b if the predicted payoff of a transaction with a is\nhigher than with b.\n\n• Unambiguity and convergence: the reputation should converge over time to a single\nfixpoint, regardless of its initial value.\n\n• Accuracy: this convergence should happen quickly, thus minimizing the total\nreputation estimation errors in the meantime.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 7 of 27\n\nMacau thus captures an important stage in trust assessment, i.e. the update of one-to-\none trustworthiness based on past transactions. It has been used to evaluate, in terms of\ntheir compliance to the properties defined above, algorithms such as TRAVOS [6] and the\nBeta Reputation System (BRS) [7] that model positive and negative experiences as ran-\ndom variables following a beta probability distribution. The comparison of trust models\nrelying on the beta distribution and their resilience to various attacks has also recently\nbeen explored in [19].\n\nART\n\nThe Agent Reputation and Trust testbed (ART) [17] provides an open-source message-\ndriven simulation engine for implementing and comparing the performance of reputation\nsystems. ART uses art painting sales as the domain.\nEach client has to sell paintings belonging to a particular era. To determine their\n\nmarket values, clients refer to agents for appraisals for a fee. Because each agent\nis an expert only in a specific era, it may not be able to provide appraisals for\npaintings from other eras and therefore refers to other agents for a fee. After such\ninteractions, agents record their experiences, calculate their reputation scores, and\nuse them to choose the most trustworthy agents for future interactions. The goal\nof each agent is to finish the simulation with the highest bank balance, and, intu-\nitively, the winning agent’s trust mechanism knows the right agents to trust for\nrecommendations.\nThe ART testbed provides a protocol that each agent must implement. The protocol\n\nspecifies the possible messages that agents can send to each other. Themessages are deliv-\nered by the simulation engine, which loops over each agent at every time interval. The\nengine is also responsible for keeping track of the bank balance of the agents, and assign-\ning new clients to agents. All results are collected and stored in a database and displayed\non a graphical user interface (GUI) at runtime.\nART is best suited for evaluating trust calculation schemes from a first person point\n\nof view. It is not meant as a platform for testing trust management as a service provided\nby the system. For example, to evaluate EigenTrust in ART, one would either need to\nconsiderably modify ART itself (for the centralized version of EigenTrust) or to require\ncooperation from the participating agents and an additional dedicated distributed infras-\ntructure (for the distributed version). Furthermore, as also pointed out in [16] and [20],\nthe comparison of the performance of different agents is not necessarily based on their\ncorrect ability to assess the reputation of other agents, but rather based on how well they\nmodel and exploit the problem domain.\n\nTREET\n\nThe Trust and Reputation Experimentation and Evaluation Testbed (TREET) [18] mod-\nels a general marketplace scenario where there are buyers, sellers, and 1,000 different\nproducts with varying prices, such that there are more inexpensive items than expensive\nones. The sale price of the products is fixed, to avoid the influence of market competition.\nThe cost of producing an item is 75% of the selling price, and the seller incurs this cost.\nTo lower this cost and increase profit, a seller can cheat by not shipping the item. Each\nproduct also has a utility value of 110% of the selling price, which encourages buyers to\npurchase.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 8 of 27\n\nAgents join or exit after 100 simulation days or after a day with a probability of 0.05,\nbut to keep the number of buyers and sellers constant, an agent is introduced for each\ndeparting agent. At initialization, each seller is assigned a random number of products\nto sell. Buyers evaluate the offers from each seller and pick a seller. Sellers are informed\nof the accepted offers and are paid. Fourteen days after a sale, the buyer knows whether\nhe has been cheated or not, depending on whether he receives the purchased item. The\nbuyer then provides feedback based on his experience of the transaction. The feedback is\nin turn used to choose sellers for future transactions.\nTREET evaluates the performance of various reputation systems under Reputation Lag\n\nattack, Proliferation attack, and Value Imbalance attack using the following metrics:\n\n1. cheater sales over honest sales ratio\n2. cheater profit over honest profit ratio\n\nMultiple seller accounts are needed to orchestrate a Proliferation Attack, but TREET\ndoes not consider attacks such as White-Washing and Self-Promoting, which require\ncreating multiple buyer accounts.\nTREET addresses many of ART’s limitation in a marketplace scenario. To name a\n\nfew [21], TREET supports both centralized and decentralized trust algorithms, allows\ncollusion attacks to be implemented, and does not put a restriction on trust score rep-\nresentation. However, like ART, the evaluation metrics in TREET are tightly coupled to\nthe marketplace domain. It is unclear how ART or TREET can be used to evaluate trust\nmodels used in other systems, such as P2P file-sharing networks, online product review\nwebsites and others that use trust. To our knowledge, there is no testbed that provides\ngeneric evaluation metrics and that is independent of the application domain.\n\nSummary\n\nTrust is a tool used in the decision-making process and it can be computed. There are\nmanymodels based on social trust that attempt to aid agents in making rational decisions.\nHowever, these models vary in terms of their input and output requirements. This makes\nevaluations against a common set of attacks difficult.\n\nProblem description andmodel\nOur goal is to have a testbed that is generic enough to accommodate as many trust\nmanagement systems and models as possible. Our requirements are:\n\n1. A model that provides an abstraction layer for developers to incorporate existing\nand new systems that match the input and output of the model.\n\n2. An evaluation framework to measure and compare the performance of trust models\nagainst trust properties and attacks independently of the application domain.\n\nIn this section, we introduce an abstract model for trust management systems. This\nmodel will be the foundation of our testbed. Our model is essentially based on the\nfollowing stages:\n\n1. In stage 1 of the trust assessment process, the feedback provided by agents on other\nagents is represented as a feedback history graph.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 9 of 27\n\n2. In stage 2, a reputation graph is produced, where the weight of an arc denotes the\nreputation of the target agent. “Reputation” here follows [14], as “an expectation\nabout an individual’s behavior based on information about or observations of its\npast behavior”. It is viewed as an estimation of trustworthiness based on a\ncombination of direct and indirect feedback.\n\n3. In the final stage, a trust graph is produced, where the existence of an arc implies\ntrust in the target agent. We take “trust” here to mean the “belief by agent A that\nagent B is trustworthy” [2, 22], and so it is boolean and subjective in our model.\n\nIn the rest of this section, we define the aforementioned graphs in stages.\n\nStage 1—obtain feedback history graph\n\nWe first define a feedback, f (a, b) ∈ R as an assessment made by agent a of an action or\ngroup of actions performed by agent b, where a and b belong to the set A of all the agents\nin the system. The list of n feedbacks by a on b, FHG(a, b), is called a feedback history,\nrepresented as follows:\n\nFHG(a, b) �→ (f1(a, b), f2(a, b), . . . , fn(a, b)) (3.1)\n\nThe feedback fi(a, b) indicates the ith satisfaction received by a from b’s action. For\nexample, in a file-sharing network, the feedback by a downloader may indicate the sat-\nisfaction received from downloading a file from an uploader in terms of a value in R.\nExisting trust models use different ranges of values for feedback, and letting the feedback\nvalue be in R allows us to include these reputation systems in our testbed.\nIf A is the set of agents, E is the set of labelled arcs (a, b), and the label is FHG(a, b)\n\nwhen FHG(a, b) \t= ∅, then the feedback histories for all agents in A are represented in a\ndirected and labelled graph called Feedback History Graph (FHG)2, FHG = (A,E):\n\nFHG : A × A → R\nN\n\n∗\n(3.2)\n\nNote that we have not included timestamps associated with each feedback (which would\nbe useful for, among other things, running our testbed as a discrete event simulator), but\nour model can be expanded to accommodate it.\nOnce the feedback history graph is obtained, the next step is to produce a reputation\n\ngraph.\n\nStage 2—obtain reputation graph\n\nA Reputation Graph (RG), RG = (A,E′\n), is a directed and weighted graph, where the\n\nweight on an arc, RG(a, b), is the trustworthiness of b from a’s perspective:\n\nRG : A × A → R (3.3)\n\nThe edges are added by computing second and nth-hand trust via transitive closure of\nedges in E. That is: if (a, b) ∈ E and (b, c) ∈ E ⇒ (a, b), (b, c), and (a, c) ∈ E′ (the value of\nthe weight of the edges, however, depends on the particular trust algorithm).\nReputation algorithms may also exhibit the reflexive property by adding looping arcs to\n\nindicate that the truster trusts itself to a certain degree for a particular task [1–3].\nThe existing literature categorizes reputation algorithms into two groups: local and\n\nglobal (Figs. 1(a) and (b), respectively) [3, 5]. Global algorithms assign a single reputa-\ntion score to each agent. Therefore, if a global algorithm is used, then the weights of the\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 10 of 27\n\nFig. 1 Examples of reputation graphs output respectively by a local and global algorithm\n\nincoming arcs of an agent should be the same, as shown in Fig. 1(b) (although for clar-\nity’s sake we will often present the graph simply as a ranking of agents in the rest of this\narticle). There is no such property for local algorithms.\nReputation algorithms may also differ in how the graphs is produced. One method is\n\nto first calculate one-to-one scores of agents using direct feedbacks and then use them\nto calculate the trustworthiness of agents previously unknown to the truster (e.g., Eigen-\nTrust). This is shown as 1a and 1b in Fig. 2. The other method (#2 in Fig. 2) skips the\nintermediate graph in the aforementioned method and produces a reputation graph (e.g.,\nPeerTrust).\n\nStage 3—obtain trust graph\n\nThe graph obtained in stage 2 contains information about the trustworthiness of agents.\nBut to use this information to make a decision about a transaction in the future, agents\nmust convert trustworthiness to boolean trust (see [23] for an example), which can also\nbe expressed as a graph. We refer to this directed graph as the Trust Graph (TG) TG =\n(A, F), where a directed edge ab ∈ F represents agent a trusting agent b.\nTo summarize ourmodel, we can represent the stages as part of a workflow as illustrated\n\nin Fig. 3.\n\nFig. 2 Two methods to obtain a reputation graph\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 11 of 27\n\nFig. 3 Overview of the stages in our model\n\nIn the next section, we see at what stages in our model do various algorithms fit, and\ndescribe criteria for chaining different algorithms.\n\nClassifying and chaining algorithms\nBy refactoring the trust models according to the stages presented in the above sections,\nwe start to see a new classification scheme. Let us take EigenTrust, PeerTrust, and Apple-\nseed as examples and describe them using our model. EigenTrust takes an FHG with\nedge labels in {0, 1}∗ as input and outputs an RG with edge labels in [ 0, 1]. PeerTrust,\non the other hand, takes an FHG with edge labels in [ 0, 1]∗ as input and outputs an\nRG with edge labels in [ 0, 1]. Meanwhile, Appleseed requires an RG with edge labels in\n[ 0, 1] as input and outputs another RG′ in the same codomain. It is also possible for an\nalgorithm to skip some stages. For example, according to our model, Aberer [5] skips\nstage 2 and does not output a reputation graph. One can also represent simple mecha-\nnisms to generate a trust graph by applying a threshold on reputation values (as output\nfor example by EigenTrust), or by selecting the top k agents. This stage transitions of\nalgorithms are depicted3 in Fig. 4. In addition to the existing classification criteria in the\nstate of the art, trust algorithms can now be classified according to their stage transi-\ntions (i.e., from one stage to another as well as transitioning within a stage) as shown in\nTable 1.\nIt is important to note that although these three algorithms output a reputation\n\ngraph with continuous reputation values between 0 and 1, the semantics of these val-\nues are different. EigenTrust outputs relative (among agents) global reputation scores,\nPeerTrust outputs an absolute global reputation score, and Appleseed produces relative\nlocal reputation scores. In other words, EigenTrust and Appleseed are ranking algorithms\n(global and local, respectively), whereas PeerTrust is not.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 12 of 27\n\nFig. 4 Stage transitions of Trust algorithms\n\nAs we can see, each step of the trust assessment process can be viewed as a\ngraph transformation function, and we can use this functional view to easily describe\nevaluation mechanisms as well. Suppose an experimenter wants to compare PeerTrust\nand EigenTrust. The inputs and outputs of these algorithms are semantically different.\nTo match the input, we can use a function that discretizes continuous feedback values\n(f (a, b)) in [0, 1] to {-1, 1}, using some threshold t:\n\nTable 1 A classification for trust models\n\nStage Global or\nAbsolute or\n\nTrust Algorithm\nTransitions\n\nInput\nLocal\n\nRelative\nReputation Scores\n\nEigenTrust 0 → 2\nsatisfaction\n\nglobal relativeratings\n\nPeerTrust 0 → 2\nsatisfaction\n\nglobal absoluteratings\n\nAppleSeed 2 → 2\nreputation\n\nlocal absolutescores\n\nAberer & Despotovic 0 → 3 complaints global N/A\n\nAdvogato 3 → 3 certificates local N/A\n\nTRAVOS 0 → 2\nsatisfaction\n\nlocal absoluteratings\n\nRanking 2 → 3\nreputation\n\nN/A relativescores\n\nThresholding 2 → 3\nreputation\n\nN/A absolutescores\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 13 of 27\n\ndiscretizet : [ 0, 1] → {−1, 1}\n\nx �→ discretizet(x) =\n{\n\n−1 if x ≤ t\n1 if x > t\n\n(4.1)\n\nTo lighten the notations, in what follows we will adopt a default threshold of 0.5 and drop\nt. We will also, in an abuse of notation, actually use discretize : FHG→ FHG, which applies\nthe function defined in 4.1 to every feedback of every edge of the graph.\nLet us turn to the output now. Recall that the output of EigenTrust is a relative global\n\nscore while PeerTrust’s is also global but absolute. To make the outputs more directly\ncomparable, we can use a normalization function on PeerTrust’s output, ensuring that the\nsum of outgoing weights for each agent is 1. It takes a reputation graph RG2 with edge\nlabels in [ 0, 1] as input, and outputs another reputation graph RG3 with edge labels in\n[ 0, 1], but where the reputation scores are relative to one another. This is achieved using\nEq. 4.2, where N(a) is the set of agents adjacent to a via outgoing edges.\n\nnormalize : A × A →[ 0, 1]\n\n(a, b) �→ normalize(a, b) = RG(a, b)∑\nc∈N(a) RG(a, c)\n\n(4.2)\n\nAgain, in an abuse of notation we will also refer in what follows to normalize as the func-\ntion in RG→ RG that applies 4.2 to every edge of the graph. Note again that normalization\nisn’t strictly speaking a necessary step if all one is concerned about is the ranking of the\nagent, and not the values attached to each agent, especially since the semantics of these\nvalues are still ultimately attached to the algorithm that is computing them. Normaliza-\ntion does not affect the ranking. Either way, Spearman’s rank correlation coefficient [24]\ncan finally be used to compare the rankings output by global trust functions such as Eigen-\nTrust and PeerTrust. This metric ∈[−1, 1] specifies the degree to which the ranking order\nof the outputs match, where 1 means a perfect match and -1 means no match: spearman:\nRG × RG → [−1, 1].\nThe choice of discretization and normalization methods are important and they may\n\nintroduce a bias when comparing two algorithms. For the purpose of demonstrating our\nmodel, we will assume that our choices are good enough.\nBy combining these algorithms, we can represent the workflow as shown in Fig. 5.\nTo more clearly capture trust management algorithms as a process consisting of graph\n\ntransformation operations, and to be able to express them in our experiments as such,\nwe introduce named functions that make these transformations explicit. Here is an\nillustration of the convention we will follow: the function that captures EigenTrust’s trans-\nformation from an FHG to an RGwill be named f 2reigentrust , where “f ” stands for FHG and\n“r” for RG. Hence, in functional programming terms, the experiment comparing Eigen-\nTrust and PeerTrust using a given feedback history graph FHG0 can be specified very\nsimply as follows:\n\nspearman(f 2reigentrust(f 2fdiscretize(FHG0)), r2rnormalize(f 2rpeertrust(FHG0)))\n\nIn the current implementation of our testbed, we assume that the pre-condition checks\nas per Table 1 are enforced by the algorithms rather than the testbed. However, this can\nbe modified easily in the future.\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 14 of 27\n\nFig. 5 Comparing EigenTrust and PeerTrust\n\nIn the next section, we present simple experiments that we ran to analyze trust\nalgorithms using the model described in this section.\n\nResults and discussion\nUsing the prototype testbed that we implemented following the model presented in\nthe previous section, we conducted several experiments and analyzed their results on\nthree trust algorithms, namely EigenTrust, PeerTrust, and Appleseed. The experiments\nthat we present in this section are grouped into two categories: vulnerability assess-\nments and trust properties assessments. But first we give a general description of the\nimplementation.\n\nImplementation\n\nA prototype was designed and built in Java to test the model described in Section ‘Prob-\nlem description and model’. The two main components of the testbed are graphs and\nalgorithms. A Graph can be a feedback history graph, a reputation graph or a trust graph.\nThese graphs follow our model as described in the previous section.\nThe graphs can be populated either programatically or by providing a file following\n\nthe Attribute-Relation File Format (ARFF) used in the Weka machine learning toolkit\n(ARFF was chosen for its simplicity, but other formats could obviously be accommodated\nas well in the future if needed). For instance, a feedback history graph is populated with\na file containing a list of relations, each containing 3 attributes: source agent identifier,\nsink agent identifier and the feedback value. An example of a ARFF file for populating a\nfeedback history graph is given in Listing 1:\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 15 of 27\n\nListing 1 An example ARFF file for populating a feedback history graph\n\n@relation feedback\n@attribute assessorID string\n@attribute assesseeID string\n@attribute feedbackValue numeric\n@data\n0,1,0.6\n0,1,0.7\n0,1,0.2\n\nIn our prototype, even though the algorithms are implemented using object-orientation,\nthey are ultimately presented to the experimenter as functions that take a graph as input\nand return a graph as the output, to fully conform to the model we have presented, and\nto make the experiments simple and intuitive to express. Other utility functions, such as\nevaluation, discretization and comparison, are also provided. See Listing 2 for an example.\n\nListing 2 How to create a simple experiment in our testbed\n\nimport static trust.Algorithms.*;\nFHG fhg = new FHG(\"ex.arff\");\neigentrust(discretize(fhg)); //the experiment!\n\nThe testbed’s source code is open and is available on Google Code4. To ensure that our\nalgorithm implementationsmatch the authors’ intentions and our own understanding, we\nalso provide a set of unit tests taken from examples found in the literature as well as our\nown additional scenarios.\n\nVulnerability assessments\n\nThe security of trust algorithms is measured by their resistance to attacks. In this section,\nwe subject them to attacks and assess their vulnerabilities. We will purposely try to come\nup with the simplest attack scenarios that can exhibit the properties we are looking for.\nThirunarayan et al. [19] follows a similar approach but restricted to trust algorithms\nrelying on the beta probability distribution.\n\nNormalization-based attack\n\nSetup In this first experiment, we want to exhibit whether two different trust algorithms\n(in this case, EigenTrust vs PeerTrust) may output different rankings given the same input.\nFor this we investigate how reputation is affected by the number of good feedbacks versus\nthe bad feedbacks. Suppose we were given FHG0 in Fig. 6(a) and FHG1 in Fig. 6(b). Since\nthere are more good feedbacks on agent 2 in FHG0 than FHG1, it is reasonable to expect\nagent 2’s reputation in RG1 to be lower than in RG2.\nWe normalize the output of PeerTrust simply to make the scale of the reputation num-\n\nbers similar to EigenTrust (we do not claim that the semantics are the same). And as\nEigenTrust requires feedbacks to be either positive (+1) or negative (-1), we first need to\nconvert the 1.0 values in the FHGs to +1 and the 0.0 values to -1, hence the use of the\ndiscretizer function in the experiment specifications below:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nspearman\n\n(\nf 2rpeertrust(FHG0), f 2rpeertrust(FHG1)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2rpeertrust(FHG0)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n)\n, f 2rpeertrust(FHG1)\n\n)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 16 of 27\n\nFig. 6 Normalization-based Discrepancy\n\nResults Comparing the rankings in Table 2, we note that EigenTrust reports no change\nin agent 2’s rank, whereas in PeerTrust its rank has changed from 1 to 2 (Spearman’s\ncoefficient = 0.83).\nWhy did agent 2’s rank not change in the EigenTrust experiment? During the initial cal-\n\nculation of its normalized local trust values (used for the calculation of the reputation\ngraph) for a given agent, Eigentrust essentially calculates the difference between positive\nand negative feedbacks for each neighbour and then it normalizes it over all its neigh-\nbours. In our case, agent 1 only has agent 2 as its neighbour and so the normalization\nprocess leads to a loss of information, namely the number of positive feedbacks.\nThus, if an agent has interacted with a malicious agent only, then the malicious agent\n\ncan get the victim to trust him fully, as long as the number of positive feedbacks that the\nmalicious agent received is greater than the number of negative feedbacks. This prob-\nlem is acknowledged by EigenTrust’s authors [1]. But PeerTrust does not suffer from this\nproblem, because it does not attempt to perform a sum of positive and negative feedbacks\nin this fashion.\n\nSelf-promoting attack\n\nSetup Suppose the least trustworthy agent rates a newcomer (i.e., one that had received\nno feedback yet) highly. If the newcomer becomes more trustworthy than before the\n\nTable 2 Rankings before and after slandering in retort (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nPeerTrust(FHG0) 3 (0.11) 1 (0.44) 1 (0.44) 4 (0.00)\n\nPeerTrust(FHG1) 3 (0.13) 1 (0.52) 2 (0.35) 4 (0.00)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 17 of 27\n\nFig. 7 Self-promoting attack – EigenTrust\n\nrating, then this can be exploited maliciously, paving the way to collusive self-promoting\nattacks [12]. Conversely, if the newcomer becomes less trustworthy than before, then the\nnewcomer is vulnerable to a slandering attack. Consider FHG1 in Fig. 7(b) which is based\non FHG0 in Fig. 7(a), after agent 3 rates agent 0 highly. The experiment set-up is therefore\nas follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nand the output can be seen in Fig. 7(c).\n\nResults Looking at Table 3, we see that agent 0’s reputation relative to agent 3 increased.\nNote that it is possible that agent 3 genuinely rated agent 0 highly, in which case Eigen-\nTrust’s output is a good assessment, otherwise it might have missed a case of collusive\nself-promoting attack. It is also worth noting that this experiment breaks PeerTrust, due\nto a division-by-0 problem in an equation. Indeed, when calculating agent 0’s reputa-\ntion score, the feedbacks provided by agent 3 and its reputation score must be taken into\naccount. In this case, agent 3’s reputation score is 0, due to the fact that it received only\n0-valued feedbacks. The division by agent 3’s reputation is the problem here.\n\nSlandering attack, scenario 1\n\nSetup When the least trustworthy agent (agent 3) gives a newcomer (agent 0) a very\nlow rating, it can either be a genuine rating or a slandering attempt [12]. The reputation\nalgorithm may resist slandering by not decreasing agent 0’s reputation, but the risk or\ntradeoff would be the possibility of ignoring a genuine assessment. The inputs for this\nexperiment are FHG0 and FHG1 as shown in Fig. 8, and the experiment is again therefore\nspecified as follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\n\nTable 3 Ranking before and after low feedback to newcomer (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.22) 2 (0.3) 1 (0.36) 4 (0.12)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 18 of 27\n\nFig. 8 Slandering attack scenario 1 – EigenTrust\n\nResults Comparing the rankings in Table 4, we observe that agent 0’s reputation and\nglobal rank has not changed. If agent 3 has indeed provided a dishonest negative feedback,\nthen we can say that EigenTrust output is a correct assessment. However, it is also possible\nthat agent 3 is the one being slandered (by agent 2) and has provided a genuine negative\nfeedback, in which case, we would expect agent 0’s rank to decrease. Thus, we can only\nsay that EigenTrust is insensitive to bad feedbacks and therefore resists the slandering of\na newcomer. Note that again PeerTrust would produce invalid results for the same reason\nas previously.\n\nSlandering attack, scenario 2\n\nSetup Now going back to the initial situation (Fig. 9(a)), what happens if, as shown\nin FHG1 in Fig. 9(b), agent 3 decides to direct its bad feedbacks to agent 2\ninstead of agent0? Looking purely at the feedbacks, one can make the following\nguesses:\n\n• Agent 3 rated agent 2 negatively to cover its malicious acts (a slandering attack).\n• Agent 2 cheated agent 3 and thus obtained negative feedbacks but acted honestly\n\nwith agent 1 to keep its reputation high (a white-washing attack).\n\nHaving received more positive feedbacks than agent 3, if agent 2’s reputation is not\naffected, then the algorithm is said to be resistant to a slandering attack but it is vulnerable\n\nTable 4 Ranking before and after high feedback to newcomer (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigenTrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigenTrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 19 of 27\n\nFig. 9 Slandering attack scenario 2 – EigenTrust\n\nto a white-washing attack. Both EigenTrust and PeerTrust can be used to run this\nexperiment, which is specified as follows:\n\nspearman\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG0)\n\n)\n, f 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n))\nspearman\n\n(\nf 2rpeertrust(FHG0), f 2rpeertrust(FHG1)\n\n)\nspearman\n\n(\nf 2reigentrust\n\n(\nf 2fdiscretize(FHG1)\n\n)\n, r2rnormalize\n\n(\nf 2rpeertrust(FHG1)\n\n))\n\nResults The rankings in Table 5 show that both EigenTrust and PeerTrust are resis-\ntant to this kind of slandering attack, but, as mentioned earlier, they are vulnerable to\nwhite-washing attacks (because the negative feedbacks by agent 3 did not lower agent 2’s\nranking).\n\nSlandering + Sybil attack for local trust algorithms\n\nSetup We now turn to finding out how many malicious agents it takes to slander\neffectively. In a Sybil attack, a malicious agent introduces a number of Sybils (i.e, accom-\nplices or pseudonyms), whose purpose is to slander a victim in the system [12]. Suppose\nwe are given the reputation graph shown in Fig. 10. Let agent 0 be a malicious agent that\nslanders agent 1. Assuming this agent has unlimited resources (e.g., an unlimited and\n\nTable 5 Rankings before and after slandering in retort (reputations in brackets)\n\nAgent 0 Agent 1 Agent 2 Agent 3\n\nEigentrust(FHG0) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nEigentrust(FHG1) 3 (0.16) 2 (0.29) 1 (0.39) 3 (0.16)\n\nPeerTrust(FHG0) 3 (0.25) 1 (1.00) 1 (1.00) 4 (0.00)\n\nPeerTrust(FHG1) 3 (0.11) 1 (0.44) 1 (0.44) 4 (0.00)\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 20 of 27\n\nFig. 10 Slandering attack using a feedback history graph\n\nlow-cost ability to create pseudonyms), it may introduce an unlimited number of Sybils\nto collectively slander agent 1. In such a scenario, it is useful to study how r(0, 1), i.e. the\nreputation of Agent 1 from the point of view of Agent 0, changes with respect to other\nagents. If r(0, 1) changes such that it is less than r(0, 2), then we can conclude that the slan-\ndering attack was successful and we would measure the effectiveness of the attack based\non the number of Sybils required. However, as explained in Section ‘Slandering attack,\nscenario 2’, one can also interpret this scenario as a white-washing attack. Thus, if an\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 21 of 27\n\nFig. 11 FHG after adding 10 sybils and slander edges\n\nalgorithm is resistant to a slandering attack, then it may be vulnerable to white-washing\nattacks.\nTo measure Appleseed’s attack resistance with the graph in Fig. 10, we first verify\n\nwhether the victim’s reputation is less than that of an agent in the attacker’s possession\n(in this case, we wish to check if r(0, 1) < r(0, 2)). If it is false, the result is updated and a\nSybil agent x is created. Edges (2, x, 1.0) and (x, 1, 0) are also added to the FHG and Apple-\nseed is run again (AppleSeed normally needs a reputation graph as input, but in this case\nsince each FHG edge has a singleton feedback, the value of the feedback can be directly\nused as reputation). This process continues and Sybils are added for a certain number of\niterations or until the attack succeeds (Fig. 11).\n\nResults Appleseed resists this type of attack well. Table 6 summarizes the values of\nr(0, 1) and r(0, 2) in RG after adding 1, 10, 50 and 100 Sybils and slander edges to\nthe FHG. We observe that r(0, 1) > r(0, 2). This confirms the attack resistance prop-\nerty of Appleseed described in [3]. This is not the case of global trust algorithms\nsuch as EigenTrust, since the addition of sybils simply dilutes the reputation values of\nagents.\n\nTrust properties assessments\n\nIn this section, trust algorithms are evaluted for their adherence to trust proper-\nties, namely: (a) weak transitivity and (b) the fact that trust is more easily lost than\ngained.\n\nWeak transitivity\n\nSetup Trustworthiness obtained via the transitive closure of trust paths should decrease\nas the length of the trust path increases [9, 25]. We subjected Appleseed and EigenTrust\nto simple tests to verify this basic rule. Even though the input and output graphs are of\ndifferent types for these two algorithms, the same assertions can be applied to test the\n\nTable 6 Reputation scores by Appleseed\n\nNo. of slandering edges added r(0, 1) r(0, 2)\n\n1 0.36 0.15\n\n10 0.34 0.15\n\n50 0.34 0.14\n\n100 0.34 0.14\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 22 of 27\n\ntransitive rules, as they do not depend on the semantics of the weight of edges or whether\nthe reputation scores are local or global.\n\nResults Table 7 presents the test cases (the \"Input FHG/RG\" column), expected results\n(the \"Output RG\" column represents the transitive closure resulting from the graph trans-\nformation, along with question marks on the edges where the weak transitivity property\nis tested) and actual results. Both Appleseed and EigenTrust passed our transitive tests.\n\nDynamic evolution of global trust\n\nAccording to Marsh [13], “When considering a simple trusting agent (i.e., one who does\nuse rules of reciprocation), the trust he has in a trustee will ordinarily increase if coop-\neration occurs, and decrease otherwise”. That is, if trust exists between two parties, then\ncooperation between them reinforces trust by increasing some trust score. If there was\nno trust, then cooperation builds trust. Furthermore, “a sudden defection from a trusted\nfriend can result in a drastic reduction of trust, to the extent that a lot of work is neces-\nsary to build that trust up again” [13]. That is, if cooperation does not ensue, then trust is\nlost at a rate higher that it was gained. Adherence to this property prevents white-washing\natttacks where attackers cheat periodically while maintaining a high reputation.\nWe propose the following method to evaluate this property.\nLet fi(∗, b) be the i’th feedback on b in feedback history FHG(∗, b), which is the list of\n\nfeedbacks on b by all agents in the system and ri(b) be b’s global reputation score following\ni’th feedback.\n\nTable 7 Transitivity tests and results\n\nExp Input FHG/RG Output RG Expectation Results\n\nEigenTrust Appleseed\n\n1 r(0, 1) >= r(0, 2)\n\nr(0, 2) == r(0, 3) true true\n\n2 r(0, 5) >= r(0, 2)\n\nr(0, 5) >= r(0, 3) true true\n\n3 r(0, 3) >= r(0, 2)\n\ntrue true\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 23 of 27\n\nAssuming both reputation scores and feedback values are on the same scale and r(b)\nis a function of the feedback history FHG(∗, b), then trust must evolve according to con-\nditions 5.1 and 5.2, where we define, for agent b and for any consecutive “instants” i − 1\nand i:\n\n• δf := fi(∗, b) − ri−1(b) (the ith feedback is compared to the reputation at instant i-1)\n• δr := ri(b) − ri−1(b) where ri(b) is obtained after feedback fi(∗, b)\n• δr\n\nδf\n+ := δr\n\nδf if\nδr\nδf ≥ 0, and δr\n\nδf\n− otherwise\n\n0 ≤ δr\nδf\n\n+\n≤ 1 (5.1)\n\n| δr\nδf\n\n+\n| ≤ | δr\n\nδf\n\n−\n| (5.2)\n\nThat is:\n\n1. Condition 5.1 verifies that if there is a positive change in feedback, then the change\nin reputation is also positive, but the change in reputation is less than the change in\nfeedback.\n\n2. Condition 5.2 verifies that the magnitude of the rate of change in reputation due to\npositive feedbacks is always less than the magnitude of the rate of change in\nreputation due to negative feedbacks.\n\nSetup and results We set up two scenarios to investigate the behaviour of PeerTrust. In\nthe first scenario, there are only two agents a and b, and the feedback history FHG(a, b) is\nhandcrafted such that a is typically satisfied with b, except once, where f (a, b) is set to 0.\nIn this scenario, we determine whether trust loss is greater than trust gain. In the second\nscenario, we introduce a third agent c and we investigate the impact of r(c, a) on r(a, b)\nand whether the above conditions are still held in this scenario.\n\nScenario 1: evolution due to direct interactions only Consider the feedback histories\nbetween two agents a and b in Table 8, which shows agent a was dissatisfied with b only\nonce. One could argue that b mounted a white-washing attack in which it behaved hon-\nestly in order to cheat once in a while, or it is also possible that a is concealing a slandering\nattack, where it unfairly rated b. Even though information such as the intention of an agent\nthus cannot be extracted from the feedback history alone, we can use the above condi-\ntions to characterize a trust algorithm’s behaviour as to whether reputation evolves as it\nshould.\nAssuming that all agents are pre-trusted equally (i.e., r0(∗, b) = 0.5), Table 8 and\n\nFig. 12 show the evolution of r0(∗, b). When f3(a, b) (a negative feedback) occurred, r(∗, b)\n\nTable 8 Evolution due to direct interactions only—Scenario 1\n\ni Assessor Assessee f r δf δr\n\n1 a b 1.0 1.0 0.5 0.5\n\n2 a b 1.0 1.0 0 0\n\n3 a b 0 0.66 −1.0 −0.33\n\n4 a b 1.0 0.75 0.33 0.08\n\n5 a b 1.0 0.8 0.25 0.05\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 24 of 27\n\nFig. 12 Evolution of reputation w.r.t feedback – Scenario 1\n\ndecreased but it increased at a slower rate when f4(a, b) (a positive feedback) occured.\nThus, PeerTrust has respected conditions 5.1 and 5.2.\nIn a separate experiment, we also noted that if there were two consecutive negative\n\nfeedbacks on b, then r(∗, b) decreased to 0.5. This suggests that an attacker can know\nexactly how many positive feedbacks are required in order to restore his lost reputation\nafter misbehaving.\n\nScenario 2: evolution due to direct and indirect interactions In this experiment, we use\nthe feedback history provided in Table 9. Initially, agent c provided a positive feedback to\na (f1), but later provides a negative feedback (f4). Because b’s reputation score is dependent\non a’s reputation score, a negative change in a’s reputation should affect b’s reputation.\nHowever, results show that it is unaffected. We explain the reason in what follows.\nIf we compare the reputation values shown in Table 10, we note that r(a) changed from\n\n1 to 0.5. However, this change did not affect r(b) after adding f5. This is because of the\nnormalization technique used by PeerTrust to calculate b’s reputation.\nThus, PeerTrust did not propagate trust as one would have expected (i.e., if c does not\n\ntrust a, it should not trust b either) and this can be easily exploited by attackers. For\nexample, if agents a and b both are part of a collusive group, then agent c cannot recognize\nthis and therefore cannot break away from a collusive group of attackers.\n\nTable 9 Evolution due to direct and indirect interactions\n\ni Assessor Assessee f r\n\n1 c a 1.0 0\n\n2 a b 0.9 0.9\n\n3 a b 0.9 0.9\n\n4 c a 0 0.9\n\n5 a b 1.0 0.93\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 25 of 27\n\nTable 10 Reputation output by PeerTrust\n\ni Assessor Assessee f r(∗, b)\n1 c a 1.0 0\n\n2 a b 0.9 0.9\n\n3 a b 0.9 0.9\n\n4 c a 0 0.9\n\n5 a b 1.0 0.93\n\nLimitations\n\n1. By definition, the global trust of an agent reflects the impact of feedbacks by all\nother agents in the system. To verify the conditions presented at the beginning of\nthis section, we only need to know the order of feedbacks for an agent, regardless of\nthe assessor. However, verifying the evolution of local trust scores according to the\nabove conditions is subjective. Suppose we obtained Table 11 from a local trust\nalgorithm. If we only consider direct experience between a and b, then we observe\nthat r(a, b) increased from 0.1 to 0.3 despite a negative feedback (f6), but this\nincrease may have been contributed by f5 and the fact that r(a, c) = 0.9. That is,\nr(a, b) is an aggregated score obtained through direct and indirect experiences and,\ndepending on the algorithm, different weights may be given to direct versus\nindirect experience [26].\n\nConclusions\nWe provided details of our prototype and evaluated trust algorithms for vulnerabili-\nties to attacks and adherence to trust properties. We were able to show that our model\ncould indeed accommodate various algorithms, and that the specification of experiments\nin terms of the model was straightforward and easy to express programmatically. The\nexperiments that we ran allowed us to make the following observations:\n\n• We were able to exhibit discrepancies between the outputs of EigenTrust and\nPeerTrust due to the way the initial normalized local trust values are calculated in\nEigenTrust; EigenTrust ignores the excess positive feedbacks once they outnumber\nnegative feedbacks;\n\n• There are tradeoffs between sensitivity to self-promotion and sensitivity to\nslandering. For example, an algorithm that is overly sensitive to slandering might take\ntoo long to incorporate bad feedback, and this allows bad behavior to go unnoticed\nfor a while. In general, the difficulty is that one cannot always detect an attack just by\nlooking at feedback history, since multiple interpretations are always possible.\n\nTable 11 Local reputation example\n\ni Assessor Assessee f r(a, b) r(a, c) r(c, b)\n\n1 a c 0.9 - 0.9 -\n\n2 a b 0.6 0.6 0.9 -\n\n3 a b 0.7 0.65 0.9 -\n\n4 a b 0.2 0.1 0.9 -\n\n5 c b 0.9 - 0.9 0.4\n\n6 a b 0 0.3 0.9 0.4\n\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 26 of 27\n\nClarification may come with the accumulation of feedback data, but multi-agent\nsystems might not be able to survive to see that accumulation, unless a certain\nnumber of pre-trusted nodes can be provided to bootstrap the system.\n\n• We were able to express and verify basic trust properties, such as weak transitivity\nand the fact that trust is more easily lost than gained. These should be building blocks\nupon which more complex properties could be expressed.\n\nWe have identified the following limitations and future work:\n\n• Since the feedback history graph limits itself to agent-to-agent transaction ratings,\nrecommender systems such as Credence [10] that use agent-to-object ratings cannot\nbe included in the testbed.\n\n• Trust systems that rely on different agent types (advisors, trusters, trustees, etc.) such\nas in [17] cannot be accommodated in our model.\n\n• Our framework cannot accommodate trust algorithms, such as REGRET [27] or FIRE\n[28], that use inputs other than feedback history.\n\n• Different agents might have different reputations in different contexts. Our testbed\ndoes not explicitly represent this context. One would have to create separate graphs\nfor each context (thus making the assumption that the contexts are independent\nfrom one another), and this might be an oversimplification in cases where trust in\none context partially influences trust in another context. We note however that in\nmost of the trust algorithms we have classified the notion of context is not explicitly\ntaken into account either.\n\n• Agent behavior simulation: it would be desirable to use agent simulations to\nautomatically generate feedback history graphs (stage 1 of the workflow). Such a\nfeature would be useful for designing large-scale experiments with each agent acting\nin a different manner.\n\n• Support for distrust: distrust indicates how much an agent is not trusted (opposite of\ntrustworthiness) and algorithms such as the distrust-aware version of Appleseed\ncompute both trust and distrust propagation. Because our reputation graphs do not\ninclude distrust information, such algorithms cannot be evaluated using our testbed.\n\nIn addition to addressing the above limitations in our model, future work involves\nimplementing more trust algorithms, building a user interface for our testbed and\nperforming large-scale experiments. In particular, experiments using large datasets from\nwebsites such as Epinions, Advogato, and Facebook can yield interesting results.\n\nEndnotes\n1In the remainder of the paper we will simply refer to the trust systems or models that\n\nhave no specific name by the name of the first author of the paper we are citing, so [5]\nwill be referred to as “Aberer”.\n\n2in what follows we will use the graph name and the labelling functions\ninterchangeably to reduce extraneous notations\n\n3Note that in this figure, we have included “0” in A × A �→ {0, 1} for a trust graph for\nclarity but in reality, it indicates that there is no edge.\n\n4http://code.google.com/p/repsystestbed/\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nhttp://code.google.com/p/repsystestbed/\n\n\nChandrasekaran and Esfandiari Journal of Trust Management (2015) 2:8 Page 27 of 27\n\nAuthors’ contributions\nPC did the implementation and ran the experiments, and participated in the formalization, the state of the art work and\nthe write-up. BE participated in the formalization, the state of the art work and the write-up. Both authors read and\napproved the final manuscript.\n\nReceived: 24 November 2014 Accepted: 3 July 2015\n\nReferences\n1. Kamvar SD, Schlosser MT, Garcia-Molina H (2003) The eigentrust algorithm for reputation management in p2p\n\nnetworks. In: WWW ’03 Proceedings of the 12th International Conference on World Wide Web. ACM, Budapest,\nHungary. pp 640–651. http://doi.acm.org/10.1145/775152.775242\n\n2. Xiong L, Liu L (2004) Peertrust: Supporting reputation-based trust for peer-to-peer electronic communities. IEEE\nTrans Knowl Data Eng 16(7):843–857\n\n3. Ziegler CN (2005) Chap. On propagating interpersonal trust in social networks. In: Computing with Social Trust.\nSpringer, London. pp 133–166\n\n4. Zimmermann PR (1995) The Official PGP User’s Guide. MIT Press, Cambridge, MA, USA\n5. Aberer K, Despotovic Z (2001) Managing trust in a peer-2-peer information system. In: CIKM ’01 Proceedings of the\n\nTenth International Conference on Information and Knowledge Management. ACM, Atlanta, GA. pp 310–317\n6. Teacy WTL, Patel J, Jennings N, Luck M (2006) Travos: Trust and reputation in the context of inaccurate information\n\nsources. Autonomous Agents Multi-Agent Syst 12(2):183–198. doi:10.1007/s10458-006-5952-x\n7. Jøsang A, Ismail R (2002) The beta reputation system. In: Proceedings of the 15th Bled Electronic Commerce\n\nConference. pp 41–55\n8. Levien R (2009) Chap. Attack-resistant Trust Metrics. In: Computing with Social Trust. Springer, London. pp 121–132\n9. Golbeck JA (2005) Computing and applying trust in web-based social networks. PhD thesis, University of Maryland\n\nat College Park\n10. Walsh K, Sirer EG (2006) Experience with an object reputation system for peer-to-peer filesharing. In: NSDI’06\n\nProceedings of the 3rd Conference on Networked Systems Design and Implementation. Usenix, Berkeley, USA\n11. Jøsang A, Pope S (2005) Semantic constraints for trust transitivity. In: Proceedings of the 2nd Asia-Pacific Conference\n\non Conceptual Modelling - Volume 43. APCCM ’05, Australian Computer Society, Inc., Darlinghurst, Australia,\nAustralia. pp 59–68. http://dl.acm.org/citation.cfm?id=1082276.1082284\n\n12. Hoffman K, Zage D, Nita-Rotaru C (2009) A survey of attack and defense techniques for reputation systems. ACM\nComput Surv 42:1–1131\n\n13. Marsh SP (1994) Formalising trust as a computational concept. PhD thesis, University of Stirling\n14. Abdul-Rahman A, Hailes S (2000) Supporting trust in virtual communities. In: HICSS ’00: Proceedings of the 33rd\n\nHawaii International Conference on System Scie", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNDkzLTAxNS0wMDE5LXoucGRm0", "metadata_author": "Partheeban Chandrasekaran", "metadata_title": "Toward a testbed for evaluating computational trust models: experiments and analysis", "metadata_creation_date": "2015-09-04T09:59:41Z", "keyphrases": [ "computational trust models", "testbed", "experiments", "analysis" ] }, { "@search.score": 1, "content": "\nImproving prediction with enhanced \nDistributed Memory‑based Resilient Dataset \nFilter\nSandhya Narayanan1*, Philip Samuel2 and Mariamma Chacko3\n\nIntroduction\nAnalyzing and processing massive volumes of data in different applications like sensor \ndata, health care and e-Commerce require big data processing technologies. Extracting \nuseful information from the enormous size of unstructured data is a crucial thing. As the \namount of data becomes more extensive, sophisticated pre-processing techniques are \nrequired to analyze the data. In social networking sites and other online shopping sites, \na massive volume of online product reviews from a large size of customers are available \n[1]. The impact of online product reviews affects 90% of the current e-Commerce mar-\nket [2]. Customer reviews contribute the product sale to an extent and product life in the \nmarket depends on online product recommendations.\n\nOnline feedback is one of the communication methods which gives direct suggestions \nfrom the customers [3, 4]. Online reviews and ratings from customers are another infor-\nmation source about product quality [5, 6]. Customer reviews can help to decide on a new \nsuccessful product launch. Online shopping has several advantages over retail shopping. In \nretail shopping, the customers visit the shop and receive price information but less product \n\nAbstract \n\nLaunching new products in the consumer electronics market is challenging. Develop-\ning and marketing the same in limited time affect the sustainability of such companies. \nThis research work introduces a model that can predict the success of a product. A \nFeature Information Gain (FIG) measure is used for significant feature identification \nand Distributed Memory-based Resilient Dataset Filter (DMRDF) is used to eliminate \nduplicate reviews, which in turn improves the reliability of the product reviews. The \npre-processed dataset is used for prediction of product pre-launch in the market using \nclassifiers such as Logistic regression and Support vector machine. DMRDF method is \nfault-tolerant because of its resilience property and also reduces the dataset redun-\ndancy; hence, it increases the prediction accuracy of the model. The proposed model \nworks in a distributed environment to handle a massive volume of the dataset and \ntherefore, it is scalable. The output of this feature modelling and prediction allows the \nmanufacturer to optimize the design of his new product.\n\nKeywords: Distributed Memory-based, Resilient Distribution Dataset, Redundancy\n\nOpen Access\n\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/.\n\nRESEARCH\n\nNarayanan et al. J Big Data (2020) 7:13 \nhttps://doi.org/10.1186/s40537‑020‑00292‑y\n\n*Correspondence: \nnairsands@gmail.com \n1 Information Technology, \nSchool of Engineering, \nCochin University of Science \n& Technology, Kochi 682022, \nIndia\nFull list of author information \nis available at the end of the \narticle\n\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://creativecommons.org/licenses/by/4.0/\nhttp://crossmark.crossref.org/dialog/?doi=10.1186/s40537-020-00292-y&domain=pdf\n\n\nPage 2 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ninformation from shop owners. On the other hand, online shopping sites give product \nreviews and previous customer feedbacks without extra cost and effort for the customers \n[7–10].\n\nInvesting in poor quality products potentially affects an industry’s brand loyalty and this \nstrategy should be changed by the eCommerce firms [5, 11]. Consumer product success \ndepends on different criteria, such as the quality of the product and marketing strategies. \nThe users should provide their valuable and accurate reviews about the products [12]. Cus-\ntomers bother to give reviews about products, whether they liked it or not. If the users \nprovide reviews, then other retailers can create some duplicated reviews [13, 14]. In online \nmarketing, the volume and value of product reviews are examined [15, 16]. The number \nof the product reviews on the shopping sites, blogs and forums has increased awareness \namong the users. This large volume of the reviews leads to the need for significant data \nprocessing methods [17, 18]. The value is the rating on the products. The ratio of positive to \nnegative reviews about the product leads to the quality of the product [19, 20].\n\nFeature selection is a crucial phase in data pre-processing [21]. Selecting features from \nan un-structured massive volume of data reduce the model complexity and improves the \nprediction accuracy. Different feature selection methods existing are the filter, wrapper and \nembedded. The wrapper feature selection method evaluates the usefulness of the feature \nand it depends on the performance of the classifier [22]. The filter method calculates the \nrelevance of the features and analyzes data in a univariate manner. The embedded process \nis similar to the wrapper method. Embedded and wrapper methods are more expensive \ncompared to the filter method. The state-of-art methods in customer review analysis gener-\nally discuss on categorizing positive and negative reviews using different natural language \nprocessing techniques and spam reviews recognition [23]. Feature selection of customer \nreviews increases prediction accuracy, thereby improves the model performance.\n\nAn enhanced method, which is a combination of filter and wrapper method is proposed \nin this work, which focuses on product pre-launch prediction with enhanced distributive \nfeature selection method. Since many redundant reviews are available on the web in large \nvolumes, a big data processing model has been implemented to filter out duplicated and \nunreliable data from customer reviews in-order to increase prediction accuracy. A scalable \nbig data processing model has been applied to predict the success or failure of a new prod-\nuct. The realization of the model has been done by Distributed Memory-based Resilient \nDataset Filter with prediction classifiers.\n\nThis paper is organized as follows. “Related work” section discusses related work. “Meth-\nodology” section contains the proposed methodology with System design, Resilient Distrib-\nuted Dataset and Prediction using classifiers. “Results and discussions” section summarizes \nresults and discussion. The conclusion of the paper is shown in “Conclusion and future \nwork” section.\n\nRelated work\nMakridakis et al. [24] illustrate that machine learning methods are alternative methods \nfor statistical analysis of multiple forecasting field. Author claims that statistical methods \nare more accurate than machine learning [25] methods. The reason for less accuracy is \nthe unknown values of data i.e., improper knowledge and pre-processing of data.\n\n\n\nPage 3 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nDifferent works have been implemented using the Matrix factorization (MF) [14] \nmethod with collaborative filtering [26]. Hao et al. [15] focused on a work based on the \nfactorization of the user rating matrix into two vectors, i.e., user latent and item latent \nwith low dimensionality. The sum of squared distance can be minimized by training a \nmodel that can find a solution using Stochastic Gradient Decent [27] or by least squares \n[28]. Salakhutdinov et al. [29] proposed a method that can be scaled linearly by probabil-\nity related matrix factorization on a big volume of datasets and then comparing it with \nthe single value decomposition method. This matrix factorization outperforms other \nprobability factorization methods like Bayesian-based probabilistic analysis [29] and \nstandard probability-based matrix factorization methods. A conventional approach, like \ntraditional collaborative Filtering [13, 30] method depends on customers and items. The \nuser item matrix factorization technique has been used for implementation purpose. \nIn the recommender system, there is a limitation in the sparsity problem and cold start \nproblem. In addition to the user item matrix factorization method, various analyses and \napproaches have been implemented to solve these recommendation issues.\n\nWietsma et al. [31] proposed a recommender system that gives information about the \nmobile decision aid and filtering function. This has been implemented with a study of \n29 features of student user behavior. The result shows the correlation among the user \nreviews and product reviews from different websites. Jianguo Chen et al. [32] proposed \na recommendation system for the treatment and diagnosis of the diseases. For cluster \nanalysis of disease symptoms, a density-peaked method is adopted. A rule-based apriori \nalgorithm is used for the diagnosis of disease and treatment. Asha et al. [33] proposed \nthe Gini-index feature method using movie review dataset. The sentimental analysis \nof the reviews are performed and opinion extraction of the sentences are done. Gini-\nindex impurity measure improves the accuracy of the polarity prediction by sentimental \nanalysis using Support vector machine [34, 35]. Depending on the frequency of occur-\nrence of a word in the document, the term frequency is calculated and opinion words \nare extracted using the Gini-index method. In this method, high term frequency words \nare not included, as it decreases the precision. The disadvantage of this method is that \nfor the huge volume of data, the prediction accuracy decreases.\n\nLuo et al. [36] proposed a method based on historical data to analyze the quality of \nservice for automatic service selection. Liu et al. [37] proposed a system in a mobile envi-\nronment for movie rating and review summarization. The authors used Latent Semantic \nAnalysis (LSA-based) method for product feature identification and feature-based sum-\nmarization. Statistical methods [38] have been used for identifying opinion words. The \ndisadvantage of this method is that LSA-based method cannot be represented efficiently; \nhence, it is difficult to index based on individual dimensions. This reduces the prediction \naccuracy in large datasets.\n\nLack of appropriate computing models for handling huge volume and redundancy in \ncustomer review datasets is a major challenge. Another major challenge handled in the \nproposed work is the existence of a pre-launch product in the industry based on the \nproduct features, which can be predicted based on the customer feedback in the form \nof reviews and ratings of the existing products. This prediction helps to optimize the \ndesign of the product to improve its quality with the required product features. Many \nof the relational database management systems are handling structured data, which is \n\n\n\nPage 4 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nnot scalable for big data that handles a large volume of unstructured data. This proposed \nmodel solves the problem of redundancy in a huge volume of the dataset for better pre-\ndiction accuracy.\n\nMethodology\nA pre-launch product prediction using different classifiers has been analysed by huge \ncustomer review and rating dataset. The product prediction is done through the phases \nconsisting of data collection phase, feature selection and duplicate data removal, build-\ning prediction classifier, training as well as testing.\n\nFigure 1 describes the various stages in system design of the model. The input dataset \nconsists of multivariate data which includes categorical, real and text data. Input dataset \nis fed for data pre-processing. Data pre-processing consists of feature selection, redun-\ndancy elimination and data integration which is done using Feature Information Gain \nand Distributed Memory-based Resilient Dataset Filter approach. The cleaned dataset \nis trained using classification algorithms. The classifiers considered for training are Sup-\nport Vector Machine (SVM) and Logistic Regression (LR). Further the dataset is tested \nfor pre-launch prediction using LR and SVM.\n\nData collection phase\n\nThis methodology can be applied for different products. Several datasets like Ama-\nzon and flip cart customer reviews are available as public datasets [39–41]. The data-\nset of customer reviews and ratings of seven brands of mobile phones for a period of \n24 months are considered in this work. The mobile phones product reviews are chosen \nbecause of two reasons. New mobile phones are launched into the market industry day \nby day which is one of the unavoidable items in everyone’s life. Market sustainability for \nthe mobile phones is very low.\n\nTable  1 shows a sample set of product reviews in which input dataset consists of \nuser features and product features. User features consists of Author, ReviewID and \nTitle depending on the user. Product feature consists of Product categories, Overall \nratings and Review Content. Since mobile phone is taken as the product, the catego-\nrization is done according to the features such as Battery life, price, camera, RAM, \n\nData collection \n\nCategorical\n\nText\n\nReal\n\nData Pre-\nprocessing\n\nFeature \nIdentification\n\nRedundancy\nRemoval\n\nData \nIntegration\n\nTraining \nDataset Using \nclassification \nalgorithms\n\nSupport \nVector \n\nLogistic \nRegression\n\nTesting Dataset \nUsing \n\nclassification \nalgorithms\n\nLogistic \nRegression\n\nSupport \nVector \n\nFig. 1 Product prelaunch prediction System Design\n\n\n\nPage 5 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nprocessor, weight etc. Some features are given a priority weightage depending on the \nproduct and user requirements. Input dataset with JSON file format is taken.\n\nDataset pre‑processing\n\nIn data pre-processing, feature selection plays a major role. In the product review \ndataset of a mobile phone, a large number of features exist. Identifying a feature from \ncustomer reviews is important for this model to improve the prediction accuracy. \nEnhanced Feature Information Gain measure has been implemented to identify sig-\nnificant feature.\n\nFeatures are identified based on the content of the product reviews, ratings of the \nproduct reviews and opinion identification of the reviews. Ratings of the product \nreviews can be further categorized based on a rating scale of 5 (1—Bad, 2—Average, \n3—Good, 4—very good, 5—Excellent). For opinion identification of the product, the \npolarity of extracted opinions for each review is classified using Senti-WordNet [42].\n\nFeature Information Gain measures the amount of information of a feature \nretrieved from a particular review. Impurity which is the measure of reliability of fea-\ntures in the input dataset should be reduced to get significant features. To measure \nfeature impurity, the best information of a feature obtained from each review is calcu-\nlated as follows\n\n• Let Pi be the probability of any feature instance \n(\n\nf\n)\n\n of k feature set F =\n{\n\nf1, f2, . . . fk\n}\n\n \nbelonging to ith customer review Ri , where i varies from 1 to N.\n\n• Let N denotes the total number of customer reviews.\n• Let OR denotes the polarity of extracted opinions of the Review.\n• Let SR denotes product rating scale of review (R).\n\nTable 1 Sample set of Product Reviews\n\n\n\nPage 6 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe information of a feature with respect to review rating and opinion is denoted by \nIf\n\nExpected information gain of the feature denoted as Ef\n\nReview Feature Impurity R(I) is calculated as\n\nThen Feature Information Gain (�G) to find out significant features are calculated \nas\n\nFeatures are selected based on the �G value and those with an Information gain \ngreater than 0.5 is selected as a significant feature. Table 2 shows the significant fea-\nture from customer reviews and ratings.\n\nNext step is to eliminate the redundant reviews and to replace null values of an \nactive customer from the customer review dataset using an enhanced big data pro-\ncessing approach. Reviews with significant features obtained from feature identifica-\ntion are considered for further processing.\n\n(1)If = log2\n\n(\n\n1\n\nP(R = F)\n\n)\n\n∗ OR ∗ SR.\n\n(2)Ef =\n\nN\n∑\n\ni=1\n\n−Pi(R = F).\n∥\n\n∥If\n∥\n\n∥\n\n1\n.\n\n(3)R(I) = −\n\nN\n∑\n\ni=1\n\nPi.log2Ef .\n\n(4)�G = R(I)−\n\nN\n∑\n\ni=1\n\n[(\n\nOR\n\nN\n∗ Ef\n\n)\n\n−\n\n(\n\nSR\n\nN\n∗ Ef\n\n)]\n\n.\n\nTable 2 Significant Features from Customer Reviews and Ratings\n\nNo Customer reviewed features No Customer reviewed features\n\n1 Author 17 RAM\n\n2 Title 18 Sim type\n\n3 ReviewID 19 Product category\n\n4 Content 20 Thickness\n\n5 Product brand 21 Weight of mobile phone\n\n6 Ratings 22 Height\n\n7 Battery life 23 Product type\n\n8 Price 24 Product rating\n\n9 Feature information gain 25 Front camera\n\n10 Review type 26 Back camera\n\n11 Product display 27 Opinion of review\n\n12 Processor 28 Multi-band\n\n13 Operating system 29 Network support\n\n14 Water proof 30 Quick charging\n\n15 Rear camera 31 Finger sensor\n\n16 Applications inbuilt 32 Internal storage\n\n\n\nPage 7 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResilient Distributed Dataset\n\nResilient Distributed Dataset (RDD) [43] is a big data processing approach, which allows \nto store cache chunks of data on memory and persevere it as per the requirements. \nThe in-memory data caching is supported by RDD. Variety of jobs at a point of time \nis another challenge which is handled by RDD. This method deals with chunks of data \nduring processing and analysis. RDD can also be used for machine learning supported \nsystems as well as in big data processing and analysis, which happens to be an almost \npervasive requirement in the industry.\n\nIn the proposed method the main actions of RDD are:\n\n• Reduce (β): Combine all the elements of the dataset using the function β.\n• First (): This function will return the first element\n• takeOrdered(n): RDD is returned with first ‘n’ elements.\n• saveAsSequenceFile(path): the elements in the dataset to be written to the local file \n\nsystem with given path.\n\nThe main Transformations of RDD are:\n\n• map(β): Elements from the input file is mapped and new dataset is returned through \nfunction β.\n\n• filter(β): New dataset is returned if the function β returns true.\n• groupBykey(): When called a dataset of (key, value) pairs, this function returns a \n\ndataset of (key, value) pairs.\n• ReduceBykey(β): A (key, value) pair dataset is returned, where the values of each key \n\nare combined using the given reduce function β.\n\nIn the proposed work an enhanced Distributed Memory-based Resilience Dataset \nFilter (DMRDF) is applied. DMRDF method have long Lineage and it is recomputed \nthemselves using prior information, thus it achieves fault-tolerance. DMRDF has been \nimplemented to remove the redundancy in the dataset for product pre-launch predic-\ntion. This enhanced method is simple and fast.\n\n• Let the list of n customers represented as C = {c1, c2, c3 . . . , cn}\n\n• Let the list of N reviews be represented as R = {r1, r2, r3 . . . , rN }\n\n• Let x significant features are identified from feature set (F ) represented as Fx ⊂ F\n\n• An active customer consists of significant feature having information Gain value \ndenoted by �G\n\nIn the DMRDF method, a product is chosen and its customer reviews are found out. \nEliminate customers with similar reviews on the selected product and also reviews \nwith insignificant features. Calculate the memory-based Resilient Dataset Filter score \nbetween each of the customer reviews with significant features.\n\nLet us consider a set C of ‘n’ number of customers, the set R of ‘N’ number of reviews and \na set of significant features ′F ′\n\nx are considered. The corresponding vectors are represented \nas KC , KR and KFx . Then KRi is represented using a row vector and KFj is represented using \nthe column vector. Each entry KCm denote the number of times the mth review arrives in \n\n\n\nPage 8 of 15Narayanan et al. J Big Data (2020) 7:13 \n\ncustomers. The similarities between ith review of mth customer is found out using L1 norm \nof KRi and KCm . The Distributed Memory-based resilient filter score δ is calculated using the \nEq. (5).\n\nThe δ score is calculated for each customer review whereas the score lies between [0,1]. \nThe significant features are found out using Eq. 4. For customer reviews without significant \nfeatures, �G value will be zero. The reviews with δ score value 0 are found to be insignificant \nwithout any significant feature or opinion and hence those reviews are eliminated and not \nconsidered for further processing in the work. More than one Distributed Memory-based \nresilient filter score value is identified then the second occurrence of the review is consid-\nered as duplicate.\n\nPrediction classifiers\n\nLogistic regression and Support Vector Machine classifiers are the supervised machine \nlearning approaches used in the proposed work for product pre-launch prediction.\n\nLogistic regression (LR)\n\nWe have implemented proposed model using logistic regression analysis for prediction. \nThis model predicts the failure or success of a new product in the market by analysing \nselected product features from customer reviews. A case study has been conducted using \nthe dataset of customer reviews of mobile phones. Success or failure is the predictor vari-\nable used for training and testing the dataset. For training the model 75% of the dataset is \nused and for testing the model, remaining 25% is used.\n\n• Let p be the prediction variable value, assigning 0 for failure and 1 for success.\n• p0 is the constant value.\n• b is the logarithmic base value.\n\nThen the logit function is,\n\nThen the Logistic regression value γ is shown in Eq. (7),\n\n(5)δ =\n\nN\nn\n�\n\ni = 1\n\nm = 1\n\n\n\n\n\n�\n\nKRi ∗\n\n�\n\n�x\nj=1 KFj\n\n��\n\n∗ KCm\n\nKRi · KCm\n\n\n\n ∗ |�G|\n\n(6)\nL0 = b\n\np0+p\nx\n∑\n\ni=1\n\nfi\n\n(7.1)γ =\nL0\n\n(\n\nbp0+p\n∑x\n\ni=1 fi\n)\n\n+ 1\n\n(7.2)=\n1\n\n1+ b\n−\n\n(\n\nb\np0+p\n\n∑x\ni=1\n\nfi\n)\n\n\n\nPage 9 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nThe probability value of γ lies between [0,1]. In this work, if this value is greater than 0.5 \nthe pre-launch prediction of the product is considered as success and for values less than \n0.5, it is considered as failure.\n\nSupport Vector Machine (SVM)\n\nSVM is the supervised machine learning method, used to learn from set of data to get new \nskills and knowledge. This classification method can learn from data features relationships \n( zi ) and its class \n\n(\n\nyi\n)\n\n that can be applied to predict the success or failure class the product \nbelongs to.\n\n• For a set T of t training feature vectors, zi ∈ RD, where i = 1 to t.\n• Let yi ∈ {+1,−1} , where +1 belongs to product success class and -1 belongs to product \n\nfailure class.\n• The data separation occurs in the real numbers denoted as X in the D dimensional \n\ninput space.\n• Let w be the hyper plane normal vector element, where w ∈ XD.\n\nThe hyper plane is placed in such a way that distance between the nearest vectors of the \ntwo classes to the hyperplane should be maximum. Thus, the decision hyper plane is calcu-\nlated as,\n\nThe conditions for training dataset d ∈ X , is calculated as\n\nTo maximize the margin the value of w should be minimized.\nThe products in the positive one class (+1) are considered as successful products, [from \n\nEq. (9)] and those in the negative one class (−1) [from Eq. (10)] are in failure class.\n\nExperimental setup\n\nThe proposed system was implemented using Apache Spark 2.2.1 framework. Spark pro-\ngramming for python using PySpark version 2.1.2, which is the Spark python API has been \nused for the application development. An Ubuntu running Apache web server using Web \nServer Gateway Interface is used. Amazon Web Services is used to run some components \nof the software system large servers (nodes), having two Intel Xeon E5-2699V4 2.2 G Hz \nprocessors (VCPUs) with 4 cores and 16 GB of RAM on different Spark cluster configura-\ntions. According to the scalability requirements the software components can be config-\nured and can run on separate servers.\n\n(8)α(w) =\n2\n\n�w�\n\n(9)wtzi + d ≥ 1, where yi = +1.\n\n(10)wtzi + d ≤ −1, whereyi = yi − 1.\n\n\n\nPage 10 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nResults and discussions\nTo evaluate our prediction system several case studies have been conducted. Support \nVector Machine and Logistic regression classifiers are employed to perform the predic-\ntion. Most significant customer review features are used to analyse the system perfor-\nmance. The prediction accuracy evaluation is taken as one of the system design factors. \nThe system response time is another major concern for big data processing system. In \nthe customer review feature identification, we propose feature information gain and \nDMRDF approach to identify significant features and to eliminate redundant customer \nreviews from the input dataset.\n\nFigure  2 illustrates significant features required for the mobile phone sustainability. \nCustomer reviews and ratings of 7 brands of mobile phones are identified and evalu-\nated with DMRDF using SVM and LR. The graph shows the significant features identi-\nfied by the model against the percentage of customers whose reviews are analysed. 88% \nof the customers identified internal storage as a significant feature. Product price has \nbeen identified by 79% of customers as significant feature. With this evaluation customer \nrequirements for a product can be analysed in a better manner, thus can optimize the \ndesign of the product for better product quality and for product sustainability in the \nindustry.\n\nFigure 3 shows the comparison of the processing time taken by the proposed model \nwith different dataset size against that of the state of art techniques. DMRDF method \ntakes less time for completion of the application compared to other gini-index and latent \nsemantic analysis methods. Hence the proposed model is fast and scalable. It provides a \nhigh-speed processing performance with large datasets. This shows the DMRDF applica-\nbility in big data analytics, whereas gini-index and LSA-based methods processing time \nis larger for large volume of dataset. From the Fig. 3 it can be seen that with 9 GB dataset \ntime taken for prediction using LSA-based model, Gini-index model and DMRDF model \nis 342 s, 495 s and 156 s respectively. With 18 GB dataset time taken for prediction using \nLSA-based model, Gini-index model and DMRDF model 740 s, 910 s and 256 s respec-\ntively. Gini-index and LSA-based methods time taken for 18 GB dataset is twice that of \n9 GB dataset. But for DMRDF model time taken for 18 GB dataset is 1.6 times that of \n\n79%\n\n15%\n\n45%\n35%\n\n22%\n\n40%\n\n22%\n\n39%\n\n88%\n\n53%\n\n21%\n\n61%\n\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n\n100%\n\nPe\nrc\n\nen\nta\n\nge\n o\n\nf C\nus\n\nto\nm\n\ner\ns\n\nIden�fied Significant Features\nFig. 2 Identified Significant Features from Customer reviews and Ratings\n\n\n\nPage 11 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n9 GB dataset and also it is 3 times lesser than Gini-index method. DMRDF model has \nmore advantage compared to the other state of art techniques in the case of application \nexecution and performance.\n\nThe reliability of the methods considered for the pre-launch prediction depends on \nprecision [44], recall and prediction accuracy measurement. Table 5 shows a comparison \nof precision, recall and accuracy measures of DMRDF, Gini-index and LSA-based meth-\nods with Support Vector Machine and Logistic Regression classifiers using customer \nreviews dataset over a period of 24 months. The results shown in Table 3 are best proved \nusing DMRDF with Support Vector Machine classification with prediction accuracy of \n95.4%. The DMRDF outperforms LSA-based and Gini-index methods in P@R, R@R and \nPA measures. Using proposed method, true positive (TP), false positive (FP), true nega-\ntive (TN) and false negative (FN) are found out. The prediction accuracy (PA), precision \n(P@R) and recall (R@R) are computed using Eqs. (10), (11), and (12) respectively.\n\n(10)PA =\nTP + TN\n\nTP + TN + FP + FN\n\n0\n\n100\n\n200\n\n300\n\n400\n\n500\n\n600\n\n700\n\n800\n\n900\n\n1000\n\n1GB 5GB 9GB 13GB 18GB\n\nGini-index\n\nDMRDF\n\nLSA-based\n\nTi\nm\n\ne \nTa\n\nke\nn \n\nin\n se\n\nc\n\nDataset size\nFig. 3 Dataset Size versus Processing Time Graph\n\nTable 3 Performance comparison of the proposed model with state of art techniques\n\nClassifier Support vector machine\n\nMethod used P@R (precision) PA % \n(prediction \naccuracy)\n\nDMRDF 0.941 0.92 95.4\n\nLSA-based 0.894 0.79 87.5\n\nGini-index 0.66 0.567 83.2\n\nClassifier Logistic regression\n\nMethod used P@R R@R % PA %\n\nDMRDF 0.915 0.849 93.5\n\nLSA-based 0.839 0.753 83\n\nGini-index 0.62 0.52 79.8\n\n\n\nPage 12 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nUsing DMRDF with SVM classifier and LR classifier, the prediction accuracy varia-\ntions are less compared to LSA-based and Gini-index methods. Hence DMRDF out-\nperforms the other two methods for customer review feature prediction.\n\nFurthermore Fig.  4, shows the DMRDF, LSA-based and Gini-index approaches as \napplied to the customer reviews and ratings datasets for 3, 6, 12, 18 and 24 months. \nIn DMRDF many features may appear in different customer review aspects, hence \nperformance evaluation will not consider duplicate customer reviews. In Gini- index, \nfeatures are extracted based on the polarity of the reviews and for large dataset P@R \nand R@R are less. The results show that DMRDF method outperforms the other two \nmethods in big data analysis. Gini-index approach does not perform well in customer \nreview feature prediction.\n\nConclusion and future work\nTechnological development in this era brings new challenges in artificial intelligence \nlike prediction, which is the next frontier for innovation and productivity. This work \nproposes the implementation of a scalable and reliable big data processing model \n\n(11)P@R =\nTP\n\nTP + FP\n\n(12)R@R =\nTP\n\nTP + FN\n\na SVM b SVM \n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nc Logistic Regression d Logistic Regression\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nP@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\n0.5\n\n0.6\n\n0.7\n\n0.8\n\n0.9\n\n1\n\n1.1\n\n3 6 12 18 24\n\nR@\nR\n\nReview in Months\n\nLSA-based DMRDF Gini-index\n\nFig. 4 Precision and Recall of DMRDF, LSA-based and Gini-index methods using SVM and LR classifiers\n\n\n\nPage 13 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nwhich identify significant features and eliminates redundant data using Feature Infor-\nmation Gain and Distributed Memory-based Resilient Dataset Filter method with \nLogistic Regression and Support Vector Machine prediction classifiers. A compari-\nson of the analysis has been conducted with state of art techniques like Gini-index \nand LSA-based approaches. The prediction accuracy, precision and recall of DMRDF \nmethod outperforms the other methods. Results show that the prediction accuracy \nof the proposed method increases by 10% using significant feature identification and \nelimination of redundancy from dataset compared to state of art techniques. Large \nfeature dimensionality reduces the prediction accuracy of the LSA-based method \nwhere as number of significant features plays an important role in prediction model-\nling. Results show that proposed DMRDF model is scalable and with huge volume of \ndataset model performance is good as well as time taken for processing the applica-\ntion is less compared to state of art techniques.\n\nResilience property of DMRDF method have long lineage, hence this can achieve \nfault-tolerance. DMRDF model is fast because of the in-memory computation \nmethod. Proposed design can be extended to other product feature identification big \ndata processing domains. As a future work, the model may be developed to make real \ntime streaming predictions through a unified API that searches customer comments, \nratings and surveys from different reliable online websites concurrently to obtain syn-\nthesis of sentiments with an information fusion approach. Since the statistical prop-\nerties of customer reviews and ratings vary over time, the performance of machine \nlearning algorithms can also come down. To cope with the limitations of deep learn-\ning matrix factorization integrated with DMRDF can be adapted.\n\nAbbreviations\nDMRDF: Distributed Memory-based Resilient Dataset Filter; FIG: Feature information gain; RDD: Resilient distributed \ndataset; SVM: Support vector machine; LR: Logistic regression; LSA: Latent semantic analysis; PA: Prediction accuracy; \nP@R: Precision; R@R: Recall; MF: Matrix factorization.\n\nAcknowledgements\nNot applicable.\n\nAuthors’ contributions\nSN designed and implemented the model for Pre-launch product prediction. SN analysed and interpreted the customer \nreviews and ratings dataset regarding the pre-launch product prediction. PS supervised the design, implementation \nand analysis of the model for pre-launch product prediction. MC was a major contributor in writing the manuscript. All \nauthors read and approved the final manuscript.\n\nFunding\nNot applicable.\n\nAvailability of data and materials\nThe datasets generated and/or analysed during the current study are available in the Kaggle repository. [snap.stanford.\nedu/data/web-Amazon.html] [40] and [http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts] [39].\n\nCompeting interests\nThe authors declare that they have no competing interests.\n\nAuthor details\n1 Information Technology, School of Engineering, Cochin University of Science & Technology, Kochi 682022, India. \n2 Department of Computer Science, Cochin University of Science & Technology, Kochi 682022, India. 3 Department \nof Ship Technology, Cochin University of Science & Technology, Kochi 682022, India. \n\nReceived: 25 October 2019 Accepted: 17 February 2020\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\n\n\nPage 14 of 15Narayanan et al. J Big Data (2020) 7:13 \n\nReferences\n 1. Lau RY, Liao SY, Kwok RC, Xu K, Xia Y, Li Y. Text mining and probabilistic modeling for online review spam detection. \n\nACM Trans Manag Inform Syst. 2011;2(4):25.\n 2. Lin X, Li Y, Wang X. Social commerce research: definition, research themes and the trends. Int J Inform Manag. \n\n2017;37:190–201.\n 3. Matos CAD, Rossi CAV. Word-of-mouth communications in marketing: a meta-analytic review of the antecedents \n\nand moderators. J Acad Market Sci. 2008;36(4):578–96.\n 4. Jeon S, et al. Redundant data removal technique for efficient big data search processing. Int J Softw Eng Appl. \n\n2013;7.4:427–36.\n 5. Dave K, Lawrence S, and Pennock D. Mining the peanut gallery: opinion extraction and semantic classification of \n\nproduct reviews. WWW’2003.\n 6. Zhou Y, Wilkinson D, Schreiber R, Pan R. Large-scale parallel collaborative filtering for the netflix prize. 2008. p. \n\n337–48. https ://doi.org/10.1007/978-3-540-68880 -8_32.\n 7. Zhang KZK, Benyoucef M. Consumer behavior in social commerce: a literature review. Dec Support Syst. \n\n2016;86:95–108.\n 8. Cui Geng, Lui Hon-Kwong, Guo Xiaoning. The effect of online consumer reviews on new product sales. Int J Electron \n\nComm. 2012;17(1):39–58.\n 9. Manek AS, Shenoy PD, Mohan MC, et al. Detection of fraudulent and malicious websites by analysing user reviews \n\nfor online shopping websites. Int J Knowl Web Intell. 2016;5(3):171–89. https ://doi.org/10.1007/s1128 0-015-0381-x.\n 10. Singh S, and Singh N. Big data analytics. In: Proceedings of the 2012 international conference on communication, \n\ninformation & computing technology (ICCICT), institute of electrical and electronics engineers (IEEE). 2012. p. 1–4. \nhttp://dx.doi.org/10.1109/iccic t.2012.63981 80.\n\n 11. Demchenko Yuri et al. Addressing big data challenges for scientific data infrastructure. In: IEEE 4th Int. conference \ncloud computing technology and science (CloudCom). 2012.\n\n 12. Sihong Xie, Guan Wang, Shuyang Lin and Yu Philip S. Review spam detection via time-series pattern discovery. In: \nACM Proceedings of the 21st international conference companion on World Wide Web. 2012. p. 635–6.\n\n 13. Koren Y, Bell R, Volinsky C. matrix factorization technique for recommender systems. Computer. 2009;8:30–7.\n 14. Salakhutdinov R, Mnih A, & Hinton G. Restricted boltzmann machines for collaborative filtering. In: Proc. of the 24th \n\nInt. conference on machine learning. 2007. p. 791–8.\n 15. Hao MA, King I, Lyu MR. Learning to recommend with explicit and implicit social relations. ACM Trans Intell Syst \n\nTechnol. 2011;2(3):29.\n 16. Bandakkanavar V, Ramesh M, Geeta V. A survey on detection of reviews using sentiment classification of methods. \n\nIJRITCC. 2014;2(2):310–4.\n 17. Gu V, and Li H. Memory or time—performance evaluation for iterative operation on hadoop and spark. In: Proc. of \n\nthe 2013 IEEE 10th Int. Con. on high-performance computing and communications. 2013. https ://doi.org/10.1109/\nhpcc.and.euc.2013.106.\n\n 18. Zhang Hanpeng, Wang Zhaohua, Chen Shengjun, Guo Chengqi. Product recommendation in online social net-\nworking communities—an empirical study of antecedents and a mediator. J Inform Manag. 2019;56(2):185–95.\n\n 19. Ghose A, Ipeirotis PG. Designing novel review ranking systems: predicting the usefulness and impact of reviews. In: \nInt Conference Electron Comm ACM. 2007. p. 303–10.\n\n 20. Chong AY, Ch’ng E, Liu MJ, Li B. Predicting consumer product demands via Big Data: the roles of online promotional \nmarketing and online reviews. Int J Prod Res. 2015;55:1–15. https ://doi.org/10.1080/00207 543.2015.10665 19.\n\n 21. Yang H, Fujimaki R, Kusumura Y, & Liu J. Online Feature Selection. In: Proceedings of the 22nd ACM SIGKDD Int. \nConference on KDD ‘16, 2016. https ://doi.org/10.1145/29396 72.29398 81.\n\n 22. Breese JS, Heckerman D, and Kadie C. Empirical analysis of predictive algorithms for collaborative filtering. In: Proc. \nof the 14th Conf. on Uncertainty in Artifical Intelligence, 1998.\n\n 23. Mukherjee A, Kumar A, Liu B, Wang J, Hsu M, Castellanos M, Ghosh R. Spotting opinion spammers using behavioral \nfootprints. In: Proc. of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining \nChicago, ACM. 2013. p. 632–40.\n\n 24. Makridakis S, Spiliotis E, Assimakopoulos V. Statistical and Machine Learning forecasting methods: concerns and \nways forward. PLoS ONE. 2018;13(3):e0194889. https ://doi.org/10.1371/journ al.pone.01948 89.\n\n 25. Imon A, Roy C, Manos C, Bhattacharjee S. Prediction of rainfall using logistic regression. Pak J Stat Oper Res. 2012. \nhttps ://doi.org/10.18187 /pjsor .v8i3.535.\n\n 26. Chen T, Zhang W, Lu Q, Chen K, Zheng Z, Yu Y. SVD Feature: a toolkit for feature-based collaborative filtering. J Mach \nLearn Res. 2012;13(1):3619–22.\n\n 27. Shi Y, Larson M, Hanjalic A. Collaborative filtering beyond the user-item matrix—a survey of the state of art and \nfuture challenges. ACM Comput Surv. 2014;47(1):3.\n\n 28. Shan H, & Banerjee A. Generalized probabilistic matrix factorizations for collaborative filtering, In Data mining \n(ICDM), IEEE 10th international conference. 2010. p. 1025–30.\n\n 29. Salakhutdinov R, & Mnih A. Bayesian probabilistic matrix factorization using Markov chain Monte Carlo. In: Proc. of \nthe 25th int. conference on machine learning. 2008. p. 880–7.\n\n 30. Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\n 31. Wietsma TA, Ricci F. Product reviews in mobile decision aid systems. Francesco: PERMID; 2005. p. 15–8.\n 32. Jianguo C, et al. A disease diagnosis and treatment recommendation system based on big data mining and cloud \n\ncomputing. Inform Sci. 2018;435:124–49.\n 33. Manek AS, Shenoy PD, Mohan MC, Venugopal KR. Aspect term extraction for sentiment analysis in large movie \n\nreviews using Gini-index feature selection method and SVM classifier. World Wide Web. 2017;20:135–54. https ://doi.\norg/10.1007/s1128 0-015-0381-x.\n\n 34. Fan RE, Chang K-W, Hsieh C-J, Wang X-R, Lin C-J. LIBLINEAR: A library for large linear classification. J Mach Learn Res. \n2008;9:1871–4.\n\nhttps://doi.org/10.1007/978-3-540-68880-8_32\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttp://dx.doi.org/10.1109/iccict.2012.6398180\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1109/hpcc.and.euc.2013.106\nhttps://doi.org/10.1080/00207543.2015.1066519\nhttps://doi.org/10.1145/2939672.2939881\nhttps://doi.org/10.1371/journal.pone.0194889\nhttps://doi.org/10.18187/pjsor.v8i3.535\nhttps://doi.org/10.1007/s11280-015-0381-x\nhttps://doi.org/10.1007/s11280-015-0381-x\n\n\nPage 15 of 15Narayanan et al. J Big Data (2020) 7:13 \n\n 35. Ribeiro MT, Singh S, and Guestrin C. Why should I trust you?: Explaining the predictions of any classifier. In: Proc. \nACMSIGKDD Int. Conf. Knowl. Discov. Data Mining. 2016. p. 1135–44.\n\n 36. Luo X, et al. An effective scheme for QoS estimation via alternating direction method-based matrix factorization. \nIEEE Trans Serv Comput. 2019;12(4):503–18.\n\n 37. Liu CL, Hsaio WH, Lee CH, Lu GC and Jou E. Movie rating and review summarization in mobile environment. In: IEEE \ntrans. systems, man and cybernetics, Part C: applications and reviews. 2012. p. 397–407.\n\n 38. Vapnik, VN. The nature of statistical learning theory, Springer, 2nd ed, 1999. Translated by Xu Jianghua, Zhang Xue-\ngong. Beijing: China Machine Press; 2000.\n\n 39. [Dataset] Flipkart-products. http://www.kaggl e.com/Promp tClou dHQ/flipk art-produ cts.\n 40. [Dataset] https ://snap.stanf ord.edu/data/web-Amazo n.html.\n 41. [Dataset] He R, McAuley J. Ups and downs: modeling the visual evolution of fashion trends with one-class collabora-\n\ntive filtering. WWW; 2016.\n 42. Popescu AM, Etzioni O. Extracting product features and opinions from reviews. 2005; EMNLP.\n 43. Zaharia M, Chowdhury M, Das T, Dave A, Ma J, McCauley M, Franklin M, Shenker S, Stoica I. Resilient distributed \n\ndatasets: A fault-tolerant abstraction for in-memory cluster computing Technical Report UCB/EECS-2011-82. UC \nBerkeley: EECS Department; 2011.\n\n 44. Davis J, Goadrich M. The relationship between precision-recall and ROC curves, In ICML. 2006. p. 233–40.\n 45. Lee JS, Lee ES. Exploring the usefulness of predicting people’s locations. Procedia Soc Beh Sci. 2014. https ://doi.\n\norg/10.1016/j.sbspr o.2014.04.451.\n\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n\nhttp://www.kaggle.com/PromptCloudHQ/flipkart-products\nhttps://snap.stanford.edu/data/web-Amazon.html\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\nhttps://doi.org/10.1016/j.sbspro.2014.04.451\n\n\tImproving prediction with enhanced Distributed Memory-based Resilient Dataset Filter\n\tAbstract \n\tIntroduction\n\tRelated work\n\tMethodology\n\tData collection phase\n\tDataset pre-processing\n\tResilient Distributed Dataset\n\n\tPrediction classifiers\n\tLogistic regression (LR)\n\tSupport Vector Machine (SVM)\n\n\tExperimental setup\n\n\tResults and discussions\n\tConclusion and future work\n\tAcknowledgements\n\tReferences\n\n\n\n\n", "metadata_storage_path": "aHR0cHM6Ly9wcm9qM3N0b3JhZ2UuYmxvYi5jb3JlLndpbmRvd3MubmV0L2xpYnJhcnkvczQwNTM3LTAyMC0wMDI5Mi15LnBkZg2", "metadata_author": "Sandhya Narayanan ", "metadata_title": "Improving prediction with enhanced Distributed Memory-based Resilient Dataset Filter", "metadata_creation_date": "2020-02-24T16:27:45Z", "keyphrases": [ "Distributed Memory-based Resilient Dataset Filter", "Improving prediction" ] } ] }

Q2- https://udacitycognitive.search.windows.net/indexes/azuretable-index/docs?api-version=2021-04-30-Preview&search=*
Output -
{ "@odata.context": "https://udacitycognitive.search.windows.net/indexes('azuretable-index')/$metadata#docs(*)", "value": [ { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGUzY2JiMzgwMC0yNTU0LTQxMjEtYmIzYS1mMzY1ZGViMGMzYjY1", "description": "Learn our best practices for various tools such as Leaflet", "duration": 2, "instructor": "Robert Gillis", "level": "intermediate", "product": "leaflet", "rating_average": 3.9, "rating_count": 28, "role": "developer", "source": "Company Moodle", "title": "Maps", "url": "https://www.example.com/course6", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU1NzhhMzMxOS1hYTdjLTRkMmYtYjZhNC0zOWU5NjM4YjBhODU1", "description": "For administrators, this course will teach you how our CI/CD pipelines work from an operations perspective", "duration": 5, "instructor": "Claudia Blackman", "level": "intermediate", "product": "jenkins", "rating_average": 4.9, "rating_count": 56, "role": "admin", "source": "Company Moodle", "title": "DevOps for Ops", "url": "https://www.example.com/course5", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU2YjVkM2Y1NS1lYjAyLTQ5OWQtOTc3NS0yZTBlMjU2NTllMDc1", "description": "Learn our company's Principles for the Responsible Use of AI", "duration": 1, "instructor": "Eileen Diaz", "level": "intermediate", "product": "NA", "rating_average": 4.3, "rating_count": 24, "role": "architect", "source": "Company Moodle", "title": "Ethics in AI", "url": "https://www.example.com/course12", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU1YjI5MzI4My04MWU2LTRmODktYTBhYi03MDUzOTg4ZDZmNmE1", "description": "Learn how to track billable and non-billable hours by assigning time to projects and other relevant time codes", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.8, "rating_count": 540, "role": "all", "source": "Company Moodle", "title": "Onboarding - Time Tracking ", "url": "https://www.example.com/course1", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGViNTFlZGUxNC0wMjVmLTQ5YWQtOWU5ZS00NGFkMjg0ZWVkZGE1", "description": "For developers, this course will teach you how to hook your dev work into our existing CI/CD pipelines.", "duration": 3, "instructor": "Claudia Blackman", "level": "intermediate", "product": "jenkins", "rating_average": 3.8, "rating_count": 101, "role": "developer", "source": "Company Moodle", "title": "DevOps for Dev", "url": "https://www.example.com/course4", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGUzMGUzYzZlNS05NDE1LTRkODUtODIyOS1jMjEzMzIwM2M1MzU1", "description": "Learn the policies related to the distribution and use of computers, phones, software, and other technology", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.9, "rating_count": 550, "role": "all", "source": "Company Moodle", "title": "Onboarding - Technology Policies ", "url": "https://www.example.com/course2", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU4NWVlNzI1Yi00YWUwLTQ3MTktODc4NS1kZGY5OWUxOWZhZjE1", "description": "For administrators, learn our best practices for securing all databases", "duration": 3, "instructor": "Eileen Diaz", "level": "advanced", "product": "SQL", "rating_average": 4.3, "rating_count": 45, "role": "admin", "source": "Company Moodle", "title": "Security for database admins", "url": "https://www.example.com/course8", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMTVjMjk0Ny04ZTRlLTQ0ZjUtYjM0Mi0wNTY0ZjBlZTJmYmY1", "description": "Learn to manage LUIS apps through versioning, key management, handling data, and improving predictions.", "duration": 24, "instructor": "", "level": "intermediate", "product": "azure", "rating_average": 4.77, "rating_count": 106, "role": "developer", "source": "MS Learn", "title": "Manage your Language Understanding Intelligent Service (LUIS) Apps", "url": "https://docs.microsoft.com/en-us/learn/modules/manage-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU5ZGY4NDRmZC1lZWZlLTQ4ODAtODM0MS05MzY3MzIxNzRiYjU1", "description": "Learn our policies for utilizing encryption including key management for projects", "duration": 3, "instructor": "Eileen Diaz", "level": "advanced", "product": "NA", "rating_average": 4.2, "rating_count": 95, "role": "architect", "source": "Company Moodle", "title": "Encryption and security", "url": "https://www.example.com/course14", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGVkM2YwYzk1NS1hYzZlLTRjZWQtYjkxYi1mZmNlZjNlOGNlZGU1", "description": "For developers, learn our best practices for writing secure code for web, server, and desktop development", "duration": 3, "instructor": "Eileen Diaz", "level": "intermediate", "product": "NA", "rating_average": 4.4, "rating_count": 132, "role": "developer", "source": "Company Moodle", "title": "Code security", "url": "https://www.example.com/course9", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMGJhYWE3NS04OWZlLTRmODYtODA1Zi1mMDgzMzZlNmFmNDg1", "description": "Explore the strategic components, use cases, and special factors of an enterprise AI strategy that creates real business value, with INSEAD and Microsoft.", "duration": 70, "instructor": "", "level": "intermediate", "product": "m365", "rating_average": 4.71, "rating_count": 2779, "role": "business-user", "source": "MS Learn", "title": "Define an AI strategy to create business value", "url": "https://docs.microsoft.com/en-us/learn/modules/ai-strategy-to-create-business-value/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMDBhMGQ1Ny1hMGZlLTQzODYtODI5Yy05OTA3NGQxYjNiOWI1", "description": "Find out about automated testing that proves your code to be maintainable, understandable, and functioning without repetitive manual testing.", "duration": 82, "instructor": "", "level": "beginner", "product": "azure-devops", "rating_average": 4.73, "rating_count": 3301, "role": "solution-architect", "source": "MS Learn", "title": "Run quality tests in your build pipeline by using Azure Pipelines", "url": "https://docs.microsoft.com/en-us/learn/modules/run-quality-tests-build-pipeline/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMTk3YzhjNi1kZmM0LTQ1MGItOWZhNy0zZjYxMDk3N2NjNzk1", "description": "Learn about AI Builder Text recognition and how to use it with other Power Platform products.", "duration": 55, "instructor": "", "level": "beginner", "product": "ai-builder", "rating_average": 4.61, "rating_count": 197, "role": "business-user", "source": "MS Learn", "title": "Get started with AI Builder Text recognition", "url": "https://docs.microsoft.com/en-us/learn/modules/get-started-with-ai-builder-text-recognition/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGUxN2IxZWVkYy0wZTk2LTRlNWItODE5OS04M2E0ODQzODhlZmU1", "description": "Learn our internal best practices for using the O365 suite including email signatures, file storage and other issues", "duration": 2, "instructor": "Gerald Dominguez", "level": "beginner", "product": "O365", "rating_average": 4.6, "rating_count": 510, "role": "all", "source": "Company Moodle", "title": "O365", "url": "https://www.example.com/course10", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGUyNzhkMjk5ZS1lZjBlLTQ3ZmItOGU5OC01YTMxYTA3MzUxOWM1", "description": "For developers, learn our best practices for securely connecting to databases", "duration": 2, "instructor": "Eileen Diaz", "level": "advanced", "product": "SQL", "rating_average": 4.8, "rating_count": 115, "role": "developer", "source": "Company Moodle", "title": "Security for database code", "url": "https://www.example.com/course7", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGVjMDE0Y2YzNi0yYTgzLTRkZWQtYmZlNS1kNjJmNDA1YmE5NzA1", "description": "This course will teach you best practices for communicating with your team while working remotely", "duration": 1, "instructor": "Gerald Dominguez", "level": "beginner", "product": "NA", "rating_average": 4.7, "rating_count": 325, "role": "all", "source": "Company Moodle", "title": "Remote work", "url": "https://www.example.com/course11", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGVjNmI3ZmJkMC03MzkwLTQzNzAtYWI3Ny00NTAyNzU5NmI1MjA1", "description": "Understand ways you can be more healthy in the work environment including what ergonomic equipment is available to you", "duration": 1, "instructor": "Mike Montoya", "level": "beginner", "product": "NA", "rating_average": 4.6, "rating_count": 525, "role": "all", "source": "Company Moodle", "title": "Workplace Health", "url": "https://www.example.com/course13", "keyphrases": [ "company", "moodle" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMDJmNDQzNi03MzYwLTRkYWEtYTIxZC1mOWRjZDM1MTg1ODk1", "description": "Enable business users with key AI use cases", "duration": 34, "instructor": "", "level": "beginner", "product": "power-platform", "rating_average": 4.75, "rating_count": 758, "role": "functional-consultant", "source": "MS Learn", "title": "Enable business users with key AI use cases", "url": "https://docs.microsoft.com/en-us/learn/modules/enable-business-users-with-key-ai-uses-cases/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "bXMtbGVhcm4wMTczMWMxMC0yMGJiLTQxZTctYmEyMS04NTI4NjY5ZGNkYzM1", "description": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps", "duration": 18, "instructor": "", "level": "advanced", "product": "azure", "rating_average": 4.75, "rating_count": 137, "role": "ai-engineer", "source": "MS Learn", "title": "Use containers for your Language Understanding Intelligent Service (LUIS) Apps", "url": "https://docs.microsoft.com/en-us/learn/modules/use-containers-language-understanding-intelligent-service-apps/?WT.mc_id=api_CatalogApi", "keyphrases": [ "ms", "learn" ] }, { "@search.score": 1, "Key": "Y29tcGFueS1tb29kbGU5NzAwZTFkYy1iMjkzLTQzMDYtOWUxYi0wZDM0NTg2M2RiNTQ1", "description": "This course will teach you the specific ways our company uses Git. You will learn details for comments, branching, pull requests, and other procsses", "duration": 3, "instructor": "Claudia Blackman", "level": "beginner", "product": "git", "rating_average": 4.5, "rating_count": 125, "role": "developer", "source": "Company Moodle", "title": "Git Workflow ", "url": "https://www.example.com/course3", "keyphrases": [ "company", "moodle" ] } ] }